{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09099650-cdc8-43aa-a285-a2a141d9fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ecd3a6-05ea-4df7-b352-c023fd58aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del texto limpio\n",
    "data_path = \"../data/plain_text/plain_text.txt\"\n",
    "with open(data_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Generar el document\n",
    "pdf_doc = Document(text=content.strip())\n",
    "\n",
    "# Carga de las preguntas\n",
    "qa_path = \"../data/exams/clean_questions/questions_all_tests.json\"\n",
    "with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Export boolean\n",
    "export_csv = False\n",
    "\n",
    "# Key\n",
    "load_dotenv()\n",
    "# Set embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528ab8d-646d-4bce-bd47-71af9f4b4fb9",
   "metadata": {},
   "source": [
    "# Primera Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a4d8e-4653-4b84-a2f8-10051b605885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model\n",
    "system_prompt = \"\"\"\n",
    "Eres un experto en Apache Spark y PySpark, y est√°s respondiendo preguntas tipo test usando exclusivamente la informaci√≥n del contexto proporcionado.\n",
    "\n",
    "No puedes utilizar ning√∫n conocimiento previo ni informaci√≥n externa al contexto.\n",
    "\n",
    "Tu tarea es:\n",
    "1. Leer el contexto cuidadosamente.\n",
    "2. Leer la pregunta y las opciones tipo test.\n",
    "3. Seleccionar la opci√≥n correcta **solo si** est√° claramente justificada por el contenido del contexto.\n",
    "4. Justificar tu elecci√≥n **citando expl√≠citamente fragmentos relevantes del contexto**.\n",
    "\n",
    "Si no puedes justificar una respuesta a partir del contexto, responde exactamente \"No he encontrado informaci√≥n al respecto.\" y no devuelvas ninguna opci√≥n como respuesta.\n",
    "\"\"\"\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "# Set embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41157eb6-1f4f-488f-995f-525c846efeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al √≠ndice optimizado\n",
    "storage_path = \"../data/index_storage/\"\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4156e7e2-1c30-4c6a-b309-85e196b00906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n auxiliar para formatear las opciones:\n",
    "def format_options(options_dict):\n",
    "    clean_options = {}\n",
    "    for key in options_dict:\n",
    "        # Elimina letras errantes como '\\nB' o '\\nC' dentro del texto\n",
    "        value = options_dict[key]\n",
    "        value = re.sub(r'\\n[A-E]\\n?', '', value).strip()\n",
    "        clean_options[key] = value\n",
    "    return \"\\n\".join([f\"{k}. {v}\" for k, v in clean_options.items()])\n",
    "\n",
    "# Funci√≥n para lanzar la query:\n",
    "def ask_question_with_options(question_obj, \n",
    "                              #query_str_template, \n",
    "                              retriever = retriever, response_mode= \"compact\"):\n",
    "    question_text = question_obj[\"question\"]\n",
    "    options_text = format_options(question_obj[\"options\"])\n",
    "\n",
    "    #qa_prompt = PromptTemplate(query_str_template)\n",
    "    #Settings.text_qa_template = qa_prompt\n",
    "\n",
    "    query_str = f\"\"\"\n",
    "    {question_text}\n",
    "\n",
    "    Opciones: \n",
    "    {options_text}\n",
    "        \n",
    "    IMPORTANTE:\n",
    "        - Usa solo informaci√≥n del contexto.\n",
    "        - No respondas con conocimientos externos.\n",
    "        - Si no puedes justificar la respuesta con fragmentos del contexto, no devuelvas ninguna opci√≥n.\n",
    "        - Si puedes justificar la respuesta con fragmentos del contexto, justificala\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar QueryBundle para pasar la informaci√≥n\n",
    "    query_bundle = QueryBundle(\n",
    "        query_str = query_str,\n",
    "        custom_embedding_strs = [question_text, options_text]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=response_mode)\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_bundle)\n",
    "    predicted_answer = response.response.strip()\n",
    "\n",
    "    return {\n",
    "        \"question_id\": question_obj[\"question_id\"],\n",
    "        \"question\" : question_obj[\"question\"],\n",
    "        \"options\": format_options(question_obj[\"options\"]),\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"correct_answer\": question_obj[\"correct_answer\"],\n",
    "        \"is_correct\": predicted_answer == question_obj[\"correct_answer\"],\n",
    "        \"response_obj\": response  # Para inspeccionar source_nodes si se desea\n",
    "    }\n",
    "\n",
    "# Validar la repsuesta:\n",
    "def display_question_result(result):\n",
    "    print(\"ID Pregunta:\", result[\"question_id\"])\n",
    "    print(\"\\nPregunta:\\n\", result[\"question\"])\n",
    "    print(\"\\nOpciones:\\n\", result[\"options\"])\n",
    "    print(\"\\nAcert√≥?:\", result[\"is_correct\"])\n",
    "    print(\"\\nRespuesta generada por el modelo:\\n\", result[\"response_obj\"])\n",
    "    print(\"\\nRespuesta correcta:\", result[\"correct_answer\"])\n",
    "    if \"response_obj\" in result:\n",
    "        print(f\"\\nüîé Chunks usados para responder la pregunta {result['question_id']}:\\n\")\n",
    "        for i, node in enumerate(result[\"response_obj\"].source_nodes):\n",
    "            print(f\"--- Chunk #{i+1} | Score: {node.score:.4f} ---\")\n",
    "            print(node.node.get_content().strip())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a57f44-6f58-44a6-969d-2afdddc77d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question =[q for q in questions if q[\"question_id\"] == \"Test_2_39\"][0]\n",
    "test_result_2_39 = ask_question_with_options(question, response_mode = \"compact\")\n",
    "# Validation\n",
    "display_question_result(test_result_2_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226d740-909d-4e01-9de6-a3d4b52a2f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question =[q for q in questions if q[\"question_id\"] == \"Test_1_27\"][0]\n",
    "test_result_2_39 = ask_question_with_options(question, response_mode = \"compact\")\n",
    "# Validation\n",
    "display_question_result(test_result_2_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d497eba-1503-4538-9aef-09a05be8f347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question =[q for q in questions if q[\"question_id\"] == \"Test_2_54\"][0]\n",
    "test_result_2_39 = ask_question_with_options(question, response_mode = \"compact\")\n",
    "# Validation\n",
    "display_question_result(test_result_2_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4945e9a-124e-4298-a4c8-a0aac8f74e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test_2_18, Test_1_9, Test_2_12, Test_1_53, Test_1_41\n",
    "\n",
    "ids = [\"Test_2_18\", \"Test_1_9\", \"Test_2_12\", \"Test_1_53\", \"Test_1_41\"]\n",
    "\n",
    "for id in ids: \n",
    "    question =[q for q in questions if q[\"question_id\"] == id][0]\n",
    "    #result_random = ask_question_with_options(question, response_mode = \"refine\")\n",
    "    result_random = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46310c05-ab05-40e7-8a17-5bab9c645fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = [\"Test_1_53\", \"Test_1_17\", \"Test_2_28\", \"Test_2_31\", \"Test_2_29\"]\n",
    "\n",
    "for id in ids: \n",
    "    question =[q for q in questions if q[\"question_id\"] == id][0]\n",
    "    #result_random = ask_question_with_options(question, response_mode = \"refine\")\n",
    "    result_random = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50c156-97aa-4dd9-b2f5-9d66412419d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_questions = random.sample(questions, k=5)\n",
    "for q in random_questions: \n",
    "    result_random = ask_question_with_options(q, response_mode = \"refine\")\n",
    "    # result_random = ask_question_with_options(q, response_mode = \"tree_summarize\")\n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93047a-cace-4667-932e-bd216af8a312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_random = ask_question_with_options(random.choice(questions), response_mode = \"refine\")\n",
    "display_question_result(result_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b21a8f-07ac-4ecf-89d2-9a2972fa9a21",
   "metadata": {},
   "source": [
    "# System Prompt menos restrictivo - Segunda Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660607da-d415-4aa5-a6a2-819c1ba4166d",
   "metadata": {},
   "source": [
    "* Este prompt permitir√≠a razonar desde el contenido si no est√° claramente la respuesta en el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3fed1-9872-432f-a397-dd54d4c111d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Eres un experto en Apache Spark y PySpark, y est√°s respondiendo preguntas tipo test usando exclusivamente la informaci√≥n del contexto proporcionado.\n",
    "\n",
    "No puedes utilizar ning√∫n conocimiento previo ni informaci√≥n externa al contexto.\n",
    "\n",
    "Tu tarea es:\n",
    "1. Leer el contexto cuidadosamente.\n",
    "2. Leer la pregunta y las opciones tipo test.\n",
    "3. Seleccionar la opci√≥n correcta si est√° claramente justificada por el contenido del contexto, y justificar tu elecci√≥n **citando expl√≠citamente fragmentos relevantes del contexto**.\n",
    "4. En caso de no estar claramente justificada por el contexto, intentar razonar desde el contenido del contexto una respuesta, indicando qu√© informaci√≥n se ha utilizado, pero NO UTILICES INFORMACI√ìN EXTERNA\n",
    "5. En caso de no encontrar informaci√≥n en el contexto relacionada con la pregunta, devolver \"No existe informaci√≥n\" aunque sepas la respuesta correcta.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96491635-5d1a-4b46-846b-33d3748a973a",
   "metadata": {},
   "source": [
    "* Permite razonar solo si se citan y conectan fragmentos de forma expl√≠cita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f57e1fd3-875b-4fe6-bc6f-3ff8d4b463a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Eres un experto en Apache Spark y PySpark, y est√°s respondiendo preguntas tipo test usando exclusivamente la informaci√≥n del contexto proporcionado.\n",
    "\n",
    "No puedes utilizar ning√∫n conocimiento previo ni informaci√≥n externa al contexto.\n",
    "\n",
    "Tu tarea es:\n",
    "1. Leer el contexto cuidadosamente.\n",
    "2. Leer la pregunta y las opciones tipo test.\n",
    "3. Seleccionar la opci√≥n correcta si est√° claramente justificada por el contenido del contexto, y justificar tu elecci√≥n citando fragmentos textuales exactos.\n",
    "4. Si no hay una justificaci√≥n directa, puedes razonar una respuesta **√∫nicamente si se basa en informaci√≥n presente en el contexto**, indicando exactamente **qu√© fragmentos usas** y **c√≥mo los conectas**. No est√° permitido inventar datos.\n",
    "5. En caso de no encontrar ninguna informaci√≥n √∫til en el contexto, responde exactamente con: **\"No existe informaci√≥n\"**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bab5a78-37e3-4277-aa8d-e354020428a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1805a7e5-ff31-4f8c-a808-1db15ecb2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predicted_letter(response_text):\n",
    "    \"\"\"\n",
    "    Extrae la letra (A, B, C, D o E) de una respuesta generada en el formato:\n",
    "    'Respuesta correcta: <LETRA>\\nJustificaci√≥n: <...>'\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Respuesta\\s+correcta\\s*:\\s*([A-E])\", response_text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None \n",
    "\n",
    "def ask_question_with_options(question_obj, \n",
    "                              #query_str_template, \n",
    "                              retriever = retriever, response_mode= \"compact\"):\n",
    "    question_text = question_obj[\"question\"]\n",
    "    options_text = format_options(question_obj[\"options\"])\n",
    "\n",
    "    #qa_prompt = PromptTemplate(query_str_template)\n",
    "    #Settings.text_qa_template = qa_prompt\n",
    "\n",
    "    query_str = f\"\"\"\n",
    "    {question_text}\n",
    "\n",
    "    Opciones: \n",
    "    {options_text}\n",
    "        \n",
    "    IMPORTANTE:\n",
    "        - Usa solo informaci√≥n del contexto.\n",
    "        - No respondas con conocimientos externos.\n",
    "        - Aunque sepas la respuesta correcta, si no existe esa informaci√≥n en el contexto devuelve \"No existe informaci√≥n\"\n",
    "    \n",
    "    Responde de la siguiente manera:\n",
    "    \"\n",
    "    Respuesta correcta: <LETRA>\n",
    "    Justificaci√≥n: <JUSTIFICACI√ìN>\n",
    "    \"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar QueryBundle para pasar la informaci√≥n\n",
    "    query_bundle = QueryBundle(\n",
    "        query_str = query_str,\n",
    "        custom_embedding_strs = [question_text, options_text]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=response_mode)\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_bundle)\n",
    "    predicted_answer = response.response.strip()\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": question_obj[\"question_id\"],\n",
    "        \"question\" : question_obj[\"question\"],\n",
    "        \"options\": format_options(question_obj[\"options\"]),\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"correct_answer\": question_obj[\"correct_answer\"],\n",
    "        \"is_correct\": extract_predicted_letter(predicted_answer) == question_obj[\"correct_answer\"],\n",
    "        \"response_obj\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2aff1-f33e-4b6a-9fd9-eb3eae4f7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preguntas que no razonaba:\n",
    "ids = [\"Test_1_17\", \"Test_2_28\", \"Test_2_31\", \"Test_1_41\"]\n",
    "for id in ids: \n",
    "    question =[q for q in questions if q[\"question_id\"] == id][0]\n",
    "    # result_random = ask_question_with_options(question, response_mode = \"refine\") # Sirve para Test_1_41 pero no para el resto. \n",
    "    result_random = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c91cfb4-561e-4df1-ae1d-2338ef8c8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_questions = random.sample(questions, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5d7308f-83fb-49ea-96d8-1fb8a2431b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response mode: TREE SUMMARIZE\n",
      "====================\n",
      "ID Pregunta: Test_1_21\n",
      "\n",
      "Pregunta:\n",
      " Which of the following code blocks performs an inner join of the salarydf and employeedf \n",
      "DataFrames for columns employeeSalaryID and employeeID, respectively?\n",
      "\n",
      "Opciones:\n",
      " A. salarydf.join(employeedf, salarydf.employeeID == employeedf.\n",
      "employeeSalaryID)\n",
      "B. i.\t\n",
      "Salarydf.createOrReplaceTempView(salarydf)\n",
      "ii.\t employeedf.createOrReplaceTempView('employeedf')\n",
      "iii.\t spark.sql(\"SELECT * FROM salarydf CROSS JOIN employeedf ON \n",
      "employeeSalaryID ==employeeID\")\n",
      "C. i.\t\n",
      "salarydf\n",
      "ii.\t .join(employeedf, col(employeeID)==col(employeeSalaryID))\n",
      "D. i.\t\n",
      "Salarydf.createOrReplaceTempView(salarydf)\n",
      "ii.\t employeedf.createOrReplaceTempView('employeedf')\n",
      "iii.\t SELECT * FROM salarydf\n",
      "iv.\t INNER JOIN employeedf\n",
      "v.\t\n",
      "ON salarydf.employeeSalaryID == employeedf. employeeID\n",
      "\n",
      "Acert√≥?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe informaci√≥n\n",
      "\n",
      "Respuesta correcta: D\n",
      "\n",
      "üîé Chunks usados para responder la pregunta Test_1_21:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7563 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames ‚Äì salary_data_with_\n",
      "id and employee_data ‚Äì joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames.\n",
      "\n",
      "--- Chunk #2 | Score: 0.7416 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames ‚Äì salary_data_with_\n",
      "id and employee_data ‚Äì joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames. Here‚Äôs what it looks like:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "\n",
      "Joining DataFrames in Spark\n",
      "\n",
      "You will notice that the how parameter defines the type of join that is being done in this statement.\n",
      "\n",
      "--- Chunk #3 | Score: 0.7216 ---\n",
      "The following code illustrates how we use an outer join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"outer\").show()\n",
      "The resulting DataFrame contains data for all the employees in the salary_data_with_id and \n",
      "employee_data DataFrames. Here‚Äôs what it looks like:\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "| ID|Employee|Department|Salary|ID|State|Gender|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "|7|Martin|Finance|3500|null| null|null|\n",
      "|8|Kiran|Sales|2200|null| null|null|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "You will notice that the how parameter has changed and says outer.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6980 ---\n",
      "We will explore some of them \n",
      "in this chapter. Let‚Äôs start with inner joins.\n",
      "Inner joins\n",
      "An inner join is used when we want to join two DataFrames based on values that are common in \n",
      "both DataFrames. Any value that doesn‚Äôt exist in any one of the DataFrames would not be part of the \n",
      "resulting DataFrame. By default, the join type is an inner join in Spark.\n",
      "Use case\n",
      "Inner joins are useful for merging data when you are interested in common elements in both DataFrames \n",
      "‚Äì for example, joining sales data with customer data to see which customers made a purchase.\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames ‚Äì salary_data_with_\n",
      "id and employee_data ‚Äì joined together in a single DataFrame.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6902 ---\n",
      "The following code illustrates how to use a right join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"right\").show()\n",
      "The resulting DataFrame contains all the data from the right-hand DataFrame ‚Äì that is, employee_\n",
      "data. It looks like this:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "Notice the how parameter has changed and now says right.\n",
      "\n",
      "====================\n",
      "Response mode: TREE SUMMARIZE\n",
      "====================\n",
      "ID Pregunta: Test_2_8\n",
      "\n",
      "Pregunta:\n",
      " Which of the following statements is accurate about the Spark driver?\n",
      "\n",
      "Opciones:\n",
      " A. There are multiple drivers in a Spark application\n",
      "B. Slots are a part of a driver\n",
      "C. Drivers execute tasks in parallel\n",
      "D. It is the responsibility of the Spark driver to transform operations into DAG computations\n",
      "\n",
      "Acert√≥?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: D  \n",
      "Justificaci√≥n: \"It is the responsibility of the driver to know how many executors are present and if any executor has failed so that it can fall back on its alternative.\" Adem√°s, se menciona que \"the driver launches the executors based on the DAG that Spark generates for its execution.\" Esto implica que el driver transforma las operaciones en computaciones DAG.\n",
      "\n",
      "Respuesta correcta: D\n",
      "\n",
      "üîé Chunks usados para responder la pregunta Test_2_8:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7291 ---\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "The Spark driver is responsible for dividing the application into smaller entities for execution. These \n",
      "entities are known as tasks. You will learn more about tasks in the upcoming sections of this chapter. \n",
      "The Spark driver also decides what data the executor will work on and what tasks are run on which \n",
      "executor. These tasks are scheduled to run on the executor nodes with the help of the cluster manager. \n",
      "This information that is driven by the driver enables fault tolerance. Since the driver has all the \n",
      "information about the number of available workers and the tasks that are running on each of them \n",
      "alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \n",
      "taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \n",
      "executor returns the task earlier would prevail.\n",
      "\n",
      "--- Chunk #2 | Score: 0.7025 ---\n",
      "As shown in Figure 3.1, the driver node contains SparkSession, which is the entry point of the \n",
      "Spark application. Previously, this was known as the SparkContext object, but in Spark 2.0, \n",
      "SparkSession handles all contexts to start execution. The application‚Äôs main method runs on the \n",
      "driver to coordinate the whole application. It runs on its own Java Virtual Machine (JVM). Spark \n",
      "driver can run as an independent process or it can run on one of the worker nodes, depending on \n",
      "the architecture.\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "The Spark driver is responsible for dividing the application into smaller entities for execution. These \n",
      "entities are known as tasks. You will learn more about tasks in the upcoming sections of this chapter. \n",
      "The Spark driver also decides what data the executor will work on and what tasks are run on which \n",
      "executor.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6952 ---\n",
      "Driver \n",
      "launches the executors based on the DAG that Spark generates for its execution. Once the tasks have \n",
      "finished executing, executors send the results back to the driver.\n",
      "Since the driver is the main controller of the Spark application, if an executor fails or takes too long to \n",
      "execute a task, the driver can choose to send that task over to other available executors. This ensures \n",
      "reliability and fault tolerance in Spark. We will read more about this later in this chapter.\n",
      "It is the responsibility of the executor to read data from external sources that are needed to run the \n",
      "tasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by \n",
      "the executor.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6901 ---\n",
      "The driver has control and knowledge of \n",
      "all the executors at any given time. It is the responsibility of the driver to know how many executors \n",
      "are present and if any executor has failed so that it can fall back on its alternative. The Spark driver \n",
      "also maintains communication with executors all the time. The driver runs on the master node of a \n",
      "machine or cluster. When a Spark application starts running, the driver keeps up with all the required \n",
      "information that is needed to run the application successfully.\n",
      "As shown in Figure 3.1, the driver node contains SparkSession, which is the entry point of the \n",
      "Spark application. Previously, this was known as the SparkContext object, but in Spark 2.0, \n",
      "SparkSession handles all contexts to start execution. The application‚Äôs main method runs on the \n",
      "driver to coordinate the whole application. It runs on its own Java Virtual Machine (JVM).\n",
      "\n",
      "--- Chunk #5 | Score: 0.6853 ---\n",
      "Now that you understand the execution hierarchy, let‚Äôs discuss each of Spark‚Äôs components in detail.\n",
      "Spark components\n",
      "Let‚Äôs dive into the inner workings of each Spark component to understand how each of them plays a \n",
      "crucial role in empowering efficient distributed data processing.\n",
      "Spark driver\n",
      "The Spark driver is the core of the intelligent and efficient computations in Spark. Spark follows an \n",
      "architecture that is commonly known as the master-worker architecture in network topology. Consider \n",
      "the Spark driver as a master and Spark executors as slaves. The driver has control and knowledge of \n",
      "all the executors at any given time. It is the responsibility of the driver to know how many executors \n",
      "are present and if any executor has failed so that it can fall back on its alternative. The Spark driver \n",
      "also maintains communication with executors all the time. The driver runs on the master node of a \n",
      "machine or cluster.\n",
      "\n",
      "====================\n",
      "Response mode: TREE SUMMARIZE\n",
      "====================\n",
      "ID Pregunta: Test_1_12\n",
      "\n",
      "Pregunta:\n",
      " Which of the following is one of the tasks of the cluster manager in Spark?\n",
      "\n",
      "Opciones:\n",
      " A. In the event of an executor failure, the cluster manager will collaborate with the driver to \n",
      "initiate a new executor\n",
      "B. The cluster manager can coalesce partitions to increase the speed of complex data processing\n",
      "C. The cluster manager collects runtime statistics of queries\n",
      "D. The cluster manager creates query plans\n",
      "\n",
      "Acert√≥?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificaci√≥n: \"The cluster manager is responsible for launching the driver program and allocating resources for execution.\" Esto implica que en caso de fallos, el cluster manager tiene un rol en la gesti√≥n de los recursos, lo que puede incluir la colaboraci√≥n con el driver para iniciar nuevos ejecutores.\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "üîé Chunks usados para responder la pregunta Test_1_12:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7036 ---\n",
      "Since the driver has all the \n",
      "information about the number of available workers and the tasks that are running on each of them \n",
      "alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \n",
      "taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \n",
      "executor returns the task earlier would prevail. The Spark driver also maintains metadata about the \n",
      "Resilient Distributed Dataset (RDD) and its partitions.\n",
      "It is the responsibility of the Spark driver to design the complete execution map. It determines which \n",
      "tasks run on which executors, as well as how the data is distributed across these executors. This is \n",
      "done by creating RDDs internally. Based on this distribution of data, the operations that are required \n",
      "are determined, such as transformations and actions that are defined in the program. A DAG is \n",
      "created based on these decisions.\n",
      "\n",
      "--- Chunk #2 | Score: 0.6956 ---\n",
      "In cluster mode, the driver program runs within \n",
      "the cluster, on one of the worker nodes.\n",
      "The driver program is responsible \n",
      "for orchestrating the execution of the \n",
      "Spark application, including creating \n",
      "SparkContext and coordinating tasks.\n",
      "The cluster manager is responsible for launching \n",
      "the driver program and allocating resources \n",
      "for execution.\n",
      "The client machine interacts directly with the \n",
      "cluster manager to request resources and launch \n",
      "executors on worker nodes.\n",
      "Once the driver program is launched, it \n",
      "coordinates with the cluster manager to request \n",
      "resources and distribute tasks to worker nodes.\n",
      "It may not be suitable for production \n",
      "deployments with large-scale applications.\n",
      "It is commonly used for production deployments \n",
      "as it allows for better resource utilization and \n",
      "scalability. It also ensures fault tolerance.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6843 ---\n",
      "4.\t\n",
      "The cluster manager then starts the executors, which can communicate with the driver directly.\n",
      "5.\t\n",
      "The driver creates a logical plan, known as a directed acyclic graph (DAG), and physical plan \n",
      "for execution based on the total number of tasks required to be executed.\n",
      "6.\t\n",
      "The driver also divides data to be run on each executor, along with tasks.\n",
      "7.\t\n",
      "Once each task finishes running, the driver gets the results.\n",
      "8.\t\n",
      "When the program finishes running, the main() method exits and Spark frees all executors \n",
      "and driver resources.\n",
      "Now that you understand the execution hierarchy, let‚Äôs discuss each of Spark‚Äôs components in detail.\n",
      "Spark components\n",
      "Let‚Äôs dive into the inner workings of each Spark component to understand how each of them plays a \n",
      "crucial role in empowering efficient distributed data processing.\n",
      "Spark driver\n",
      "The Spark driver is the core of the intelligent and efficient computations in Spark.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6582 ---\n",
      "Once the application has finished execution, these resources are released back to the cluster manager.\n",
      "Applications have their dedicated executor processes that parallelize how tasks are run. The advantage \n",
      "is that each application is independent of the other and runs on its own schedule. Data also becomes \n",
      "independent for each of these applications, so data sharing can only take place by writing data to disk \n",
      "so that it can be shared across applications.\n",
      "Cluster modes\n",
      "Cluster modes define how Spark applications utilize cluster resources, manage task execution, and \n",
      "interact with cluster managers for resource allocation.\n",
      "If there is more than one user sharing resources on the cluster, be it Spark applications or other \n",
      "applications that need cluster resources, they have to be managed based on different modes. There \n",
      "are two types of modes available for cluster managers ‚Äì standalone client mode and cluster mode.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6495 ---\n",
      "The client machine interacts directly with the \n",
      "cluster manager to request resources and launch \n",
      "executors on worker nodes.\n",
      "Once the driver program is launched, it \n",
      "coordinates with the cluster manager to request \n",
      "resources and distribute tasks to worker nodes.\n",
      "It may not be suitable for production \n",
      "deployments with large-scale applications.\n",
      "It is commonly used for production deployments \n",
      "as it allows for better resource utilization and \n",
      "scalability. It also ensures fault tolerance.\n",
      "Table 3.1: Client mode versus cluster mode\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "Now, we will talk about different deployment modes and their corresponding managers in Spark:\n",
      "‚Ä¢\t Built-in standalone mode (Spark‚Äôs native manager): A simple cluster manager bundled with \n",
      "Spark that‚Äôs suitable for small to medium-scale deployments without external dependencies.\n",
      "‚Ä¢\t Apache YARN (Hadoop‚Äôs resource manager): Integrated with Spark, YARN enables Spark \n",
      "applications to share Hadoop‚Äôs cluster resources efficiently.\n",
      "\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for question in random_questions: \n",
    "    \n",
    "    print(\"Response mode: TREE SUMMARIZE\")\n",
    "    print(\"=\"*20)\n",
    "    result_random = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4f318-3aac-4a39-be82-87d8d62f4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"=\"*20)\n",
    "    print(\"Response mode: REFINE\")\n",
    "    print(\"=\"*20)\n",
    "    result_random = ask_question_with_options(question, response_mode = \"refine\") # Sirve para Test_1_41 pero no para el resto. \n",
    "    display_question_result(result_random)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3744299-7f3f-4ea7-ba3e-4e255c9cba9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Pregunta: Test_2_57\n",
      "\n",
      "Pregunta:\n",
      " Which code block will write the df DataFrame as a parquet file on the filePath path partitioning \n",
      "it on the department column?\n",
      "\n",
      "Opciones:\n",
      " A. df.write.partitionBy(\"department\").parquet(filePath)\n",
      "B. df.write.partition(\"department\").parquet(filePath)\n",
      "C. df.write.parquet(\"department\").partition(filePath)\n",
      "D. df.write.coalesce(\"department\").parquet(filePath)\n",
      "\n",
      "Acert√≥?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe informaci√≥n\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "üîé Chunks usados para responder la pregunta Test_2_57:\n",
      "\n",
      "--- Chunk #1 | Score: 0.5939 ---\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "Reading and writing Parquet files\n",
      "In this section, we will discuss the Parquet file format. Parquet is a columnar file format that makes data \n",
      "reading and writing very efficient. It is also a compact file format that facilitates faster reads and writes.\n",
      "Let‚Äôs learn how to write Parquet files with Spark by running the following code:\n",
      "salary_data_with_id.write.parquet('salary_data.parquet', \n",
      "mode='overwrite')\n",
      "spark.read.parquet(' /salary_data.parquet').show()\n",
      "The resulting DataFrame, named salary_data_with_id,\n",
      "\n",
      "--- Chunk #2 | Score: 0.5816 ---\n",
      "It is also a compact file format that facilitates faster reads and writes.\n",
      "Let‚Äôs learn how to write Parquet files with Spark by running the following code:\n",
      "salary_data_with_id.write.parquet('salary_data.parquet', \n",
      "mode='overwrite')\n",
      "spark.read.parquet(' /salary_data.parquet').show()\n",
      "The resulting DataFrame, named salary_data_with_id, looks like this:\n",
      "+---+--------+----------+------+\n",
      "|ID |Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "| 1 | John| Field-eng|3500|\n",
      "| 2 | Robert | Sales|4000|\n",
      "| 3 | Maria| Finance|3500|\n",
      "| 4 | Michael| Sales|3000|\n",
      "| 5 | Kelly| Finance|3500|\n",
      "| 6 | Kate| Finance|3000|\n",
      "| 7 | Martin | Finance|3500|\n",
      "| 8 | Kiran| Sales|2200|\n",
      "+---+--------+----------+------+\n",
      "There are certain parameters in the dataframe.\n",
      "\n",
      "--- Chunk #3 | Score: 0.5751 ---\n",
      "First, we specify the format \n",
      "of the file that needs to be read. Then, we can perform different function calls for different options. In \n",
      "the next call, we specify that the file has a header, so the DataFrame expects to have a header. Then, we \n",
      "specify that we need to have a schema for this data, which is defined in the schema variable. Finally, \n",
      "in the load function, we define the path of the file to be loaded.\n",
      "Next, we will learn how to read and write Parquet files with Spark.\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "Reading and writing Parquet files\n",
      "In this section, we will discuss the Parquet file format. Parquet is a columnar file format that makes data \n",
      "reading and writing very efficient. It is also a compact file format that facilitates faster reads and writes.\n",
      "Let‚Äôs learn how to write Parquet files with Spark by running the following code:\n",
      "salary_data_with_id.\n",
      "\n",
      "--- Chunk #4 | Score: 0.5117 ---\n",
      "Reading and writing Delta files\n",
      "The Delta file format is an open format that is more optimized than Parquet and other columnar \n",
      "formats. When the data is stored in Delta format, you will notice that the underlying files are in \n",
      "Parquet. The Delta format adds a transactional log on top of Parquet files to make data reads and \n",
      "writes a lot more efficient.\n",
      "Let‚Äôs learn how to read and write Delta files with Spark by running the following code:\n",
      "salary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/\n",
      "salary_data_with_id\", mode='overwrite')\n",
      "df = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\n",
      "df.show()\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "The resulting DataFrame, named salary_data_with_id,\n",
      "\n",
      "--- Chunk #5 | Score: 0.5097 ---\n",
      "write() function that we can see. The first call \n",
      "is to the orc function to define the file type. Then, as the next parameter, we specify the path where \n",
      "this Parquet file needs to be written.\n",
      "In the next statement, we‚Äôre reading back the same file that we wrote to see its contents. We can see \n",
      "that the resulting file contains the same data that we wrote.\n",
      "Next, we will look at how we can read and write Delta files with Spark.\n",
      "Reading and writing Delta files\n",
      "The Delta file format is an open format that is more optimized than Parquet and other columnar \n",
      "formats. When the data is stored in Delta format, you will notice that the underlying files are in \n",
      "Parquet. The Delta format adds a transactional log on top of Parquet files to make data reads and \n",
      "writes a lot more efficient.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test_2_57\n",
    "question =[q for q in questions if q[\"question_id\"] == \"Test_2_57\"][0]\n",
    "test_result = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "# Validation\n",
    "display_question_result(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3bd3c26-3b66-4eb1-816f-d6fc0edad794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Pregunta: Test_1_12\n",
      "\n",
      "Pregunta:\n",
      " Which of the following is one of the tasks of the cluster manager in Spark?\n",
      "\n",
      "Opciones:\n",
      " A. In the event of an executor failure, the cluster manager will collaborate with the driver to \n",
      "initiate a new executor\n",
      "B. The cluster manager can coalesce partitions to increase the speed of complex data processing\n",
      "C. The cluster manager collects runtime statistics of queries\n",
      "D. The cluster manager creates query plans\n",
      "\n",
      "Acert√≥?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificaci√≥n: \"The cluster manager is responsible for launching the driver program and allocating resources for execution.\" Adem√°s, se menciona que \"the driver has all the information about the number of available workers and the tasks that are running on each of them alongside data in case a worker fails, that task can be reassigned to a different cluster.\" Esto implica que el cluster manager colabora con el driver en la gesti√≥n de recursos y la re-asignaci√≥n de tareas en caso de fallos.\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "üîé Chunks usados para responder la pregunta Test_1_12:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7036 ---\n",
      "Since the driver has all the \n",
      "information about the number of available workers and the tasks that are running on each of them \n",
      "alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \n",
      "taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \n",
      "executor returns the task earlier would prevail. The Spark driver also maintains metadata about the \n",
      "Resilient Distributed Dataset (RDD) and its partitions.\n",
      "It is the responsibility of the Spark driver to design the complete execution map. It determines which \n",
      "tasks run on which executors, as well as how the data is distributed across these executors. This is \n",
      "done by creating RDDs internally. Based on this distribution of data, the operations that are required \n",
      "are determined, such as transformations and actions that are defined in the program. A DAG is \n",
      "created based on these decisions.\n",
      "\n",
      "--- Chunk #2 | Score: 0.6955 ---\n",
      "In cluster mode, the driver program runs within \n",
      "the cluster, on one of the worker nodes.\n",
      "The driver program is responsible \n",
      "for orchestrating the execution of the \n",
      "Spark application, including creating \n",
      "SparkContext and coordinating tasks.\n",
      "The cluster manager is responsible for launching \n",
      "the driver program and allocating resources \n",
      "for execution.\n",
      "The client machine interacts directly with the \n",
      "cluster manager to request resources and launch \n",
      "executors on worker nodes.\n",
      "Once the driver program is launched, it \n",
      "coordinates with the cluster manager to request \n",
      "resources and distribute tasks to worker nodes.\n",
      "It may not be suitable for production \n",
      "deployments with large-scale applications.\n",
      "It is commonly used for production deployments \n",
      "as it allows for better resource utilization and \n",
      "scalability. It also ensures fault tolerance.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6843 ---\n",
      "4.\t\n",
      "The cluster manager then starts the executors, which can communicate with the driver directly.\n",
      "5.\t\n",
      "The driver creates a logical plan, known as a directed acyclic graph (DAG), and physical plan \n",
      "for execution based on the total number of tasks required to be executed.\n",
      "6.\t\n",
      "The driver also divides data to be run on each executor, along with tasks.\n",
      "7.\t\n",
      "Once each task finishes running, the driver gets the results.\n",
      "8.\t\n",
      "When the program finishes running, the main() method exits and Spark frees all executors \n",
      "and driver resources.\n",
      "Now that you understand the execution hierarchy, let‚Äôs discuss each of Spark‚Äôs components in detail.\n",
      "Spark components\n",
      "Let‚Äôs dive into the inner workings of each Spark component to understand how each of them plays a \n",
      "crucial role in empowering efficient distributed data processing.\n",
      "Spark driver\n",
      "The Spark driver is the core of the intelligent and efficient computations in Spark.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6583 ---\n",
      "Once the application has finished execution, these resources are released back to the cluster manager.\n",
      "Applications have their dedicated executor processes that parallelize how tasks are run. The advantage \n",
      "is that each application is independent of the other and runs on its own schedule. Data also becomes \n",
      "independent for each of these applications, so data sharing can only take place by writing data to disk \n",
      "so that it can be shared across applications.\n",
      "Cluster modes\n",
      "Cluster modes define how Spark applications utilize cluster resources, manage task execution, and \n",
      "interact with cluster managers for resource allocation.\n",
      "If there is more than one user sharing resources on the cluster, be it Spark applications or other \n",
      "applications that need cluster resources, they have to be managed based on different modes. There \n",
      "are two types of modes available for cluster managers ‚Äì standalone client mode and cluster mode.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6495 ---\n",
      "The client machine interacts directly with the \n",
      "cluster manager to request resources and launch \n",
      "executors on worker nodes.\n",
      "Once the driver program is launched, it \n",
      "coordinates with the cluster manager to request \n",
      "resources and distribute tasks to worker nodes.\n",
      "It may not be suitable for production \n",
      "deployments with large-scale applications.\n",
      "It is commonly used for production deployments \n",
      "as it allows for better resource utilization and \n",
      "scalability. It also ensures fault tolerance.\n",
      "Table 3.1: Client mode versus cluster mode\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "Now, we will talk about different deployment modes and their corresponding managers in Spark:\n",
      "‚Ä¢\t Built-in standalone mode (Spark‚Äôs native manager): A simple cluster manager bundled with \n",
      "Spark that‚Äôs suitable for small to medium-scale deployments without external dependencies.\n",
      "‚Ä¢\t Apache YARN (Hadoop‚Äôs resource manager): Integrated with Spark, YARN enables Spark \n",
      "applications to share Hadoop‚Äôs cluster resources efficiently.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test_1_12 \n",
    "question =[q for q in questions if q[\"question_id\"] == \"Test_1_12\"][0]\n",
    "test_result = ask_question_with_options(question, response_mode = \"tree_summarize\")\n",
    "# Validation\n",
    "display_question_result(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
