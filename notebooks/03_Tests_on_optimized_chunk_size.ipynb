{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df938d8d-fdbb-4dac-888d-5e26c6225878",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da8f3ba-ee83-44f5-81f6-10ddefa3e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine # para crear una query_engine con un retriever específico.\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "import pandas as pd\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Carga del texto limpio\n",
    "data_path = \"../data/plain_text/plain_text.txt\"\n",
    "\n",
    "with open(data_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "pdf_doc = Document(text=content.strip())\n",
    "\n",
    "# Export boolean\n",
    "export_csv = False\n",
    "\n",
    "# Key\n",
    "load_dotenv()\n",
    "\n",
    "# Set llm model\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "Settings.llm = llm\n",
    "# Set embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c9f672-2dc9-46d3-94e1-b7462b2a9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar functions\n",
    "def run_query_and_inspect(query, query_engine, show_nodes=True):\n",
    "    \"\"\"\n",
    "    Ejecuta la query sobre el query_engine dado, imprime la respuesta y los chunks recuperados.\n",
    "    \"\"\"\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    print(f\"\\nConsulta: {query}\\n{'='*60}\")\n",
    "    print(f\"\\nRespuesta: {response.response}\\n{'='*60}\")\n",
    "\n",
    "    if show_nodes:\n",
    "        for i, node in enumerate(response.source_nodes):\n",
    "            print(f\"\\n--- Nodo {i+1} ---\")\n",
    "            print(f\"Score: {node.score:.4f}\")\n",
    "            print(node.node.get_content())\n",
    "    \n",
    "    return response  # Devolvemos también el objeto response si lo quieres seguir utilizando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f178e8-45a2-4297-a77d-4966152b284d",
   "metadata": {},
   "source": [
    "## Get VectorStoreIndex w optimized chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbbd45fc-697b-4e82-8c2f-45c2d58bebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"../data/index_storage/\"\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a210ce34-6f2e-40cc-bc0b-6117238699c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine retriever\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf1e22-1ad9-4340-b331-715836dbc1f2",
   "metadata": {},
   "source": [
    "# Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3147ba72-f07a-45ca-9807-7324791fe5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer, node_postprocessors=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91354e70-8159-4f15-8a65-50a3c1d9b689",
   "metadata": {},
   "source": [
    "# System prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0bcbb9-721f-4854-81c5-17898c0ec6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How can we configure the Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.6063\n",
      "This capability, along with \n",
      "other features, makes Spark the tool of choice for any production-grade applications.\n",
      "\n",
      "Understanding Apache Spark and Its Applications\n",
      "\n",
      "Multiple language support\n",
      "Spark supports multiple languages for development such as Java, R, Scala, and Python. This gives \n",
      "users the flexibility to use any language of choice to build applications in Spark.\n",
      "The components of Spark\n",
      "Let’s talk about the different components Spark has. As you can see in Figure 1.1, Spark Core is the \n",
      "backbone of operations in Spark and spans across all the other components that Spark has. Other \n",
      "components that we’re going to discuss in this section are Spark SQL, Spark Streaming, Spark MLlib, \n",
      "and GraphX.\n",
      "Figure 2.1: Spark components\n",
      "Let’s look at the first component of Spark.\n",
      "Spark Core\n",
      "Spark Core is central to all the other components of Spark. It provides functionalities and core features \n",
      "for all the different components.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.5980\n",
      "All of these APIs use SparkSession from its core to interact with the Spark application.\n",
      "SparkSession keeps track of Spark executors throughout the application’s execution.\n",
      "\n",
      "Spark components\n",
      "\n",
      "Cluster manager\n",
      "Spark is a distributed framework, which requires it to have access to computing resources. This access is \n",
      "governed and controlled by a process known as the cluster manager. It is the responsibility of the cluster \n",
      "manager to allocate computing resources for the Spark application when the application execution \n",
      "starts. These resources become available at the request of the application master. In the Apache Spark \n",
      "ecosystem, the application master plays a crucial role in managing and coordinating the execution \n",
      "of Spark applications within a distributed cluster environment. It’s an essential component that’s \n",
      "responsible for negotiating resources, scheduling tasks, and monitoring the application’s execution.\n",
      "Once the resources are available, the driver is made aware of those resources.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.5807\n",
      "Apache Spark offers different cluster and deployment modes to run applications across distributed \n",
      "computing environments. We’ll take a look at them in the next section.\n",
      "Deployment modes\n",
      "There are different deployment modes available in Spark. These deployment modes define how Spark \n",
      "applications are launched, executed, and managed in diverse computing infrastructures. Based on \n",
      "these different deployment modes, it gets decided where the Spark driver, executor, and cluster \n",
      "manager will run.\n",
      "The different deployment modes that are available in Spark are as follows:\n",
      "•\t Local: In local mode, the Spark driver and executor run on a single JVM and the cluster manager \n",
      "runs on the same host as the driver and executor.\n",
      "•\t Standalone: In standalone mode, the driver can run on any node of the cluster and the executor \n",
      "will launch its own independent JVM. The cluster manager can remain on any of the hosts in \n",
      "the cluster.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.5686\n",
      "Spark executions start with a user submitting a spark-submit request to the Spark engine. This \n",
      "will create a Spark application. Once an action is performed, it will result in a job being created.\n",
      "2.\t\n",
      "This request will initiate communication with the cluster manager. In turn, the cluster manager \n",
      "initializes the Spark driver to execute the main() method of the Spark application. To execute \n",
      "this method, SparkSession is created.\n",
      "3.\t\n",
      "The driver starts communicating with the cluster manager and asks for resources to start \n",
      "planning for execution.\n",
      "4.\t\n",
      "The cluster manager then starts the executors, which can communicate with the driver directly.\n",
      "5.\t\n",
      "The driver creates a logical plan, known as a directed acyclic graph (DAG), and physical plan \n",
      "for execution based on the total number of tasks required to be executed.\n",
      "6.\t\n",
      "The driver also divides data to be run on each executor, along with tasks.\n",
      "7.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.5681\n",
      "•\t Apache YARN (Hadoop’s resource manager): Integrated with Spark, YARN enables Spark \n",
      "applications to share Hadoop’s cluster resources efficiently.\n",
      "•\t Apache Mesos (resource sharing platform): Mesos offers efficient resource sharing across \n",
      "multiple applications, allowing Spark to run alongside other frameworks.\n",
      "We will talk more about deployment modes later in this chapter.\n",
      "Spark executors\n",
      "Spark executors are the processes that run on the worker node and execute tasks sent by the driver. \n",
      "The data is stored in memory primarily but can also be written to disk storage closest to them. Driver \n",
      "launches the executors based on the DAG that Spark generates for its execution. Once the tasks have \n",
      "finished executing, executors send the results back to the driver.\n",
      "Since the driver is the main controller of the Spark application, if an executor fails or takes too long to \n",
      "execute a task, the driver can choose to send that task over to other available executors.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Eres un experto en PySpark y Spark, especializado en el temario de la certificación Databricks Certified Associate Developer for Apache Spark.\n",
    "\n",
    "Debes responder única y exclusivamente utilizando la información que se encuentra en el contexto. \n",
    "\n",
    "Si la respuesta a la pregunta no se puede obtener del contexto, responde exactamente: \"No he encontrado información al respecto.\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" PROMPT TEMPLATE INICIAL --------------------\n",
    "qa_template_str = (\n",
    "    \"Contexto: {context_str}\\n\\n\"\n",
    "    \"Pregunta: {query_str}\\n\"\n",
    "    \"Respuesta:\"\n",
    ")\n",
    "\n",
    "------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "qa_template_str = \"\"\"\n",
    "Contexto: {context_str}\n",
    "Utiliza exclusivamente la información anterior para responder. \n",
    "Pregunta: {query_str}\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "qa_template = PromptTemplate(qa_template_str)\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "response_synthesizer_2 = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    llm=llm,\n",
    "    text_qa_template=qa_template\n",
    ")\n",
    "\n",
    "query_engine_2 = RetrieverQueryEngine(\n",
    "    retriever=retriever, \n",
    "    response_synthesizer=response_synthesizer_2\n",
    "    # ,\n",
    "    # node_postprocessors=[]\n",
    ")\n",
    "\n",
    "# Ejemplo de pregunta\n",
    "query = \"How can we configure the Spark application?\"\n",
    "response = run_query_and_inspect(query = query, query_engine=query_engine_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7c05e1-6bd9-41ff-8e83-cf652290c87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: What does collect() function does in pyspark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: The collect() function retrieves all elements from the DataFrame or RDD and returns them as a list. It should be used with caution as it brings all data to the driver node, and if the driver doesn’t have enough memory to hold the data, it may result in out-of-memory errors.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can we control the intern process in a Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the under-the-hood workflow in a Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: The under-the-hood workflow in a Spark application involves the following steps:\n",
      "\n",
      "1. A user submits a spark-submit request to the Spark engine, creating a Spark application. An action performed will result in a job being created.\n",
      "2. The request initiates communication with the cluster manager, which initializes the Spark driver to execute the main() method of the Spark application. A SparkSession is created for this execution.\n",
      "3. The driver communicates with the cluster manager to request resources for execution planning.\n",
      "4. The cluster manager starts the executors, which can communicate directly with the driver.\n",
      "5. The driver creates a logical plan, known as a directed acyclic graph (DAG), and a physical plan for execution based on the total number of tasks required.\n",
      "6. The driver divides the data to be processed across each executor, along with the associated tasks.\n",
      "7. At the end of processing, results are aggregated and sent back to the driver for further steps.\n",
      "\n",
      "This workflow is facilitated by the inherent process of DAG creation, which Spark performs before execution, prioritizing the necessary steps based on internal algorithms.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the role of the Driver component?\n",
      "============================================================\n",
      "\n",
      "Respuesta: El Driver es el núcleo de las computaciones inteligentes y eficientes en Spark. Su rol incluye dividir la aplicación en tareas más pequeñas para su ejecución, decidir qué datos procesarán los ejecutores y qué tareas se ejecutarán en cada uno de ellos. Además, el Driver mantiene comunicación constante con los ejecutores, controla el número de ejecutores presentes y gestiona la recuperación en caso de fallos. También crea un plan lógico y físico para la ejecución basado en el número total de tareas requeridas. Una vez que las tareas finalizan, el Driver recibe los resultados y, al finalizar el programa, libera todos los recursos de los ejecutores y del Driver.\n",
      "============================================================\n",
      "\n",
      "Consulta: How is the execution hierarchy organized in Spark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: The execution hierarchy in Spark is organized as follows:\n",
      "\n",
      "1. A user submits a spark-submit request to the Spark engine, creating a Spark application. This leads to the creation of a job when an action is performed.\n",
      "2. Spark builds a logical map of the queries and plans the best path of execution. Execution does not begin until an action statement is encountered.\n",
      "3. Spark keeps track of the steps in the code that need to be executed by creating a directed acyclic graph (DAG) for the transformations and actions.\n",
      "4. The Spark driver, which acts as the master, manages the executors (slaves) and maintains communication with them.\n",
      "5. The cluster manager starts the executors, which communicate directly with the driver.\n",
      "6. The driver divides the data and tasks among the executors for execution.\n",
      "7. Once tasks finish running, the driver collects the results.\n",
      "8. After the program completes, the main() method exits, and Spark frees all resources associated with the executors and driver.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I get the number of rows in a Spark DataFrame?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Para obtener el número de filas en un DataFrame de Spark, se utiliza el método `data_df.count()`.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I create a DataFrame in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Puedes crear un DataFrame en PySpark utilizando la función `pyspark.sql.SparkSession.createDataFrame`. Hay varias maneras de hacerlo, incluyendo:\n",
      "\n",
      "1. Usando listas de filas: Puedes crear un DataFrame a partir de listas de datos donde cada lista representa una fila.\n",
      "   \n",
      "   ```python\n",
      "   from pyspark.sql import Row\n",
      "   data_df = spark.createDataFrame([\n",
      "       Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n",
      "       Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0))\n",
      "   ])\n",
      "   ```\n",
      "\n",
      "2. Usando RDDs: Puedes crear un DataFrame a partir de un RDD.\n",
      "\n",
      "   ```python\n",
      "   rdd = spark.sparkContext.parallelize([\n",
      "       (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
      "       (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0))\n",
      "   ])\n",
      "   data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n",
      "   ```\n",
      "\n",
      "3. Usando Pandas DataFrames: Primero creas un DataFrame en Pandas y luego lo conviertes a un DataFrame de PySpark.\n",
      "\n",
      "Además, puedes especificar un esquema para el DataFrame utilizando el argumento `schema`, o dejar que Spark infiera el esquema automáticamente.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the optimized way to perform joins in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La forma optimizada de realizar joins en PySpark implica planificar el número óptimo de particiones para minimizar la sobrecarga de shuffle. Además, es importante elegir el tipo de join adecuado según el tamaño de los DataFrames. Para conjuntos de datos grandes y comparables, los shuffle joins son apropiados, mientras que para conjuntos de datos más pequeños, se pueden considerar otras técnicas como los broadcast joins.\n",
      "============================================================\n",
      "\n",
      "Consulta: What does a 'broadcast join' means?\n",
      "============================================================\n",
      "\n",
      "Respuesta: A broadcast join is a highly efficient technique for joining a small DataFrame with a larger one, where the smaller DataFrame is broadcast to all worker nodes. This approach eliminates the need for shuffling data across the network, making it a specific optimization technique applicable when one of the DataFrames is small enough to fit in memory. By broadcasting the small DataFrame, costly shuffling is avoided, leading to reduced network overhead and improved performance.\n",
      "============================================================\n",
      "\n",
      "Consulta: How does persist() differ from cache() in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: The `persist()` method allows you to specify different storage levels for data, including options for storing data in memory, on disk, or both. In contrast, the `cache()` method is a specific case of `persist()` that defaults to the memory-only storage level. Thus, while both methods are used for caching data, `persist()` offers more flexibility in choosing the storage level.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I select specific columns from a PySpark DataFrame?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Para seleccionar columnas específicas de un DataFrame de PySpark, se puede utilizar la función select(). Por ejemplo, para seleccionar una columna llamada col_3, se puede hacer de la siguiente manera:\n",
      "\n",
      "```python\n",
      "data_df.select(data_df.col_3).show()\n",
      "```\n",
      "\n",
      "Esto mostrará solo la columna seleccionada con sus datos. También se puede usar la sintaxis alternativa:\n",
      "\n",
      "```python\n",
      "data_df.select('col_3').show()\n",
      "```\n",
      "\n",
      "Ambas formas permiten seleccionar columnas en un DataFrame de Spark.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What does collect() function does in pyspark?\",\n",
    "    \"How can we control the intern process in a Spark application?\", \n",
    "    \"What is the under-the-hood workflow in a Spark application?\",\n",
    "    \"What is the role of the Driver component?\",\n",
    "    \"How is the execution hierarchy organized in Spark?\",\n",
    "    \"How can I get the number of rows in a Spark DataFrame?\",\n",
    "    \"How can I create a DataFrame in PySpark?\",\n",
    "    \"What is the optimized way to perform joins in PySpark?\",\n",
    "    \"What does a 'broadcast join' means?\",\n",
    "    \"How does persist() differ from cache() in PySpark?\",\n",
    "    \"How can I select specific columns from a PySpark DataFrame?\"\n",
    "]\n",
    "\n",
    "for query in queries: \n",
    "    response = run_query_and_inspect(query, query_engine_2, show_nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff52a3f-3273-4000-8f96-439b1daeb947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How does persist() differ from cache() in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.6865\n",
      "Data persistence ensures that the intermediate results are \n",
      "available for reuse without recomputation.\n",
      "•\t Caching versus persistence: Caching is a specific form of data persistence that stores data in \n",
      "memory, while persistence encompasses both in-memory and on-disk storage.\n",
      "Caching data\n",
      "Caching is a form of data persistence that stores DataFrames, RDDs, or datasets in memory for fast \n",
      "access. It is an essential optimization technique that improves the performance of Spark applications, \n",
      "particularly when dealing with iterative algorithms or repeated computations.\n",
      "To cache a DataFrame or an RDD, you can use the .cache() or .persist() method while \n",
      "specifying the storage level:\n",
      "•\t Memory-only: This option stores data in memory but does not replicate it for fault tolerance. \n",
      "Use .cache() or .persist(StorageLevel.MEMORY_ONLY).\n",
      "•\t Memory-only, serialized: This option stores data in memory in a serialized form, reducing \n",
      "memory usage.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.6616\n",
      "Caching and persistence are techniques that \n",
      "allow you to store intermediate or frequently used data in memory or on disk, reducing the need for \n",
      "recomputation and enhancing overall performance. This section explores the concepts of persisting \n",
      "and caching in Spark.\n",
      "Understanding data persistence\n",
      "Data persistence is the process of storing the intermediate or final results of Spark transformations \n",
      "in memory or on disk. By persisting data, you reduce the need to recompute it from the source data, \n",
      "thereby improving query performance.\n",
      "The following key concepts are related to data persistence:\n",
      "•\t Storage levels: Spark offers multiple storage levels for data, ranging from memory-only to \n",
      "disk, depending on your needs. Each storage level comes with its trade-offs in terms of speed \n",
      "and durability.\n",
      "•\t Lazy evaluation: Spark follows a lazy evaluation model, meaning transformations are not \n",
      "executed until an action is called. Data persistence ensures that the intermediate results are \n",
      "available for reuse without recomputation.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.6529\n",
      "Here’s an example of unpersisting data:\n",
      "# Cache a DataFrame\n",
      "df.cache()\n",
      "# Unpersist the cached DataFrame\n",
      "df.unpersist()\n",
      "Best practices\n",
      "To use caching and persistence effectively in your Spark applications, consider the following best practices:\n",
      "•\t Cache only what’s necessary: Caching consumes memory, so cache only the data that is \n",
      "frequently used or costly to compute\n",
      "•\t Monitor memory usage: Regularly monitor memory usage to avoid running out of memory \n",
      "or excessive disk spills\n",
      "•\t Automate unpersistence: If you have limited memory resources, automate the unpersistence \n",
      "of less frequently used data to free up memory for more critical operations\n",
      "•\t Consider serialization: Depending on your use case, consider using serialized storage levels \n",
      "to reduce memory overhead\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "In this section, we explored the concepts of persistence and caching in Apache Spark.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.6325\n",
      "Use .persist(StorageLevel.DISK_ONLY).\n",
      "Caching is particularly beneficial in the following scenarios:\n",
      "•\t Iterative algorithms: Caching is vital for iterative algorithms such as machine learning, graph \n",
      "processing, and optimization problems, where the same data is used repeatedly\n",
      "•\t Multiple actions: When a DataFrame is used for multiple actions, caching it after the first \n",
      "action can improve performance\n",
      "•\t Avoiding recomputation: Caching helps avoid recomputing the same data when multiple \n",
      "transformations depend on it\n",
      "•\t Interactive queries: In interactive data exploration or querying, caching frequently used \n",
      "intermediate results can speed up ad hoc analysis\n",
      "Unpersisting data\n",
      "Caching consumes memory, and in a cluster environment, it’s essential to manage memory efficiently. \n",
      "You can release cached data from memory using the .unpersist() method. This method allows \n",
      "you to specify whether to release the data immediately or only when it is no longer needed.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.6092\n",
      "Use .persist(StorageLevel.MEMORY_ONLY_SER).\n",
      "•\t Memory and disk: This option stores data in memory and spills excess data to disk when \n",
      "memory is full. Use .persist(StorageLevel.MEMORY_AND_DISK).\n",
      "\n",
      "Persisting and caching in Apache Spark\n",
      "\n",
      "•\t Disk-only: This option stores data only on disk, avoiding memory usage. \n",
      "Use .persist(StorageLevel.DISK_ONLY).\n"
     ]
    }
   ],
   "source": [
    "query = \"How does persist() differ from cache() in PySpark?\"\n",
    "response = run_query_and_inspect(query, query_engine_2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b653f2ec-362f-4a18-a919-64aafba80967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How does persist() differ from cache() in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Prueba con otros prompts también sigue dando el mismo resultado. \n",
    "# Comprobación de temperatura: \n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "response_synthesizer_3 = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    llm=llm,\n",
    "    text_qa_template=qa_template\n",
    ")\n",
    "\n",
    "query_engine_3 = RetrieverQueryEngine(\n",
    "    retriever=retriever, \n",
    "    response_synthesizer=response_synthesizer_3,\n",
    "    node_postprocessors=[]\n",
    ")\n",
    "\n",
    "query = \"How does persist() differ from cache() in PySpark?\"\n",
    "response = run_query_and_inspect(query, query_engine_3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510a27be-ba77-45cc-8e5e-398920cdfe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How does persist() differ from cache() in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La diferencia entre `persist()` y `cache()` en PySpark radica en que `cache()` es una forma específica de `persist()` que almacena datos en memoria. Por otro lado, `persist()` permite especificar diferentes niveles de almacenamiento, que pueden incluir tanto almacenamiento en memoria como en disco. En resumen, `cache()` es un método que utiliza el nivel de almacenamiento `MEMORY_ONLY`, mientras que `persist()` puede utilizar varios niveles de almacenamiento, incluyendo opciones como `MEMORY_ONLY`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK`, y `DISK_ONLY`.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Prueba con otros prompts también sigue dando el mismo resultado. \n",
    "# Comprobación de response_mode\n",
    "system_prompt = \"\"\"\n",
    "Eres un experto en PySpark y Spark, especializado en el temario de la certificación Databricks Certified Associate Developer for Apache Spark.\n",
    "\n",
    "Debes responder única y exclusivamente utilizando la información que se encuentra en los documentos proporcionados como contexto. \n",
    "\n",
    "Si la respuesta a la pregunta no se obtiene claramente del contexto, intenta razonar con el contexto una respuesta útil para responder a la consulta.\n",
    "\n",
    "Si la respuesta a la pregunta no se puede obtener del contexto ni se puede razonar a partir de él, responde exactamente: \"No he encontrado información al respecto.\"\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "qa_template_str = \"\"\"\n",
    "{context_str}\n",
    "Utiliza exclusivamente la información anterior para responder. \n",
    "Pregunta: {query_str}\n",
    "Si no encuentras suficiente información para dar una respuesta clara, responde: 'No he encontrado información al respecto.'\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "qa_template = PromptTemplate(qa_template_str)\n",
    "\n",
    "response_synthesizer_3 = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    llm=llm,\n",
    "    text_qa_template=qa_template\n",
    ")\n",
    "\n",
    "query_engine_3 = RetrieverQueryEngine(\n",
    "    retriever=retriever, \n",
    "    response_synthesizer=response_synthesizer_3,\n",
    "    node_postprocessors=[]\n",
    ")\n",
    "\n",
    "query = \"How does persist() differ from cache() in PySpark?\"\n",
    "response = run_query_and_inspect(query, query_engine_3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe7ff0-13f7-4564-92f9-7977760a0d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b93a5-204a-49d2-82bc-6a2112f6cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer_3 = get_response_synthesizer(\n",
    "    response_mode=\"simple_summarize\",\n",
    "    llm=llm,\n",
    "    text_qa_template=qa_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "320b009b-981c-47f4-9c87-a9e23106219b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: What does collect() function does in pyspark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La función collect() en PySpark recupera todos los elementos del DataFrame o RDD y los devuelve como una lista. Debe usarse con precaución, ya que trae todos los datos al nodo del driver, lo que puede causar errores de falta de memoria si el driver no tiene suficiente memoria para contener los datos procesados.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can we control the intern process in a Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the under-the-hood workflow in a Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: El flujo de trabajo en una aplicación Spark, desde la presentación de un trabajo hasta la liberación de recursos, se puede resumir en los siguientes pasos:\n",
      "\n",
      "1. El proceso comienza cuando un usuario envía una solicitud `spark-submit` al motor de Spark, creando así una aplicación Spark. Una vez que se realiza una acción, se genera un trabajo.\n",
      "2. Spark Core controla todas las funcionalidades y características de Spark, proporcionando capacidades de computación en memoria, un modelo de ejecución generalizado y APIs para Java, Scala y Python.\n",
      "3. La solicitud inicia la comunicación con el administrador del clúster, que a su vez inicializa el Spark driver para ejecutar el método `main()` de la aplicación Spark. Se crea un `SparkSession` para ejecutar este método.\n",
      "4. El driver se comunica con el administrador del clúster para solicitar recursos para la ejecución.\n",
      "5. El administrador del clúster inicia los ejecutores, que pueden comunicarse directamente con el driver.\n",
      "6. El driver crea un plan lógico, conocido como un grafo acíclico dirigido (DAG), y un plan físico para la ejecución basado en el número total de tareas que deben ejecutarse.\n",
      "7. El driver también divide los datos que se ejecutarán en cada ejecutor, junto con las tareas.\n",
      "8. Una vez que cada tarea termina de ejecutarse, el driver recibe los resultados.\n",
      "9. El driver mantiene comunicación constante con los ejecutores y se ejecuta en el nodo maestro de una máquina o clúster.\n",
      "\n",
      "Este flujo asegura que la aplicación se ejecute de manera eficiente y organizada.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the role of the Driver component?\n",
      "============================================================\n",
      "\n",
      "Respuesta: El rol del componente Driver en Spark es ser el núcleo de las computaciones inteligentes y eficientes. Actúa como el maestro en la arquitectura maestro-esclavo, manteniendo el control y conocimiento de todos los ejecutores en todo momento. Es responsable de dividir la aplicación en tareas más pequeñas, decidir qué datos procesará cada ejecutor y programar las tareas en los nodos ejecutores con la ayuda del administrador del clúster. Además, el Driver mantiene la comunicación constante con los ejecutores, gestiona la asignación de tareas y asegura la tolerancia a fallos al poder reasignar tareas en caso de que un ejecutor falle o tarde demasiado en ejecutar una tarea. También mantiene metadatos sobre el Resilient Distributed Dataset (RDD) y sus particiones. En resumen, el Driver orquesta la ejecución de la aplicación Spark, coordinando la creación de SparkContext y la distribución de tareas a los nodos trabajadores.\n",
      "============================================================\n",
      "\n",
      "Consulta: How is the execution hierarchy organized in Spark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La jerarquía de ejecución en Spark se organiza de la siguiente manera:\n",
      "\n",
      "1. Un usuario envía una solicitud de `spark-submit` al motor de Spark, creando así una aplicación Spark. Una vez que se realiza una acción, se genera un trabajo.\n",
      "2. Spark construye un mapa lógico de las consultas y planifica el mejor camino de ejecución. La ejecución no comienza hasta que se encuentra una declaración de \"acción\".\n",
      "3. El controlador (Spark driver) se encarga de diseñar el mapa de ejecución completo, determinando qué tareas se ejecutan en qué ejecutores y cómo se distribuye la data.\n",
      "4. El controlador crea un plan lógico, conocido como un grafo acíclico dirigido (DAG), y un plan físico para la ejecución basado en el número total de tareas requeridas.\n",
      "5. El controlador también divide los datos que se ejecutarán en cada ejecutor, junto con las tareas.\n",
      "6. Una vez que cada tarea termina de ejecutarse, el controlador recibe los resultados.\n",
      "7. Cuando el programa finaliza, el método `main()` sale y Spark libera todos los recursos de los ejecutores y del controlador.\n",
      "\n",
      "Esta secuencia describe cómo se organiza la ejecución de un trabajo en Spark desde la presentación de la solicitud hasta la liberación de recursos.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I get the number of rows in a Spark DataFrame?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Para obtener el número de filas en un DataFrame de Spark, puedes utilizar el método `count()`. Por ejemplo, puedes ejecutar `data_df.count()` y como resultado verás el total de filas en el DataFrame.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I create a DataFrame in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Para crear un DataFrame en PySpark, puedes utilizar la función `pyspark.sql.SparkSession.createDataFrame`. Puedes crear el DataFrame a partir de listas, listas de listas, tuplas, diccionarios, DataFrames de Pandas, RDDs y `pyspark.sql.Rows`. Además, puedes especificar el esquema del DataFrame utilizando el argumento `schema`.\n",
      "\n",
      "Aquí hay un ejemplo de cómo crear un DataFrame utilizando tuplas:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import Row\n",
      "rdd = spark.sparkContext.parallelize([\n",
      "    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
      "    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
      "    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
      "])\n",
      "\n",
      "data_df = spark.createDataFrame([\n",
      "    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n",
      "    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n",
      "    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n",
      "])\n",
      "```\n",
      "\n",
      "Recuerda que también puedes asignar el DataFrame resultante a un nuevo DataFrame o sobrescribir el DataFrame original.\n",
      "============================================================\n",
      "\n",
      "Consulta: What is the optimized way to perform joins in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La forma optimizada de realizar joins en PySpark incluye el uso de dos enfoques fundamentales: shuffle joins y broadcast joins. \n",
      "\n",
      "1. **Shuffle Joins**: Son adecuados para uniones de grandes conjuntos de datos y redistribuyen los datos a través de las particiones para asegurar que las claves coincidentes estén en los mismos nodos de trabajo. Para optimizar el rendimiento de los shuffle joins, se pueden emplear:\n",
      "   - **Sort-merge joins**: Utilizan algoritmos de unión por orden de mezcla para minimizar el movimiento de datos durante las operaciones de unión.\n",
      "   - **AQE (Adaptive Query Execution)**: Permite optimizar dinámicamente las operaciones de shuffle basándose en estadísticas de tiempo de ejecución y distribución de datos.\n",
      "\n",
      "2. **Broadcast Joins**: Son altamente eficientes para unir un DataFrame pequeño con uno más grande, ya que el DataFrame más pequeño se transmite a todos los nodos de trabajo, eliminando la necesidad de mover datos a través de la red.\n",
      "\n",
      "Además, se recomienda seguir las mejores prácticas para gestionar los shuffles, como perfilar y monitorear las operaciones de shuffle, determinar tamaños de partición óptimos, utilizar caché o persistencia para resultados intermedios y realizar ajustes regulares en las configuraciones de Spark relacionadas con las operaciones de shuffle. \n",
      "\n",
      "Implementando estas estrategias, se puede optimizar el rendimiento de las uniones en PySpark.\n",
      "============================================================\n",
      "\n",
      "Consulta: What does a 'broadcast join' means?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Un 'broadcast join' es una técnica de optimización específica que se aplica cuando uno de los DataFrames es lo suficientemente pequeño como para caber en memoria. En este caso, el DataFrame pequeño se transmite (broadcast) a todos los nodos de trabajo, evitando el costoso proceso de redistribución de datos (shuffling). Esto reduce significativamente la sobrecarga de red y el I/O de disco, ya que se evita el shuffling de datos. Los 'broadcast joins' son útiles cuando se unen un DataFrame grande con uno mucho más pequeño, como en el caso de unir una tabla de hechos con tablas de dimensiones en escenarios de almacenamiento de datos.\n",
      "============================================================\n",
      "\n",
      "Consulta: How does persist() differ from cache() in PySpark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La diferencia entre `persist()` y `cache()` en PySpark radica en que `cache()` es una forma específica de `persist()` que almacena datos en memoria. Por otro lado, `persist()` permite especificar diferentes niveles de almacenamiento, que pueden incluir tanto almacenamiento en memoria como en disco. En resumen, `cache()` es un método que utiliza el nivel de almacenamiento `MEMORY_ONLY`, mientras que `persist()` puede utilizar varios niveles de almacenamiento según las necesidades del usuario.\n",
      "============================================================\n",
      "\n",
      "Consulta: How can I select specific columns from a PySpark DataFrame?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Para seleccionar columnas específicas de un DataFrame en PySpark, se puede utilizar la función `select()`. Aquí hay algunos ejemplos de cómo hacerlo:\n",
      "\n",
      "1. Usando el nombre de la columna como una cadena:\n",
      "   ```python\n",
      "   data_df.select('col_3').show()\n",
      "   ```\n",
      "\n",
      "2. Usando la notación de acceso a atributos:\n",
      "   ```python\n",
      "   data_df.select(data_df['col_3']).show()\n",
      "   ```\n",
      "\n",
      "3. También se puede usar la notación de punto:\n",
      "   ```python\n",
      "   data_df.select(data_df.col_3).show()\n",
      "   ```\n",
      "\n",
      "Estos métodos mostrarán solo la columna seleccionada del DataFrame.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    response = run_query_and_inspect(query, query_engine_3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a7904e5-548a-4652-92a7-bd6dff105e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How can we control the intern process in a Spark application?\n",
      "============================================================\n",
      "\n",
      "Respuesta: No he encontrado información al respecto.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.5720\n",
      "This access is \n",
      "governed and controlled by a process known as the cluster manager. It is the responsibility of the cluster \n",
      "manager to allocate computing resources for the Spark application when the application execution \n",
      "starts. These resources become available at the request of the application master. In the Apache Spark \n",
      "ecosystem, the application master plays a crucial role in managing and coordinating the execution \n",
      "of Spark applications within a distributed cluster environment. It’s an essential component that’s \n",
      "responsible for negotiating resources, scheduling tasks, and monitoring the application’s execution.\n",
      "Once the resources are available, the driver is made aware of those resources. It’s the responsibility of \n",
      "the driver to manage these resources based on tasks that need to be executed by the Spark application. \n",
      "Once the application has finished execution, these resources are released back to the cluster manager.\n",
      "Applications have their dedicated executor processes that parallelize how tasks are run.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.5421\n",
      "Applications have their dedicated executor processes that parallelize how tasks are run. The advantage \n",
      "is that each application is independent of the other and runs on its own schedule. Data also becomes \n",
      "independent for each of these applications, so data sharing can only take place by writing data to disk \n",
      "so that it can be shared across applications.\n",
      "Cluster modes\n",
      "Cluster modes define how Spark applications utilize cluster resources, manage task execution, and \n",
      "interact with cluster managers for resource allocation.\n",
      "If there is more than one user sharing resources on the cluster, be it Spark applications or other \n",
      "applications that need cluster resources, they have to be managed based on different modes. There \n",
      "are two types of modes available for cluster managers – standalone client mode and cluster mode. \n",
      "The following table highlights some of the differences between the two:\n",
      "Client Mode\n",
      "Cluster Mode\n",
      "In client mode, the driver program runs on \n",
      "the machine where the Spark application \n",
      "is submitted.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.5420\n",
      "The Spark driver \n",
      "also maintains communication with executors all the time. The driver runs on the master node of a \n",
      "machine or cluster. When a Spark application starts running, the driver keeps up with all the required \n",
      "information that is needed to run the application successfully.\n",
      "As shown in Figure 3.1, the driver node contains SparkSession, which is the entry point of the \n",
      "Spark application. Previously, this was known as the SparkContext object, but in Spark 2.0, \n",
      "SparkSession handles all contexts to start execution. The application’s main method runs on the \n",
      "driver to coordinate the whole application. It runs on its own Java Virtual Machine (JVM). Spark \n",
      "driver can run as an independent process or it can run on one of the worker nodes, depending on \n",
      "the architecture.\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "The Spark driver is responsible for dividing the application into smaller entities for execution.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.5187\n",
      "This \n",
      "reduces the latency by a great deal. This means that there would be less delay in the response time of \n",
      "the processes. Another point to note here is that this is true for the data as well. The executor reading \n",
      "the data close to it would have better performance than otherwise. Ideally, the driver and worker nodes \n",
      "should be run in the same local area network (LAN) for the best performance.\n",
      "The Spark driver also creates a web UI for the execution details. This UI is very helpful in determining \n",
      "the performance of the application. In cases where troubleshooting is required and some bottlenecks \n",
      "need to be identified in the Spark process, this UI is very helpful.\n",
      "SparkSession\n",
      "SparkSession is the main point of entry and interaction with Spark.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.5168\n",
      "All the functionality and features of Spark are controlled by Spark \n",
      "Core. It provides in-memory computing capabilities to deliver speed, a generalized execution model \n",
      "to support a wide variety of applications, and Java, Scala, and Python APIs for ease of development.\n",
      "In all of these different components, you can write queries in supported languages. Spark will then \n",
      "convert these queries to directed acyclic graphs (DAGs), and Spark Core has the responsibility of \n",
      "executing them.\n",
      "The key responsibilities of Spark Core are as follows:\n",
      "•\t Interacting with storage systems\n",
      "•\t Memory management\n",
      "•\t Task distribution\n",
      "\n",
      "What is Apache Spark?\n",
      "\n",
      "•\t Task scheduling\n",
      "•\t Task monitoring\n",
      "•\t In-memory computation\n",
      "•\t Fault tolerance\n",
      "•\t Optimization\n",
      "Spark Core contains an API for RDDs which are an integral part of Spark. It also provides different \n",
      "APIs to interact and work with RDDs.\n"
     ]
    }
   ],
   "source": [
    "# Probar en la otra consulta sin respuesta: \n",
    "query = \"How can we control the intern process in a Spark application?\"\n",
    "response = run_query_and_inspect(query, query_engine_3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e30e3ab-82e9-4ccb-a3e9-bdf30ca2b8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: How does the Catalyst optimizer improve query execution in Spark SQL?\n",
      "============================================================\n",
      "\n",
      "Respuesta: El Catalyst optimizer mejora la ejecución de consultas en Spark SQL mediante el uso de técnicas avanzadas de optimización que transforman el plan de consulta lógico en un plan físico más eficiente. Esto se logra a través de optimizaciones basadas en reglas, que aplican un conjunto de reglas específicas para mejorar aspectos como el empuje de predicados, la plegadura de constantes y la poda de columnas. Además, el optimizador utiliza optimización basada en costos, que estima el costo de diferentes planes de ejecución considerando factores como la distribución de datos, estrategias de unión y recursos disponibles. Esto permite a Spark elegir el plan más eficiente basado en las características reales de ejecución.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.7645\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "Apache Spark is well-known for its powerful optimization capabilities, which significantly enhance \n",
      "the performance of distributed data processing tasks. At the heart of this optimization framework lies \n",
      "the Catalyst optimizer, an integral component that plays a pivotal role in enhancing query execution \n",
      "efficiency. This is achieved before the query is executed.\n",
      "The Catalyst optimizer works primarily on static optimization plans that are generated during query \n",
      "compilation. However, AQE, which was introduced in Spark 3.0, is a dynamic and adaptive approach to \n",
      "optimizing query plans at runtime based on the actual data characteristics and execution environment. \n",
      "We will learn more about both these paradigms in the next section.\n",
      "Catalyst optimizer\n",
      "The Catalyst optimizer is an essential part of Apache Spark’s query execution engine. It is a powerful \n",
      "tool that uses advanced techniques to optimize query plans, thus improving the performance of Spark \n",
      "applications.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.7284\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "This section provided a solid introduction to the Catalyst optimizer, its components, and a practical \n",
      "example. You can expand on this foundation by delving deeper into rule-based and cost-based \n",
      "optimization techniques, as well as discussing real-world scenarios where the Catalyst optimizer can \n",
      "have a substantial impact on query performance.\n",
      "Next, we will see how AQE takes optimizations to the next level in Spark.\n",
      "Adaptive Query Execution (AQE)\n",
      "Apache Spark, a powerful distributed computing framework, offers a multitude of optimization \n",
      "techniques to enhance the performance of data processing jobs. One such advanced optimization \n",
      "feature is AQE, a dynamic approach that significantly improves query processing efficiency.\n",
      "AQE dynamically adjusts execution plans during runtime based on actual data statistics and hardware \n",
      "conditions. It collects and utilizes runtime statistics to optimize join strategies, partitioning methods, \n",
      "and broadcast operations.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.7043\n",
      "This is particularly useful when \n",
      "dealing with complex queries.\n",
      "Let’s take a look at the different components that make up the Catalyst optimizer.\n",
      "Catalyst optimizer components\n",
      "To gain a deeper understanding of the Catalyst optimizer, it’s essential to examine its core components.\n",
      "Logical query plan\n",
      "The logical query plan represents the high-level, abstract structure of a query. It defines what you \n",
      "want to accomplish without specifying how to achieve it. Spark’s Catalyst optimizer works with this \n",
      "logical plan to determine the optimal physical plan.\n",
      "\n",
      "Optimizations in Apache Spark\n",
      "\n",
      "Rule-based optimization\n",
      "Rule-based optimization is the backbone of the Catalyst optimizer. It comprises a set of rules that \n",
      "transform the logical query plan into a more efficient version. Each rule focuses on a specific aspect \n",
      "of optimization, such as predicate pushdown, constant folding, or column pruning.\n",
      "Physical query plan\n",
      "The physical query plan defines how to execute the query.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.6942\n",
      "Physical query plan\n",
      "The physical query plan defines how to execute the query. Once the logical plan is optimized using \n",
      "rule-based techniques, it’s converted into a physical plan, taking into account the available resources \n",
      "and the execution environment. This phase ensures that the plan is executable in a distributed and \n",
      "parallel manner.\n",
      "Cost-based optimization\n",
      "In addition to rule-based optimization, the Catalyst optimizer can use cost-based optimization. It \n",
      "estimates the cost of different execution plans, taking into account factors such as data distribution, \n",
      "join strategies, and available resources. This approach helps Spark choose the most efficient plan based \n",
      "on actual execution characteristics.\n",
      "Catalyst optimizer in action\n",
      "To witness the Catalyst optimizer in action, let’s consider a practical example using Spark’s SQL API.\n",
      "In this code example, we’re loading data from a CSV file, applying a selection operation to pick specific \n",
      "columns, and filtering rows based on a condition.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.6907\n",
      "•\t Using Spark SQL for different operations\n",
      "•\t The Catalyst optimizer, a pivotal component in Spark’s query execution engine that employs \n",
      "rule-based and cost-based optimizations to enhance query performance\n",
      "•\t The distinction between narrow and wide transformations in Spark and when to use each type \n",
      "to achieve optimal parallelism and resource efficiency\n",
      "•\t Data persistence and caching techniques to reduce recomputation and expedite data processing, \n",
      "with best practices for efficient memory management\n",
      "•\t Data partitioning through repartition and coalesce, and how to use these operations to balance \n",
      "workloads and optimize data distribution\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "•\t User-defined functions (UDFs) and custom functions, which allow you to implement specialized \n",
      "data processing logic,\n",
      "\n",
      "Consulta: What is the difference between repartitioning and coalescing in Spark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La diferencia entre repartitioning y coalescing en Spark radica en cómo manejan la distribución de datos a través de las particiones:\n",
      "\n",
      "- **Repartitioning**: Se utiliza para aumentar o redistribuir el número de particiones, especialmente cuando los datos están desigualmente distribuidos, lo que puede causar cargas de trabajo sesgadas. Repartitioning puede ser beneficioso para optimizar operaciones de unión al minimizar el movimiento de datos. Este método implica un mayor costo de reordenamiento de datos, ya que puede requerir un shuffle completo.\n",
      "\n",
      "- **Coalescing**: Se utiliza para reducir el número de particiones mientras se preserva la localidad de los datos. Es más eficiente que repartitioning porque minimiza el movimiento de datos al fusionar particiones localmente siempre que sea posible. Coalescing es ideal cuando se necesita reducir el número de particiones sin incurrir en el costo completo del shuffle de datos.\n",
      "\n",
      "En resumen, repartitioning es útil para equilibrar la carga de trabajo y optimizar uniones, mientras que coalescing es más eficiente para reducir particiones y minimizar el shuffle de datos.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.7701\n",
      "Caching and \n",
      "persistence are powerful techniques for optimizing performance in Spark applications, particularly when \n",
      "dealing with iterative algorithms or scenarios where the same data is used repeatedly. Understanding when \n",
      "and how to use these techniques can significantly improve the efficiency of your data processing workflows.\n",
      "In the next section, we’ll learn how repartition and coalesce work in Spark.\n",
      "Repartitioning and coalescing in Apache Spark\n",
      "Efficient data partitioning plays a crucial role in optimizing data processing workflows in Apache \n",
      "Spark. Repartitioning and coalescing are operations that allow you to control the distribution of data \n",
      "across partitions. In this section, we’ll explore the concepts of repartitioning and coalescing and their \n",
      "significance in Spark applications.\n",
      "Understanding data partitioning\n",
      "Data partitioning in Apache Spark involves dividing a dataset into smaller, manageable units called \n",
      "partitions.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.7370\n",
      "You can use the .coalesce() method to specify the target number of partitions.\n",
      "Here are some key points related to coalescing data:\n",
      "•\t Decreasing partitions: Coalescing is used when you want to decrease the number of partitions \n",
      "to optimize data processing\n",
      "•\t Minimizing data movement: Unlike repartitioning, coalescing minimizes data shuffling by \n",
      "merging partitions locally whenever possible\n",
      "•\t Efficient for data reduction: Coalescing is efficient when you need to reduce the number of \n",
      "partitions without incurring the full cost of data shuffling\n",
      "Here’s an example of coalescing data:\n",
      "# Coalesce a DataFrame to 4 partitions\n",
      "df.coalesce(4)\n",
      "Use cases for repartitioning and coalescing\n",
      "Understanding when to repartition and coalesce is critical for optimizing your Spark applications.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.7309\n",
      "Therefore, it should be used judiciously.\n",
      "•\t Even data distribution: Repartitioning is useful when the original data is unevenly distributed \n",
      "across partitions, causing skewed workloads.\n",
      "•\t Optimizing for joins: Repartitioning can be beneficial when performing joins to minimize \n",
      "data shuffling.\n",
      "Here’s an example of repartitioning data:\n",
      "# Repartition a DataFrame into 8 partitions\n",
      "df.repartition(8)\n",
      "\n",
      "Repartitioning and coalescing in Apache Spark\n",
      "\n",
      "Coalescing data\n",
      "Coalescing is the process of reducing the number of partitions while preserving data locality. It is a \n",
      "more efficient operation than repartitioning because it avoids unnecessary data shuffling whenever \n",
      "possible. You can use the .coalesce() method to specify the target number of partitions.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.7206\n",
      "The following are some use cases for repartitioning:\n",
      "•\t Data skew: When data is skewed across partitions, repartitioning can balance the workload\n",
      "•\t Join optimization: For optimizing join operations by ensuring that the joining keys are collocated\n",
      "•\t Parallelism control: Adjusting the level of parallelism to optimize resource utilization\n",
      "Now, let’s look at some use cases for coalescing:\n",
      "•\t Reducing data: When you need to reduce the number of partitions to save memory and \n",
      "reduce overhead\n",
      "•\t Minimizing shuffling: To avoid unnecessary data shuffling and minimize network communication\n",
      "•\t Post-filtering: After applying a filter or transformation that significantly reduces the dataset size\n",
      "Best practices\n",
      "To repartition and coalesce effectively in your Spark applications, consider these best practices:\n",
      "•\t Profile and monitor: Profile your application to identify performance bottlenecks related to \n",
      "data partitioning.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.6588\n",
      "Use Spark’s UI and monitoring tools to track data shuffling.\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "•\t Consider data size: Consider the size of your dataset and the available cluster resources when \n",
      "deciding on the number of partitions.\n",
      "•\t Balance workloads: Aim for a balanced workload distribution across partitions to \n",
      "optimize parallelism.\n",
      "•\t Coalesce where possible: When reducing the number of partitions, prefer coalescing over \n",
      "repartitioning to minimize data shuffling.\n",
      "•\t Plan for joins: When performing joins, plan for the optimal number of partitions to minimize \n",
      "shuffle overhead.\n",
      "In this section, we explored the concepts of repartitioning and coalescing in Apache Spark. Understanding \n",
      "how to efficiently control data partitioning can significantly impact the performance of your Spark \n",
      "applications, especially when you’re working with large datasets and complex operations.\n",
      "\n",
      "Consulta: How does watermarking help with late data in Structured Streaming?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Watermarking helps with late data in Structured Streaming by providing a threshold timestamp that indicates the maximum event time seen by the system up to a certain point. This mechanism allows the system to track the progress of event time and determine when it is safe to emit results for a specific window. By using watermarks, Structured Streaming can effectively handle delayed or late-arriving data, enabling the system to manage event time and make decisions about processing late data, such as discarding it, updating existing results, or storing it separately for further analysis.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.7718\n",
      "Based on that, the strategy for data \n",
      "processing can be determined.\n",
      "Watermarking and late data handling\n",
      "Now, we will discuss how to handle data that doesn’t arrive at the defined time in real-time applications. \n",
      "There are different ways to handle that situation. Structured Streaming has a built-in mechanism to \n",
      "handle this type of data. These mechanisms include:\n",
      "•\t Watermarking: Watermarking is a mechanism in Structured Streaming used to deal with \n",
      "event time and handle delayed or late-arriving data. A watermark is a threshold timestamp \n",
      "that indicates the maximum event time seen by a system up to a certain point. It allows the \n",
      "system to track the progress of event time and determine when it is safe to emit results for a \n",
      "specific window.\n",
      "•\t Late data handling: Late-arriving data refers to events that have timestamps beyond the \n",
      "watermark threshold.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.6210\n",
      "Structured Streaming provides options to handle late data, such as \n",
      "discarding it, updating existing results, or storing it separately for further analysis.\n",
      "These built-in mechanisms save users a lot of time and efficiently handle late-arriving data.\n",
      "Next, we will see, once the data arrives, how we start the operations on it in streaming.\n",
      "Triggers and output modes\n",
      "Triggers determine when a streaming application should emit results or trigger the execution of the \n",
      "computation. Structured Streaming supports different types of triggers:\n",
      "•\t Event time triggers: Event time triggers operate based on the arrival of new events or when \n",
      "a watermark advances beyond a certain threshold. They enable more accurate and efficient \n",
      "processing, based on event time semantics.\n",
      "•\t Processing time triggers: These triggers operate based on processing time, allowing you to \n",
      "specify time intervals or durations at which the computation should be executed.\n",
      "\n",
      "Structured Streaming in Spark\n",
      "\n",
      "Structured Streaming also offers different output modes.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.6023\n",
      "Structured Streaming, \n",
      "on the other hand, provides exactly-once processing semantics, ensuring that each event is \n",
      "processed exactly once, even in the presence of failures or retries.\n",
      "\n",
      "Streaming fundamentals\n",
      "\n",
      "Limitations and considerations\n",
      "While Structured Streaming offers significant advantages, there are certain limitations and considerations \n",
      "to keep in mind:\n",
      "•\t Event time handling: Proper handling of event time, such as timestamp extraction, watermarking, \n",
      "and late data handling, is essential in Structured Streaming. Care should be taken to ensure the \n",
      "correct processing and handling of out-of-order events.\n",
      "•\t State management: Structured Streaming allows you to maintain stateful information \n",
      "across batches, which can introduce challenges related to state management and scalability. \n",
      "Monitoring memory usage and configuring appropriate state retention policies are crucial for \n",
      "optimal performance.\n",
      "•\t Ecosystem compatibility: While Structured Streaming integrates well with the Spark ecosystem, \n",
      "certain libraries and features might not be fully compatible with real-time streaming use cases.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.5644\n",
      "Some of them are as follows.\n",
      "Handling fault tolerance\n",
      "Fault tolerance is crucial in streaming systems to ensure data integrity and reliability. Structured \n",
      "Streaming provides built-in fault tolerance mechanisms to handle failures in both streaming sources \n",
      "and sinks:\n",
      "•\t Source fault tolerance: Structured Streaming ensures end-to-end fault tolerance in sources, by \n",
      "tracking the progress of event time using watermarks and checkpointing the metadata related \n",
      "to the stream. If there are failures, the system can recover and resume processing from the last \n",
      "consistent state.\n",
      "•\t Sink fault tolerance: Fault tolerance in sinks depends on the guarantees provided by the specific \n",
      "sink implementation. Some sinks may inherently provide exactly-once semantics, while others \n",
      "may rely on idempotent writes or deduplication techniques to achieve at-least-once semantics. \n",
      "Sink implementations should be carefully chosen to ensure data consistency and reliability.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.5559\n",
      "There are instances when you would need to write the \n",
      "data back to a system that might not support streaming. It could be a database or a file-based storage \n",
      "system. Custom streaming sinks enable integration with external systems or databases that are not \n",
      "supported by the built-in sinks.\n",
      "\n",
      "Advanced techniques in Structured Streaming\n",
      "\n",
      "When implementing a custom streaming sink, developers need to define how output data is written \n",
      "or processed by the external system. This may involve establishing connections, handling batching \n",
      "or buffering, and ensuring fault tolerance and exactly-once semantics.\n",
      "In the next section, we will talk about advanced techniques in Structured Streaming.\n",
      "Advanced techniques in Structured Streaming\n",
      "There are certain built-in capabilities of Structured Streaming that makes it the default choice for even \n",
      "some batch operations. Instead of architecting things yourself, Structured Streaming handles these \n",
      "properties for you. Some of them are as follows.\n",
      "\n",
      "Consulta: What are the key stages of the machine learning life cycle in Spark ML?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Los key stages of the machine learning life cycle en Spark ML son:\n",
      "\n",
      "1. **Problem definition**: Definir claramente el problema que se desea resolver y entender los objetivos del proyecto de ML.\n",
      "\n",
      "Además, se menciona que el ciclo de vida de ML implica varias etapas, cada una con sus propias tareas y consideraciones, aunque no se detallan más etapas específicas en el contexto proporcionado.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.7613\n",
      "As we’ve seen, \n",
      "Spark MLlib is the original library with a rich set of distributed algorithms, while Spark ML is a newer \n",
      "library with a more user-friendly API and integration with DataFrames. The choice between the two \n",
      "depends on your Spark version, preference for API style, and specific requirements of your ML tasks.\n",
      "\n",
      "Machine Learning with Spark ML\n",
      "\n",
      "ML life cycle\n",
      "The ML life cycle encompasses the end-to-end process of developing and deploying ML models. It \n",
      "involves several stages, each with its own set of tasks and considerations. Understanding the ML life \n",
      "cycle is crucial for building robust and successful ML solutions. In this section, we will explore the \n",
      "key stages of the ML life cycle:\n",
      "1.\t\n",
      "Problem definition: The first stage of the ML life cycle is problem definition. It involves clearly \n",
      "defining the problem you want to solve and understanding the goals and objectives of your \n",
      "ML project.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.6999\n",
      "We will cover the following topics:\n",
      "•\t Key concepts in ML\n",
      "•\t Different types of ML\n",
      "•\t ML with Spark\n",
      "•\t Considering the ML life cycle with the help of a real-world example\n",
      "•\t Different case studies for ML\n",
      "•\t Future trends in Spark ML and distributed ML\n",
      "ML encompasses diverse methodologies tailored to different data scenarios. We will start by learning \n",
      "about different key concepts in ML.\n",
      "Introduction to ML\n",
      "ML is a field of study that focuses on the development of algorithms and models that enable computer \n",
      "systems to learn and make predictions or decisions without being explicitly programmed. It is a subset \n",
      "of artificial intelligence (AI) that aims to provide systems with the ability to automatically learn and \n",
      "improve from data and experience.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.6758\n",
      "Machine Learning with Spark ML\n",
      "\n",
      "In today’s world, where vast amounts of data are being generated at an unprecedented rate, ML plays a \n",
      "critical role in extracting meaningful insights, making accurate predictions, and automating decision-\n",
      "making processes. As data grows, machines can learn the patterns better, thus making it even easier \n",
      "to gain insights from this data. It finds applications in various domains, including finance, healthcare, \n",
      "marketing, image and speech recognition, recommendation systems, and many more.\n",
      "The key concepts of ML\n",
      "To understand ML, it is important to grasp the fundamental concepts that underpin its methodology.\n",
      "Data\n",
      "Data is the foundation of any ML process. It can be structured, semi-structured, or unstructured and \n",
      "encompasses various types, such as numerical, categorical, text, images, and more. ML algorithms \n",
      "require high-quality, relevant, and representative data to learn patterns and make accurate predictions.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.6626\n",
      "We also compared Spark MLlib \n",
      "and Spark ML, highlighting their respective features and use cases.\n",
      "Throughout this chapter, we discussed various key concepts and techniques related to Spark ML. \n",
      "We explored different types of ML, such as classification, regression, time series analysis, supervised \n",
      "learning, and unsupervised learning. We highlighted the importance of data preparation and feature \n",
      "engineering in building effective ML pipelines. We also touched upon fault-tolerance and reliability \n",
      "aspects in Spark ML, ensuring robustness and data integrity.\n",
      "Furthermore, we examined real-world use cases, including customer churn prediction and fraud \n",
      "detection, to demonstrate the practical applications of Spark ML in solving complex business challenges. \n",
      "\n",
      "Machine Learning with Spark ML\n",
      "\n",
      "These case studies showcased how organizations can leverage Spark ML to enhance decision-making, \n",
      "improve customer retention, and mitigate financial risks.\n",
      "As you continue your journey in ML with Spark ML, it is important to keep the iterative and dynamic \n",
      "nature of the field in mind.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.6380\n",
      "With changing data, the model also needs to be updated from time to time. \n",
      "To get a sense of when a model needs to be updated, we need to have monitoring in place for models as \n",
      "well. This monitoring would constantly compare old model results with the new data and see whether \n",
      "model performance is degrading. If that’s the case, it’s time to update the model.\n",
      "This whole life cycle of model deployment is typically the responsibility of machine learning engineers. \n",
      "Note that the process would slightly vary for different problems, but the overall idea remains the same.\n",
      "Summary\n",
      "In this chapter, we learned about the basics of Apache Spark and why Spark is becoming a lot more \n",
      "prevalent in the industry for big data applications. We also learned about the different components of \n",
      "Spark and how these components are helpful in terms of application development.\n",
      "\n",
      "Consulta: What are the different storage levels available when persisting data in Spark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: Los diferentes niveles de almacenamiento disponibles al persistir datos en Spark son:\n",
      "\n",
      "1. **MEMORY_ONLY**: Almacena datos en memoria, pero no los replica para tolerancia a fallos.\n",
      "2. **MEMORY_ONLY_SER**: Almacena datos en memoria en forma serializada, reduciendo el uso de memoria.\n",
      "3. **MEMORY_AND_DISK**: Almacena datos en memoria y derrama los datos excedentes en disco cuando la memoria está llena.\n",
      "4. **DISK_ONLY**: Almacena datos solo en disco, evitando el uso de memoria.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.7691\n",
      "Use .persist(StorageLevel.MEMORY_ONLY_SER).\n",
      "•\t Memory and disk: This option stores data in memory and spills excess data to disk when \n",
      "memory is full. Use .persist(StorageLevel.MEMORY_AND_DISK).\n",
      "\n",
      "Persisting and caching in Apache Spark\n",
      "\n",
      "•\t Disk-only: This option stores data only on disk, avoiding memory usage. \n",
      "Use .persist(StorageLevel.DISK_ONLY).\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.7341\n",
      "Caching and persistence are techniques that \n",
      "allow you to store intermediate or frequently used data in memory or on disk, reducing the need for \n",
      "recomputation and enhancing overall performance. This section explores the concepts of persisting \n",
      "and caching in Spark.\n",
      "Understanding data persistence\n",
      "Data persistence is the process of storing the intermediate or final results of Spark transformations \n",
      "in memory or on disk. By persisting data, you reduce the need to recompute it from the source data, \n",
      "thereby improving query performance.\n",
      "The following key concepts are related to data persistence:\n",
      "•\t Storage levels: Spark offers multiple storage levels for data, ranging from memory-only to \n",
      "disk, depending on your needs. Each storage level comes with its trade-offs in terms of speed \n",
      "and durability.\n",
      "•\t Lazy evaluation: Spark follows a lazy evaluation model, meaning transformations are not \n",
      "executed until an action is called. Data persistence ensures that the intermediate results are \n",
      "available for reuse without recomputation.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.6976\n",
      "Data persistence ensures that the intermediate results are \n",
      "available for reuse without recomputation.\n",
      "•\t Caching versus persistence: Caching is a specific form of data persistence that stores data in \n",
      "memory, while persistence encompasses both in-memory and on-disk storage.\n",
      "Caching data\n",
      "Caching is a form of data persistence that stores DataFrames, RDDs, or datasets in memory for fast \n",
      "access. It is an essential optimization technique that improves the performance of Spark applications, \n",
      "particularly when dealing with iterative algorithms or repeated computations.\n",
      "To cache a DataFrame or an RDD, you can use the .cache() or .persist() method while \n",
      "specifying the storage level:\n",
      "•\t Memory-only: This option stores data in memory but does not replicate it for fault tolerance. \n",
      "Use .cache() or .persist(StorageLevel.MEMORY_ONLY).\n",
      "•\t Memory-only, serialized: This option stores data in memory in a serialized form, reducing \n",
      "memory usage.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.6511\n",
      "Here’s an example of unpersisting data:\n",
      "# Cache a DataFrame\n",
      "df.cache()\n",
      "# Unpersist the cached DataFrame\n",
      "df.unpersist()\n",
      "Best practices\n",
      "To use caching and persistence effectively in your Spark applications, consider the following best practices:\n",
      "•\t Cache only what’s necessary: Caching consumes memory, so cache only the data that is \n",
      "frequently used or costly to compute\n",
      "•\t Monitor memory usage: Regularly monitor memory usage to avoid running out of memory \n",
      "or excessive disk spills\n",
      "•\t Automate unpersistence: If you have limited memory resources, automate the unpersistence \n",
      "of less frequently used data to free up memory for more critical operations\n",
      "•\t Consider serialization: Depending on your use case, consider using serialized storage levels \n",
      "to reduce memory overhead\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "In this section, we explored the concepts of persistence and caching in Apache Spark.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.6286\n",
      "Remember, Spark is an in-memory \n",
      "framework, which means that all the operations take place in the memory of the compute or cluster. \n",
      "Once these operations are completed, we’ll want to write that data to disk. Similarly, before we \n",
      "manipulate any data, we’ll likely need to read data from disk as well.\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "There are several data formats that Spark supports for reading and writing different types of data files. \n",
      "We will discuss the following formats in this chapter.\n",
      "•\t Comma Separated Values (CSV)\n",
      "•\t Parquet\n",
      "•\t Optimized Row Columnar (ORC)\n",
      "Please note that these are not the only formats that Spark supports but this is a very popular subset of \n",
      "formats. A lot of other formats are also supported by Spark, such as Avro, text, JDBC, Delta, and others.\n",
      "\n",
      "Consulta: What is the difference between narrow and wide transformations in Spark?\n",
      "============================================================\n",
      "\n",
      "Respuesta: La diferencia entre las transformaciones estrechas (narrow) y amplias (wide) en Spark radica en cómo manejan el movimiento de datos y su impacto en el rendimiento:\n",
      "\n",
      "1. **Movimiento de datos**: \n",
      "   - Las transformaciones estrechas procesan datos dentro de las particiones localmente, minimizando el movimiento de datos. \n",
      "   - Las transformaciones amplias, por otro lado, implican el \"shuffling\" de datos y el movimiento a través de particiones.\n",
      "\n",
      "2. **Impacto en el rendimiento**: \n",
      "   - Las transformaciones estrechas suelen ofrecer un rendimiento más alto debido a la reducción del movimiento de datos.\n",
      "   - Las transformaciones amplias introducen sobrecarga adicional debido al \"shuffling\" de datos, lo que puede afectar el rendimiento.\n",
      "\n",
      "3. **Alcance de paralelismo**: \n",
      "   - Las transformaciones estrechas permiten un máximo paralelismo dentro de las particiones.\n",
      "   - Las transformaciones amplias pueden limitar el paralelismo debido a la dependencia de múltiples particiones.\n",
      "\n",
      "En resumen, las transformaciones estrechas son más eficientes y rápidas, mientras que las transformaciones amplias son necesarias para ciertas operaciones, aunque conllevan un costo de rendimiento.\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.8064\n",
      "In \n",
      "the next section, we will cover different types of transformations that exist in Spark.\n",
      "Narrow and wide transformations in Apache Spark\n",
      "As discussed in Chapter 3, transformations are the core operations for processing data. Transformations \n",
      "are categorized into two main types: narrow transformations and wide transformations. Understanding \n",
      "the distinction between these two types of transformations is essential for optimizing the performance \n",
      "of your Spark applications.\n",
      "Narrow transformations\n",
      "Narrow transformations are operations that do not require data shuffling or extensive data movement \n",
      "across partitions. They can be executed on a single partition without the need to communicate with other \n",
      "partitions. This inherent locality makes narrow transformations highly efficient and faster to execute.\n",
      "The following are some of the key characteristics of narrow transformations:\n",
      "•\t Single-partition processing: Narrow transformations operate on a single partition of the data \n",
      "independently, which minimizes communication overhead.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.7995\n",
      "join, and sortByKey are typical examples of \n",
      "wide transformations\n",
      "Now, let’s look at their significance:\n",
      "•\t Network and disk overhead: Wide transformations introduce network and disk overhead due \n",
      "to data shuffling, impacting performance\n",
      "•\t Stage boundary creation: They define stage boundaries within a Spark job, resulting in distinct \n",
      "stages during job execution\n",
      "The following are the differences between narrow and wide transformations:\n",
      "•\t Data movement: Narrow transformations process data within partitions locally, minimizing data \n",
      "movement, while wide transformations involve data shuffling and movement across partitions\n",
      "•\t Performance impact: Narrow transformations typically offer higher performance due to reduced \n",
      "data movement, whereas wide transformations involve additional overhead due to data shuffling\n",
      "•\t Parallelism scope: Narrow transformations enable maximum parallelism within partitions, \n",
      "while wide transformations might limit parallelism due to dependency on multiple partitions\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "In Apache Spark, understanding the distinction between narrow and wide transformations is crucial.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.7795\n",
      "Let’s discuss which transformation works best, depending on the operation.\n",
      "\n",
      "Narrow and wide transformations in Apache Spark\n",
      "\n",
      "Choosing between narrow and wide transformations\n",
      "Selecting the appropriate type of transformation depends on the specific use case and the data at hand. \n",
      "Here are some considerations for choosing between narrow and wide transformations:\n",
      "•\t Data size: If your data is small enough to fit comfortably within a single partition, it’s preferable \n",
      "to use narrow transformations. This minimizes the overhead associated with shuffling.\n",
      "•\t Data distribution: If your data is distributed unevenly across partitions, wide transformations \n",
      "might be necessary to reorganize and balance the data.\n",
      "•\t Performance: Narrow transformations are typically faster and more efficient, so if performance \n",
      "is a critical concern, they are preferred.\n",
      "•\t Complex operations: Some operations, such as joining large DataFrames, often require wide \n",
      "transformations. In such cases, the performance trade-off is inevitable.\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.7590\n",
      "understanding the distinction between narrow and wide transformations is crucial. \n",
      "Narrow transformations excel in local processing within partitions, optimizing performance, while \n",
      "wide transformations, although necessary for certain operations, introduce overhead due to data \n",
      "shuffling and global reorganization across partitions.\n",
      "Let’s look at the significance of Spark RDDs:\n",
      "•\t Distributed data processing: RDDs enable distributed processing of large-scale data across a \n",
      "cluster of machines, promoting parallelism and scalability\n",
      "•\t Fault tolerance and reliability: Their immutability and lineage-based recovery ensure fault \n",
      "tolerance and reliability in distributed environments\n",
      "•\t Flexibility in operations: RDDs support a wide array of transformations and actions, allowing \n",
      "diverse data manipulations and processing operations\n",
      "Evolution and alternatives\n",
      "While RDDs remain fundamental, Spark’s DataFrame and Dataset APIs offer optimized, higher-level \n",
      "abstractions suitable for structured data processing and optimization.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.7304\n",
      "This is a different paradigm \n",
      "than most other programming paradigms. This is what we call lazy evaluation in Spark.\n",
      "Actions bring about computation and collect results to be sent to the driver program.\n",
      "Now that we’ve covered the basics of transformations and actions in Spark, let’s move on to understanding \n",
      "the two types of transformations it offers.\n",
      "Types of transformations\n",
      "Apache Spark’s transformations are broadly categorized into narrow and wide transformations, each \n",
      "serving distinct purposes in the context of distributed data processing.\n",
      "Narrow transformations\n",
      "Narrow transformations, also known as local transformations, operate on individual partitions of \n",
      "data without shuffling or redistributing data across partitions. These transformations enable Spark to \n",
      "process data within a single partition independently. In narrow transformations, Spark will work with \n",
      "a single input partition and a single output partition. This means that these types of transformations \n",
      "would result in an operation that can be performed on a single partition.\n"
     ]
    }
   ],
   "source": [
    "queries_2 = [\n",
    "    \"How does the Catalyst optimizer improve query execution in Spark SQL?\",\n",
    "    \"What is the difference between repartitioning and coalescing in Spark?\",\n",
    "    \"How does watermarking help with late data in Structured Streaming?\",\n",
    "    \"What are the key stages of the machine learning life cycle in Spark ML?\",\n",
    "    \"What are the different storage levels available when persisting data in Spark?\",\n",
    "    \"What is the difference between narrow and wide transformations in Spark?\"\n",
    "]\n",
    "\n",
    "for query in queries_2:\n",
    "    response = run_query_and_inspect(query, query_engine_3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296d2dc-6ce1-4c24-a376-ed75a15c8c61",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f6d18e-aca7-498e-af41-313ce2e1138b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consulta: What does collect() function do in PySpark?\n",
      "============================================================\n",
      "\n",
      "--- Nodo 1 ---\n",
      "Score: 0.5924\n",
      "0 |string_test_3|\n",
      "+-------+-------+-------+-------------+\n",
      "Now, let’s take a look at the collect statement.\n",
      "Collecting the data\n",
      "A collect statement is used when we want to get all the data that is being processed in different clusters \n",
      "back to the driver. When using a collect statement, we need to make sure that the driver has enough \n",
      "memory to hold the processed data. If the driver doesn’t have enough memory to hold the data, we \n",
      "will get out-of-memory errors.\n",
      "This is how you show the collect statement:\n",
      "data_df.collect()\n",
      "This statement will then show result as follows:\n",
      "[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.\n",
      "\n",
      "--- Nodo 2 ---\n",
      "Score: 0.5830\n",
      "especially in cases where data is sorted in \n",
      "descending order\n",
      "•\t It performs a more expensive operation compared to head(n) as it may involve scanning \n",
      "the entire dataset\n",
      "In summary, take and collect are used to retrieve data elements, with take being more suitable \n",
      "for small subsets and collect for retrieving all data (with caution). show is used for visual inspection, \n",
      "head retrieves the first rows as Row objects, and tail retrieves the last rows of the dataset. Each \n",
      "method serves different purposes and should be chosen based on the specific requirements of the \n",
      "data analysis task.\n",
      "When working with data in PySpark, sometimes, you will need to use some Python functions on the \n",
      "DataFrames. To achieve that, you will have to convert PySpark DataFrames to Pandas DataFrames. \n",
      "Now, let’s take a look at how we can convert a Pyspark DataFrame to a Pandas DataFrame.\n",
      "\n",
      "--- Nodo 3 ---\n",
      "Score: 0.5377\n",
      "Here’s a summary of the major differences between take, collect, \n",
      "show, head, and tail that we used earlier in this section for data retrieval.\n",
      "take(n)\n",
      "This function returns an array containing the first n elements from the DataFrame or RDD\n",
      "•\t It is useful for quickly inspecting a small subset of the data\n",
      "•\t It performs a lazy evaluation, meaning it only computes the required number of elements\n",
      "collect()\n",
      "This function retrieves all elements from the DataFrame or RDD and returns them as a list\n",
      "•\t It should be used with caution as it brings all data to the driver node,\n",
      "\n",
      "--- Nodo 4 ---\n",
      "Score: 0.4995\n",
      "This name and its \n",
      "corresponding values exist in the original DataFrame twice.\n",
      "Now that we have learned about different data filtering techniques, we will now see how we can \n",
      "aggregate data in Pyspark DataFrames.\n",
      "Using aggregates in a DataFrame\n",
      "Some of the methods available in Spark for aggregating data are as follows:\n",
      "•\t agg\n",
      "•\t avg\n",
      "•\t count\n",
      "•\t max\n",
      "•\t mean\n",
      "•\t min\n",
      "•\t pivot\n",
      "•\t sum\n",
      "\n",
      "Spark DataFrames and their Operations\n",
      "\n",
      "We will see some of them in action in the following code examples.\n",
      "Average (avg)\n",
      "In the following example, we see how to use aggregate functions in Spark.\n",
      "\n",
      "--- Nodo 5 ---\n",
      "Score: 0.4961\n",
      "We will discuss query plans \n",
      "later in this book.\n",
      "Here are some of the operations that can be classified as actions:\n",
      "•\t show()\n",
      "•\t take()\n",
      "•\t count()\n",
      "•\t collect()\n",
      "•\t save()\n",
      "•\t foreach()\n",
      "•\t first()\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "All of these operations would result in Spark triggering code execution and thus operations are run.\n",
      "Let’s take a look at the following code to understand these concepts better:\n",
      "# Python\n",
      ">>> df = spark.read.text(\"{path_to_data_file}\")\n",
      ">>> names_df = df.select(col(\"firstname\"),col(\"lastname\"))\n",
      ">>> names_df.show()\n",
      "In the preceding code, until line 2, nothing would be executed. On line 3, an action is triggered and \n",
      "thus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the \n",
      "wrong column names in line 2, Spark will not detect this until it runs line 3.\n"
     ]
    }
   ],
   "source": [
    "retriever= index.as_retriever(similarity_top_k =5)\n",
    "query = \"What does collect() function do in PySpark?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# 6️⃣ Mostrar los chunks devueltos\n",
    "print(f\"\\nConsulta: {query}\\n{'='*60}\")\n",
    "\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(f\"\\n--- Nodo {i+1} ---\")\n",
    "    print(f\"Score: {node.score:.4f}\")\n",
    "    print(node.node.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ca2c7c-657a-4746-abec-5311adccaa26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What does collect() function does in pyspark?\n",
      "\n",
      "\n",
      "Response: The collect() function retrieves all elements from a DataFrame or RDD and returns them as a list. It should be used with caution, as it brings all data to the driver node, which requires sufficient memory to avoid out-of-memory errors.\n",
      "\n",
      "\n",
      "\n",
      "Query: How can we control the intern process in a Spark application?\n",
      "\n",
      "\n",
      "Response: The internal process in a Spark application can be controlled through the application master and the driver. The application master is responsible for managing and coordinating the execution of the application, negotiating resources, scheduling tasks, and monitoring execution. The driver, which can run on the master node or as an independent process, manages the allocated resources and divides the application into smaller tasks for execution. Additionally, the SparkSession serves as the main entry point for interaction with Spark, allowing for the execution of queries and managing the overall application process.\n",
      "\n",
      "\n",
      "\n",
      "Query: What is the under-the-hood workflow in a Spark application?\n",
      "\n",
      "\n",
      "Response: In a Spark application, the workflow begins when a user submits a spark-submit request, which creates a Spark application. This triggers the creation of a job once an action is performed. The Spark Core, which controls all functionalities, manages in-memory computing, task distribution, and other key responsibilities.\n",
      "\n",
      "The process involves the following steps:\n",
      "\n",
      "1. The Spark driver is initialized by the cluster manager to execute the main method of the application, creating a SparkSession.\n",
      "2. The driver communicates with the cluster manager to request resources for execution.\n",
      "3. The cluster manager starts the executors, which can directly communicate with the driver.\n",
      "4. The driver constructs a logical plan, represented as a directed acyclic graph (DAG), and a physical plan based on the tasks required.\n",
      "5. Data is divided among the executors along with the associated tasks.\n",
      "6. As each task completes, the driver collects the results.\n",
      "7. Throughout this process, the driver maintains constant communication with the executors, coordinating the execution of the application.\n",
      "\n",
      "This structured approach ensures efficient processing and resource management within the Spark framework.\n",
      "\n",
      "\n",
      "\n",
      "Query: What is the role of the Driver component?\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m dispatcher.event(\n\u001b[32m     54\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:183\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    180\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    181\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m    182\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_synthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:242\u001b[39m, in \u001b[36mBaseSynthesizer.synthesize\u001b[39m\u001b[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m     query = QueryBundle(query_str=query)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager.event(\n\u001b[32m    239\u001b[39m     CBEventType.SYNTHESIZE,\n\u001b[32m    240\u001b[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001b[32m    241\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     response_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetadataMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     additional_source_nodes = additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    251\u001b[39m     source_nodes = \u001b[38;5;28mlist\u001b[39m(nodes) + \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:43\u001b[39m, in \u001b[36mCompactAndRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_texts = \u001b[38;5;28mself\u001b[39m._make_compact_text_chunks(query_str, text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:179\u001b[39m, in \u001b[36mRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_give_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._refine_response_single(\n\u001b[32m    185\u001b[39m             prev_response, query_str, text_chunk, **response_kwargs\n\u001b[32m    186\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:241\u001b[39m, in \u001b[36mRefine._give_response_single\u001b[39m\u001b[34m(self, query_str, text_chunk, **response_kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    239\u001b[39m         structured_response = cast(\n\u001b[32m    240\u001b[39m             StructuredRefineResponse,\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         query_satisfied = structured_response.query_satisfied\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:85\u001b[39m, in \u001b[36mDefaultRefineProgram.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m     83\u001b[39m         answer = answer.model_dump_json()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer=answer, query_satisfied=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:616\u001b[39m, in \u001b[36mLLM.predict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    615\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:385\u001b[39m, in \u001b[36mOpenAI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    384\u001b[39m     chat_fn = completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m._complete)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:111\u001b[39m, in \u001b[36mllm_retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    104\u001b[39m retry = create_retry_decorator(\n\u001b[32m    105\u001b[39m     max_retries=max_retries,\n\u001b[32m    106\u001b[39m     random_exponential=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     max_seconds=\u001b[32m20\u001b[39m,\n\u001b[32m    110\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:481\u001b[39m, in \u001b[36mOpenAI._chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m message_dicts = to_openai_message_dicts(\n\u001b[32m    476\u001b[39m     messages,\n\u001b[32m    477\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reuse_client:\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What does collect() function does in pyspark?\",\n",
    "    \"How can we control the intern process in a Spark application?\", \n",
    "    \"What is the under-the-hood workflow in a Spark application?\",\n",
    "    \"What is the role of the Driver component?\",\n",
    "    \"How is the execution hierarchy organized in Spark?\",\n",
    "    \"How can I get the number of rows in a Spark DataFrame?\",\n",
    "    \"How can I create a DataFrame in PySpark?\",\n",
    "    \"What is the optimized way to perform joins in PySpark?\",\n",
    "    \"What does a 'broadcast join' means?\",\n",
    "    \"How does persist() differ from cache() in PySpark?\",\n",
    "    \"How can I select specific columns from a PySpark DataFrame?\"\n",
    "]\n",
    "\n",
    "for query in queries: \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Response: {query_engine.query(query)}\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
