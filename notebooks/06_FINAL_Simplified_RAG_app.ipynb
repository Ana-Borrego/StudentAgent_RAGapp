{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4be3c35-5c6f-4e29-8a0a-7fc49e78e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up enviroment\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Key\n",
    "load_dotenv()\n",
    "# Set embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Accuracy\n",
    "tasa_acierto = True\n",
    "test_exam = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b2a4a8-b842-4e96-a1b4-725b94d814ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de las preguntas\n",
    "qa_path = \"../data/exams_test2/clean_questions/questions_all_tests.json\"\n",
    "with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de65d67e-bdfe-49ee-8058-798197881894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al índice optimizado\n",
    "storage_path = \"../data/index_storage/\"\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae99a13-8976-467c-968f-683e333fe798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para formatear las opciones:\n",
    "def format_options(options_dict):\n",
    "    clean_options = {}\n",
    "    for key in options_dict:\n",
    "        # Elimina letras errantes como '\\nB' o '\\nC' dentro del texto\n",
    "        value = options_dict[key]\n",
    "        value = re.sub(r'\\n[A-E]\\n?', '', value).strip()\n",
    "        clean_options[key] = value\n",
    "    return \"\\n\".join([f\"{k}. {v}\" for k, v in clean_options.items()])\n",
    "\n",
    "# Validar la repsuesta:\n",
    "def display_question_result(result):\n",
    "    print(\"ID Pregunta:\", result[\"question_id\"])\n",
    "    print(\"\\nPregunta:\\n\", result[\"question\"])\n",
    "    print(\"\\nOpciones:\\n\", result[\"options\"])\n",
    "    print(\"\\nAcertó?:\", result[\"is_correct\"])\n",
    "    print(\"\\nRespuesta generada por el modelo:\\n\", result[\"response_obj\"])\n",
    "    print(\"\\nRespuesta correcta:\", result[\"correct_answer\"])\n",
    "    if \"response_obj\" in result:\n",
    "        print(f\"\\n🔎 Chunks usados para responder la pregunta {result['question_id']}:\\n\")\n",
    "        for i, node in enumerate(result[\"response_obj\"].source_nodes):\n",
    "            print(f\"--- Chunk #{i+1} | Score: {node.score:.4f} ---\")\n",
    "            print(node.node.get_content().strip())\n",
    "            print()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Eres un experto en Apache Spark y PySpark, y estás respondiendo preguntas tipo test usando exclusivamente la información del contexto proporcionado.\n",
    "\n",
    "No puedes utilizar ningún conocimiento previo ni información externa al contexto.\n",
    "\n",
    "Tu tarea es:\n",
    "1. Leer el contexto cuidadosamente.\n",
    "2. Leer la pregunta y las opciones tipo test.\n",
    "3. Seleccionar la opción correcta si está claramente justificada por el contenido del contexto, y justificar tu elección citando fragmentos textuales exactos.\n",
    "4. Si no hay una justificación directa, puedes razonar una respuesta **únicamente si se basa en información presente en el contexto**, indicando exactamente **qué fragmentos usas** y **cómo los conectas**. No está permitido inventar datos.\n",
    "5. En caso de no encontrar ninguna información útil en el contexto, responde exactamente con: **\"No existe información\"**\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "def extract_predicted_letter(response_text):\n",
    "    \"\"\"\n",
    "    Extrae la letra (A, B, C, D o E) de una respuesta generada en el formato:\n",
    "    'Respuesta correcta: <LETRA>\\nJustificación: <...>'\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Respuesta\\s+correcta\\s*:\\s*([A-E])\", response_text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None \n",
    "\n",
    "def ask_question_with_options(question_obj, \n",
    "                              #query_str_template, \n",
    "                              retriever = retriever, response_mode= \"compact\"):\n",
    "    question_text = question_obj[\"question\"]\n",
    "    options_text = format_options(question_obj[\"options\"])\n",
    "\n",
    "    #qa_prompt = PromptTemplate(query_str_template)\n",
    "    #Settings.text_qa_template = qa_prompt\n",
    "\n",
    "    query_str = f\"\"\"\n",
    "    {question_text}\n",
    "\n",
    "    Opciones: \n",
    "    {options_text}\n",
    "        \n",
    "    IMPORTANTE:\n",
    "        - Usa solo información del contexto.\n",
    "        - No respondas con conocimientos externos.\n",
    "        - Aunque sepas la respuesta correcta, si no existe esa información en el contexto devuelve \"No existe información\"\n",
    "    \n",
    "    Responde de la siguiente manera:\n",
    "    \"\n",
    "    Respuesta correcta: <LETRA>\n",
    "    Justificación: <JUSTIFICACIÓN>\n",
    "    \"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar QueryBundle para pasar la información\n",
    "    query_bundle = QueryBundle(\n",
    "        query_str = query_str,\n",
    "        custom_embedding_strs = [question_text, options_text]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=response_mode)\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_bundle)\n",
    "    predicted_answer = response.response.strip()\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": question_obj[\"question_id\"],\n",
    "        \"question\" : question_obj[\"question\"],\n",
    "        \"options\": format_options(question_obj[\"options\"]),\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"correct_answer\": question_obj[\"correct_answer\"],\n",
    "        \"is_correct\": extract_predicted_letter(predicted_answer) == question_obj[\"correct_answer\"],\n",
    "        \"response_obj\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417efaee-afb4-4ecb-b5da-74f29540f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_evaluate = random.sample(questions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ad0839-5e83-400a-b71e-7602af5e8557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Pregunta: Test_2_13\n",
      "\n",
      "Pregunta:\n",
      " What is the core of Spark’s fault-tolerant mechanism?\n",
      "\n",
      "Opciones:\n",
      " A. RDD is at the core of Spark, which is fault tolerant by design\n",
      "B. Data partitions, since data can be recomputed\n",
      "C. DataFrame is at the core of Spark since it is immutable\n",
      "D. Executors ensure that Spark remains fault tolerant\n",
      "\n",
      "Acertó?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificación: \"RDDs serve as the core data structure in Spark, enabling fault-tolerant and parallel operations on large-scale distributed datasets and they are immutable.\" Además, se menciona que \"Spark is designed to be fault-tolerant,\" lo que refuerza que RDD es fundamental para este mecanismo.\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_13:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7342 ---\n",
      "RDDs are fault-tolerant. This means that if there are failures, RDDs have the ability to self-recover. \n",
      "Spark achieves that by distributing these RDDs to different worker nodes while keeping in view what \n",
      "task is performed by which worker node. This handling of worker nodes is done by the Spark driver. \n",
      "We will discuss this in detail in upcoming chapters.\n",
      "RDDs give a lot of power to Spark in terms of resilience and fault-tolerance. This capability, along with \n",
      "other features, makes Spark the tool of choice for any production-grade applications.\n",
      "\n",
      "Understanding Apache Spark and Its Applications\n",
      "\n",
      "Multiple language support\n",
      "Spark supports multiple languages for development such as Java, R, Scala, and Python. This gives \n",
      "users the flexibility to use any language of choice to build applications in Spark.\n",
      "The components of Spark\n",
      "Let’s talk about the different components Spark has.\n",
      "\n",
      "--- Chunk #2 | Score: 0.6864 ---\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "This immutability and lineage gives Spark the ability to reproduce any DataFrame in case of failure \n",
      "and it makes fault-tolerant by design. Since RDD is the lowest level of abstraction in Spark, all other \n",
      "datasets built on top of RDDs share these properties. The high-level DataFrame API is built on top of \n",
      "the low-level RDD API as well, so DataFrames also share the same properties.\n",
      "RDDs are also partitioned by Spark and each partition is distributed to multiple nodes in the cluster.\n",
      "Here are some of the key characteristics of Spark RDDs:\n",
      "•\t Immutable nature: RDDs are immutable, ensuring that once created, they cannot be altered, \n",
      "allowing for a lineage of transformations.\n",
      "•\t Resilience through lineage: RDDs store lineage information, enabling reconstruction of lost \n",
      "partitions in case of failures. Spark is designed to be fault-tolerant.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6845 ---\n",
      "Here are some of the key characteristics of Spark RDDs:\n",
      "•\t Immutable nature: RDDs are immutable, ensuring that once created, they cannot be altered, \n",
      "allowing for a lineage of transformations.\n",
      "•\t Resilience through lineage: RDDs store lineage information, enabling reconstruction of lost \n",
      "partitions in case of failures. Spark is designed to be fault-tolerant. Therefore, if an executor \n",
      "on a worker node fails while calculating an RDD, that RDD can be recomputed by another \n",
      "executor using the lineage that Spark has created.\n",
      "•\t Partitioned data: RDDs divide data into partitions, distributed across multiple nodes in a \n",
      "cluster for parallel processing.\n",
      "•\t Parallel execution: Spark executes operations on RDDs in parallel across distributed partitions, \n",
      "enhancing performance.\n",
      "Let’s discuss some more characteristics in detail.\n",
      "Lazy computation\n",
      "RDDs support lazy evaluation, deferring execution of transformations until an action is invoked.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6668 ---\n",
      "•\t Task scheduling\n",
      "•\t Task monitoring\n",
      "•\t In-memory computation\n",
      "•\t Fault tolerance\n",
      "•\t Optimization\n",
      "Spark Core contains an API for RDDs which are an integral part of Spark. It also provides different \n",
      "APIs to interact and work with RDDs. All the components of Spark work with underlying RDDs for \n",
      "data manipulation and processing. RDDs make it possible for Spark to have a lineage for data, since \n",
      "they are immutable. This means that every time an operation is run on an RDD that requires changes \n",
      "in it, Spark will create a new RDD for it. Hence, it maintains the lineage information of RDDs and \n",
      "their corresponding operations.\n",
      "Spark SQL\n",
      "SQL is the most popular language for database and data warehouse applications. Analysts use this \n",
      "language for all their exploratory data analysis on relational databases and their counterparts in \n",
      "traditional data warehouses. Spark SQL adds this advantage to the Spark ecosystem.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6628 ---\n",
      "Next, we will look at RDDs, which serve as foundational data abstractions in Apache Spark, enabling \n",
      "distributed processing, fault tolerance, and flexibility in handling large-scale data operations. While \n",
      "RDDs continue to be a fundamental concept, Spark’s DataFrame and Dataset APIs offer advancements \n",
      "in structured data processing and performance optimization.\n",
      "RDDs\n",
      "Apache Spark’s RDD stands as a foundational abstraction that underpins the distributed computing \n",
      "capabilities within the Spark framework. RDDs serve as the core data structure in Spark, enabling \n",
      "fault-tolerant and parallel operations on large-scale distributed datasets and they are immutable. This \n",
      "means that they cannot be changed over time. For any operations, a new RDD has to be generated from \n",
      "the existing RDD. When a new RDD originates from the original RDD, the new RDD has a pointer to \n",
      "the RDD it is generated from. This is the way Spark documents the lineage for all the transformations \n",
      "taking place on an RDD.\n",
      "\n",
      "ID Pregunta: Test_2_7\n",
      "\n",
      "Pregunta:\n",
      " Which of the following statements is accurate about executors?\n",
      "\n",
      "Opciones:\n",
      " A. Slots are not a part of an executor\n",
      "B. Executors are able to run tasks in parallel via slots\n",
      "C. Executors are always equal to tasks\n",
      "D. An executor is responsible for distributing tasks for a job\n",
      "\n",
      "Acertó?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: B  \n",
      "Justificación: \"Tasks run in parallel across executors. They can be run on multiple nodes and are independent of each other. This is done with the help of slots.\"\n",
      "\n",
      "Respuesta correcta: B\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_7:\n",
      "\n",
      "--- Chunk #1 | Score: 0.5881 ---\n",
      "This ensures \n",
      "reliability and fault tolerance in Spark. We will read more about this later in this chapter.\n",
      "It is the responsibility of the executor to read data from external sources that are needed to run the \n",
      "tasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by \n",
      "the executor.\n",
      "The key functions of an executor are as follows:\n",
      "•\t Task execution: Executors run tasks assigned by the Spark application, processing data stored \n",
      "in RDDs or DataFrames\n",
      "•\t Resource allocation: Each Spark application has a set of executors allocated by the cluster \n",
      "manager for managing resources such as CPU cores and memory\n",
      "In Apache Spark, the concepts of job, stage, and task form the fundamental building blocks of its \n",
      "distributed computing framework. Understanding these components is essential to grasp the core \n",
      "workings of Spark’s parallel processing and task execution.\n",
      "\n",
      "--- Chunk #2 | Score: 0.5686 ---\n",
      "Driver \n",
      "launches the executors based on the DAG that Spark generates for its execution. Once the tasks have \n",
      "finished executing, executors send the results back to the driver.\n",
      "Since the driver is the main controller of the Spark application, if an executor fails or takes too long to \n",
      "execute a task, the driver can choose to send that task over to other available executors. This ensures \n",
      "reliability and fault tolerance in Spark. We will read more about this later in this chapter.\n",
      "It is the responsibility of the executor to read data from external sources that are needed to run the \n",
      "tasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by \n",
      "the executor.\n",
      "\n",
      "--- Chunk #3 | Score: 0.5635 ---\n",
      "Tasks are \n",
      "essentially a series of operations such as filter, groupBy, and others.\n",
      "Tasks run in parallel across executors. They can be run on multiple nodes and are independent of \n",
      "each other. This is done with the help of slots. Each task processes a portion of the data partition. \n",
      "Occasionally, a group of these tasks has to finish execution to begin the next task’s execution.\n",
      "Now that we understand these concepts, let’s see why they are significant in Spark:\n",
      "•\t Parallel processing: Executors, jobs, stages, and tasks collaborate to enable parallel execution \n",
      "of computations, optimizing performance by leveraging distributed computing\n",
      "•\t Task granularity and efficiency: Tasks divide computations into smaller units, facilitating \n",
      "efficient resource utilization and parallelism across cluster nodes\n",
      "Next, we will move on to discuss a significant concept that enhances efficiency in computation.\n",
      "\n",
      "--- Chunk #4 | Score: 0.5197 ---\n",
      "Since the driver has all the \n",
      "information about the number of available workers and the tasks that are running on each of them \n",
      "alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \n",
      "taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \n",
      "executor returns the task earlier would prevail. The Spark driver also maintains metadata about the \n",
      "Resilient Distributed Dataset (RDD) and its partitions.\n",
      "It is the responsibility of the Spark driver to design the complete execution map. It determines which \n",
      "tasks run on which executors, as well as how the data is distributed across these executors. This is \n",
      "done by creating RDDs internally. Based on this distribution of data, the operations that are required \n",
      "are determined, such as transformations and actions that are defined in the program. A DAG is \n",
      "created based on these decisions.\n",
      "\n",
      "--- Chunk #5 | Score: 0.5187 ---\n",
      "It determines which \n",
      "tasks run on which executors, as well as how the data is distributed across these executors. This is \n",
      "done by creating RDDs internally. Based on this distribution of data, the operations that are required \n",
      "are determined, such as transformations and actions that are defined in the program. A DAG is \n",
      "created based on these decisions. The Spark driver optimizes the logical plan (DAG) and finds the \n",
      "best possible execution strategy for the DAG, in addition to determining the most optimal location for \n",
      "the execution of a particular task. These executions are done in parallel. The executors simply follow \n",
      "these commands without doing any optimization on their end.\n",
      "For performance considerations, it is optimal to have the Spark driver work close to the executor. This \n",
      "reduces the latency by a great deal. This means that there would be less delay in the response time of \n",
      "the processes.\n",
      "\n",
      "ID Pregunta: Test_2_5\n",
      "\n",
      "Pregunta:\n",
      " Which one of these operations is an action?\n",
      "\n",
      "Opciones:\n",
      " A. DataFrame.count()\n",
      "B. DataFrame.filter()\n",
      "C. DataFrame.select()\n",
      "D. DataFrame.groupBy()\n",
      "\n",
      "Acertó?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificación: \"Aquí hay algunas de las operaciones que se pueden clasificar como acciones: • show() • take() • count() • collect() • save() • foreach() • first()\" En esta lista, \"count()\" es mencionada como una acción.\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_5:\n",
      "\n",
      "--- Chunk #1 | Score: 0.5869 ---\n",
      "We will discuss query plans \n",
      "later in this book.\n",
      "Here are some of the operations that can be classified as actions:\n",
      "•\t show()\n",
      "•\t take()\n",
      "•\t count()\n",
      "•\t collect()\n",
      "•\t save()\n",
      "•\t foreach()\n",
      "•\t first()\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "All of these operations would result in Spark triggering code execution and thus operations are run.\n",
      "Let’s take a look at the following code to understand these concepts better:\n",
      "# Python\n",
      ">>> df = spark.read.text(\"{path_to_data_file}\")\n",
      ">>> names_df = df.select(col(\"firstname\"),col(\"lastname\"))\n",
      ">>> names_df.show()\n",
      "In the preceding code, until line 2, nothing would be executed. On line 3, an action is triggered and \n",
      "thus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the \n",
      "wrong column names in line 2, Spark will not detect this until it runs line 3.\n",
      "\n",
      "--- Chunk #2 | Score: 0.5547 ---\n",
      "Execution is triggered by actions only, not by transformations. When an \n",
      "action is called, this is when Spark starts execution on the DAG it created during the analysis phase of \n",
      "code. With the DAG created, Spark creates multiple query plans based on its internal optimizations. \n",
      "Then, it executes the plan that is the most efficient and cost-effective. We will discuss query plans \n",
      "later in this book.\n",
      "Here are some of the operations that can be classified as actions:\n",
      "•\t show()\n",
      "•\t take()\n",
      "•\t count()\n",
      "•\t collect()\n",
      "•\t save()\n",
      "•\t foreach()\n",
      "•\t first()\n",
      "\n",
      "Spark Architecture and Transformations\n",
      "\n",
      "All of these operations would result in Spark triggering code execution and thus operations are run.\n",
      "\n",
      "--- Chunk #3 | Score: 0.5507 ---\n",
      "This means all these operations on \n",
      "DataFrames result in a new DataFrame, but they are not executed until an action is followed by them. \n",
      "This would return a DataFrame or RDD when it is triggered by an action.\n",
      "Actions and computation execution\n",
      "Actions (for example, collect, count, and saveAsTextFile) prompt the execution of \n",
      "transformations on RDDs. Execution is triggered by actions only, not by transformations. When an \n",
      "action is called, this is when Spark starts execution on the DAG it created during the analysis phase of \n",
      "code. With the DAG created, Spark creates multiple query plans based on its internal optimizations. \n",
      "Then, it executes the plan that is the most efficient and cost-effective. We will discuss query plans \n",
      "later in this book.\n",
      "\n",
      "--- Chunk #4 | Score: 0.5151 ---\n",
      "Due to this, when certain developers try to time \n",
      "the code from Spark, they see that certain operations’ runtime is very fast. The reason could be that \n",
      "the code is only comprised of transformations until that point. Since no action is present, the code \n",
      "doesn’t run. To accurately measure the runtime of each operation, actions have to be called to force \n",
      "Spark to execute those statements.\n",
      "Here are some of the operations that can be classified as transformations:\n",
      "•\t orderBy()\n",
      "•\t groupBy()\n",
      "•\t filter()\n",
      "•\t select()\n",
      "•\t join()\n",
      "When these commands are executed, they are evaluated lazily. This means all these operations on \n",
      "DataFrames result in a new DataFrame, but they are not executed until an action is followed by them. \n",
      "This would return a DataFrame or RDD when it is triggered by an action.\n",
      "\n",
      "--- Chunk #5 | Score: 0.4903 ---\n",
      "One example can be a filter operation. A filter operation would generally reduce the amount of data \n",
      "that the subsequent operations have to work with if the filter operation can be applied before other \n",
      "transformations. This is exactly how Spark operates. It will execute filters as early in the process as \n",
      "possible, thus making the next operations more performant.\n",
      "This also implies that Spark jobs would fail only at execution time. Since Spark uses lazy evaluation, \n",
      "until an action is called, the code is not executed and certain errors can be missed. To catch these \n",
      "errors, Spark code would need to have an action for execution and hence error handling.\n",
      "\n",
      "RDDs\n",
      "\n",
      "Transformations\n",
      "Transformations create new RDDs by applying functions to existing RDDs (for example, map, \n",
      "filter, and reduce). Transformations are operations that do not result in any code execution. \n",
      "These statements result in Spark creating a DAG for execution.\n",
      "\n",
      "ID Pregunta: Test_2_54\n",
      "\n",
      "Pregunta:\n",
      " The following code block contains an error. The code block is supposed to capitalize the employee \n",
      "names using a udf:\n",
      "capitalize_udf = udf(lambda x: x.upper(), StringType())\n",
      "df_with_capitalized_names = df.withColumn(\"capitalized_name\", \n",
      "capitalize(\"employee\"))\n",
      "Identify the error:\n",
      "\n",
      "Opciones:\n",
      " A. The capitalize_udf function should be called instead of capitalize\n",
      "B. The udf function, capitalize_udf, is not capitalizing correctly\n",
      "C. Instead of StringType(), IntegerType() should be used\n",
      "D. Instead of d f . w i t h C o l u m n ( \" c a p i t a l i z e d _ n a m e \" , \n",
      "capitalize(\"employee\")), it should use df.withColumn(\"employee\", \n",
      "capitalize(\"capitalized_name\"))\n",
      "\n",
      "Acertó?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificación: El código contiene un error porque se debe llamar a la función `capitalize_udf` en lugar de `capitalize`. Esto se deduce del fragmento que dice: \"We use the withColumn() method to apply the UDF capitalize_udf to the name column\".\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_54:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7215 ---\n",
      "withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
      "# Display the result\n",
      "df_with_capitalized_names.show()\n",
      "\n",
      "Advanced Spark SQL operations\n",
      "\n",
      "The output will be the following:\n",
      "+---+--------+----------+------+---+----------------+\n",
      "| ID|Employee|Department|Salary|Age|capitalized_name|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "|1|John| Field-eng|3500| 40|JOHN|\n",
      "|2|Robert|Sales|4000| 38|ROBERT|\n",
      "|3|Maria|Finance|3500| 28|MARIA|\n",
      "|4| Michael|Sales|3000| 20|MICHAEL|\n",
      "|5|Kelly|Finance|3500| 35|KELLY|\n",
      "|6|Kate|Finance|3000| 45|KATE|\n",
      "|7|Martin|Finance|3500|\n",
      "\n",
      "--- Chunk #2 | Score: 0.6891 ---\n",
      "38|ROBERT|\n",
      "|3|Maria|Finance|3500| 28|MARIA|\n",
      "|4| Michael|Sales|3000| 20|MICHAEL|\n",
      "|5|Kelly|Finance|3500| 35|KELLY|\n",
      "|6|Kate|Finance|3000| 45|KATE|\n",
      "|7|Martin|Finance|3500| 26|MARTIN|\n",
      "|8|Kiran|Sales|2200| 35|KIRAN|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "In this example, we start by defining a UDF called capitalize_udf using the udf() function. It \n",
      "applies a lambda function that changes the input string to upper case. We use the withColumn() \n",
      "method to apply the UDF capitalize_udf to the name column, creating a new column called \n",
      "capitalized_name in the resulting DataFrame.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6812 ---\n",
      "this example, we start by defining a UDF called capitalize_udf using the udf() function. It \n",
      "applies a lambda function that changes the input string to upper case. We use the withColumn() \n",
      "method to apply the UDF capitalize_udf to the name column, creating a new column called \n",
      "capitalized_name in the resulting DataFrame.\n",
      "Finally, we call the show() method to display the DataFrame with the transformed column.\n",
      "UDFs allow us to apply custom logic and transformations to columns in DataFrames, enabling us to \n",
      "handle complex computations, perform string manipulations, or apply domain-specific operations \n",
      "that are not available in Spark’s built-in functions.\n",
      "In this section, we explored the concept of UDFs in Spark SQL. We discussed the syntax for defining \n",
      "UDFs and demonstrated their usage through a code example.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6626 ---\n",
      "Applying a UDF to a DataFrame\n",
      "Let’s explore a practical example that demonstrates the usage of UDFs in Spark SQL by applying a \n",
      "custom function to a DataFrame:\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import udf\n",
      "from pyspark.sql.types import StringType\n",
      "# Define a UDF to capitalize a string\n",
      "capitalize_udf = udf(lambda x: x.upper(), StringType())\n",
      "# Apply the UDF to a column\n",
      "df_with_capitalized_names = salary_data_with_\n",
      "id.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
      "# Display the result\n",
      "df_with_capitalized_names.show()\n",
      "\n",
      "Advanced Spark SQL operations\n",
      "\n",
      "The output will be the following:\n",
      "+---+--------+----------+------+---+----------------+\n",
      "| ID|Employee|Department|Salary|Age|capitalized_name|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "|1|John|\n",
      "\n",
      "--- Chunk #5 | Score: 0.5093 ---\n",
      "IntegerType())\n",
      "# Using the UDF in a DataFrame operation\n",
      "df = df.withColumn(\"new_column\", my_udf(df[\"input_column\"]))\n",
      "Creating UDFs in Scala\n",
      "We can use the following code to create a UDF in Scala:\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types._\n",
      "// Define a UDF in Scala\n",
      "val myUDF: UserDefinedFunction = udf((inputParam: InputType) => {\n",
      "// Your custom logic here\n",
      "processedValue }, OutputType)\n",
      "// Using the UDF in a DataFrame operation\n",
      "val df = df.withColumn(\"newColumn\", myUDF(col(\"inputColumn\")))\n",
      "Use cases for UDFs\n",
      "UDFs are versatile and can be used in a wide range of scenarios, including, but not limited to, \n",
      "the following:\n",
      "•\t Data transformation: Applying custom logic to transform data, such as data enrichment, \n",
      "cleansing,\n",
      "\n",
      "ID Pregunta: Test_2_25\n",
      "\n",
      "Pregunta:\n",
      " Which of the following code blocks performs a join in which the small salaryDf DataFrame is sent to \n",
      "all executors so that it can be joined with the employeeDf DataFrame on the employeeSalaryID \n",
      "and EmployeeID columns, respectively?\n",
      "\n",
      "Opciones:\n",
      " A. employeeDf.join(salaryDf, \"employeeDf.employeeID == salaryDf.\n",
      "employeeSalaryID\", \"inner\")\n",
      "B. employeeDf.join(salaryDf, \"employeeDf.employeeID == salaryDf.\n",
      "employeeSalaryID\", \"broadcast\")\n",
      "C. employeeDf.join(broadcast(salaryDf), employeeDf.employeeID \n",
      "== salaryDf.employeeSalaryID)\n",
      "D. salaryDf.join(broadcast(employeeDf), employeeDf.employeeID \n",
      "== salaryDf.employeeSalaryID)\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe información\n",
      "\n",
      "Respuesta correcta: C\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_25:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7119 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames – salary_data_with_\n",
      "id and employee_data – joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames.\n",
      "\n",
      "--- Chunk #2 | Score: 0.6954 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames – salary_data_with_\n",
      "id and employee_data – joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames. Here’s what it looks like:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "\n",
      "Joining DataFrames in Spark\n",
      "\n",
      "You will notice that the how parameter defines the type of join that is being done in this statement.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6943 ---\n",
      "The following code illustrates how we use an outer join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"outer\").show()\n",
      "The resulting DataFrame contains data for all the employees in the salary_data_with_id and \n",
      "employee_data DataFrames. Here’s what it looks like:\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "| ID|Employee|Department|Salary|ID|State|Gender|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "|7|Martin|Finance|3500|null| null|null|\n",
      "|8|Kiran|Sales|2200|null| null|null|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "You will notice that the how parameter has changed and says outer.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6777 ---\n",
      "The following code illustrates how to use a right join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"right\").show()\n",
      "The resulting DataFrame contains all the data from the right-hand DataFrame – that is, employee_\n",
      "data. It looks like this:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "Notice the how parameter has changed and now says right.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6658 ---\n",
      "The following code illustrates how we can use a left join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"left\").show()\n",
      "The resulting DataFrame contains all the data from the left DataFrame – that is, salary_data_\n",
      "with_id. It looks like this:\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "| ID|Employee|Department|Salary|ID|State|Gender|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "|7|Martin|Finance|3500|null| null|null|\n",
      "|8|Kiran|Sales|2200|null| null|null|\n",
      "+---+--------+----------+------+----+-----+------+\n",
      "Note that the how parameter has changed and says left.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions_to_evaluate:\n",
    "    response = ask_question_with_options(q, response_mode = \"tree_summarize\")\n",
    "    display_question_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8652787",
   "metadata": {},
   "source": [
    "## Implementación de extract_predicted_letter con `pydantic`\n",
    "\n",
    "#### Objetivo\n",
    "\n",
    "Sustituir la función tradicional `extract_predicted_letter`, basada en expresiones regulares, por un mecanismo robusto y validado mediante el modelo `pydantic.BaseModel`.\n",
    "\n",
    "Este modelo permite extraer y validar automáticamente los dos campos clave generados por el modelo LLM:\n",
    "\n",
    "+   respuesta_correcta: la letra elegida como respuesta\n",
    "+   justificacion: explicación textual del porqué\n",
    "\n",
    "#### Modelo de validación estructural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55581294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    respuesta_correcta: str = Field(\n",
    "        ...,\n",
    "        pattern=\"^[A-E]$\",\n",
    "        description=\"Letra correspondiente a la opción seleccionada por el modelo como respuesta correcta. Debe ser una letra entre A y E.\"\n",
    "    )\n",
    "    justificacion: str = Field(\n",
    "        ...,\n",
    "        description=\"Explicación textual que justifica por qué se ha seleccionado la respuesta correcta, basándose únicamente en el contexto proporcionado.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85934cc4",
   "metadata": {},
   "source": [
    "Este modelo reemplaza la lógica basada en re.search(...) y permite validación tipada, restringiendo la salida del modelo a formatos esperados.\n",
    "\n",
    "#### Nueva integración con `ask_question_with_options`\n",
    "\n",
    "En la función de evaluación de preguntas tipo test, se actualizó la configuración del `response_synthesizer` para que utilice el modelo `QuestionAnswer`:\n",
    "\n",
    "```\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=response_mode,\n",
    "    output_cls=QuestionAnswer\n",
    ")\n",
    "```\n",
    "\n",
    "Esto fuerza al motor de consulta a intentar parsear la respuesta generada directamente como una instancia válida de `QuestionAnswer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23710b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_with_options(question_obj, \n",
    "                              #query_str_template, \n",
    "                              retriever = retriever, response_mode= \"compact\"):\n",
    "    question_text = question_obj[\"question\"]\n",
    "    options_text = format_options(question_obj[\"options\"])\n",
    "\n",
    "    #qa_prompt = PromptTemplate(query_str_template)\n",
    "    #Settings.text_qa_template = qa_prompt\n",
    "\n",
    "    query_str = f\"\"\"\n",
    "    {question_text}\n",
    "\n",
    "    Opciones: \n",
    "    {options_text}\n",
    "        \n",
    "    IMPORTANTE:\n",
    "        - Usa solo información del contexto.\n",
    "        - No respondas con conocimientos externos.\n",
    "        - Aunque sepas la respuesta correcta, si no existe esa información en el contexto devuelve \"No existe información\"\n",
    "    \n",
    "    Responde de la siguiente manera:\n",
    "    Respuesta correcta: <LETRA>\n",
    "    Justificación: <JUSTIFICACIÓN>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar QueryBundle para pasar la información\n",
    "    query_bundle = QueryBundle(\n",
    "        query_str = query_str,\n",
    "        custom_embedding_strs = [question_text, options_text]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        response_mode=response_mode,\n",
    "        output_cls=QuestionAnswer\n",
    "    )\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_bundle)\n",
    "    # predicted_answer = response.response.strip()\n",
    "\n",
    "    print(\"🧪 Texto bruto generado por el modelo:\\n\")\n",
    "    print(response.response)\n",
    "\n",
    "    parsed = response.response\n",
    "\n",
    "    if parsed is None:\n",
    "        print(\"\\n❌ No se pudo parsear la respuesta con el modelo Pydantic.\")\n",
    "    else:\n",
    "        print(\"\\n✅ Letra extraída:\", parsed.respuesta_correcta)\n",
    "        print(\"🧠 Justificación:\", parsed.justificacion)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": question_obj[\"question_id\"],\n",
    "        \"question\" : question_obj[\"question\"],\n",
    "        \"options\": format_options(question_obj[\"options\"]),\n",
    "        \"predicted_answer\": parsed.respuesta_correcta,\n",
    "        \"correct_answer\": question_obj[\"correct_answer\"],\n",
    "        \"is_correct\": (parsed.respuesta_correcta == question_obj[\"correct_answer\"]) if parsed else False,\n",
    "        \"response_obj\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd0533ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Texto bruto generado por el modelo:\n",
      "\n",
      "respuesta_correcta='A' justificacion='AQE collects runtime statistics, such as data size, skewness, and partitioning, during query execution.'\n",
      "\n",
      "✅ Letra extraída: A\n",
      "🧠 Justificación: AQE collects runtime statistics, such as data size, skewness, and partitioning, during query execution.\n",
      "ID Pregunta: Test_2_3\n",
      "\n",
      "Pregunta:\n",
      " Which of the following is one of the tasks of Adaptive Query Execution in Spark?\n",
      "\n",
      "Opciones:\n",
      " A. Adaptive Query Execution collects runtime statistics during query execution to optimize \n",
      "query plans\n",
      "B. Adaptive Query Execution is responsible for distributing tasks to executors\n",
      "C. Adaptive Query Execution is responsible for wide operations in Spark\n",
      "D. Adaptive Query Execution is responsible for fault tolerance in Spark\n",
      "\n",
      "Acertó?: True\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " {\"respuesta_correcta\":\"A\",\"justificacion\":\"AQE collects runtime statistics, such as data size, skewness, and partitioning, during query execution.\"}\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_3:\n",
      "\n",
      "--- Chunk #1 | Score: 0.7296 ---\n",
      "Adaptive Query Execution (AQE)\n",
      "Apache Spark, a powerful distributed computing framework, offers a multitude of optimization \n",
      "techniques to enhance the performance of data processing jobs. One such advanced optimization \n",
      "feature is AQE, a dynamic approach that significantly improves query processing efficiency.\n",
      "AQE dynamically adjusts execution plans during runtime based on actual data statistics and hardware \n",
      "conditions. It collects and utilizes runtime statistics to optimize join strategies, partitioning methods, \n",
      "and broadcast operations.\n",
      "Let’s look at its key components:\n",
      "•\t Runtime statistics collection: AQE collects runtime statistics, such as data size, skewness, and \n",
      "partitioning, during query execution\n",
      "•\t Adaptive optimization rules: It utilizes collected statistics to adjust and optimize join strategies, \n",
      "partitioning methods, and broadcast operations dynamically\n",
      "Now, let’s consider its benefits and significance:\n",
      "•\t Improved performance: AQE significantly enhances performance by optimizing execution \n",
      "plans dynamically,\n",
      "\n",
      "--- Chunk #2 | Score: 0.6939 ---\n",
      "such as data size, skewness, and \n",
      "partitioning, during query execution\n",
      "•\t Adaptive optimization rules: It utilizes collected statistics to adjust and optimize join strategies, \n",
      "partitioning methods, and broadcast operations dynamically\n",
      "Now, let’s consider its benefits and significance:\n",
      "•\t Improved performance: AQE significantly enhances performance by optimizing execution \n",
      "plans dynamically, leading to better resource utilization and reduced execution time\n",
      "•\t Handling variability: It efficiently handles variations in data sizes, skewed data distributions, \n",
      "and changing hardware conditions during query execution\n",
      "•\t Efficient resource utilization: It optimizes query plans in real time, leading to better resource \n",
      "utilization and reduced execution time\n",
      "AQE workflow\n",
      "Let’s look at how AQE optimizes workflows in Spark 3.0:\n",
      "•\t Runtime statistics collection: During query execution, Spark collects statistics related to data \n",
      "distribution, partition sizes, and join keys’ cardinality\n",
      "•\t Adaptive optimization: Utilizing the collected statistics,\n",
      "\n",
      "--- Chunk #3 | Score: 0.6766 ---\n",
      "leading to better resource \n",
      "utilization and reduced execution time\n",
      "AQE workflow\n",
      "Let’s look at how AQE optimizes workflows in Spark 3.0:\n",
      "•\t Runtime statistics collection: During query execution, Spark collects statistics related to data \n",
      "distribution, partition sizes, and join keys’ cardinality\n",
      "•\t Adaptive optimization: Utilizing the collected statistics, Spark dynamically adjusts the query \n",
      "execution plan, optimizing join strategies, partitioning methods, and data redistribution techniques\n",
      "•\t Enhanced performance: The adaptive optimization ensures that Spark adapts to changing data \n",
      "and runtime conditions, resulting in improved query performance and resource utilization\n",
      "\n",
      "Optimizations in Apache Spark\n",
      "\n",
      "AQE in Apache Spark represents a significant advancement in query optimization, moving beyond \n",
      "static planning to adapt to runtime conditions and data characteristics. By dynamically adjusting \n",
      "execution plans based on real-time statistics, it optimizes query performance, ensuring efficient and \n",
      "scalable processing of large-scale datasets.\n",
      "\n",
      "--- Chunk #4 | Score: 0.6483 ---\n",
      "resulting in improved query performance and resource utilization\n",
      "\n",
      "Optimizations in Apache Spark\n",
      "\n",
      "AQE in Apache Spark represents a significant advancement in query optimization, moving beyond \n",
      "static planning to adapt to runtime conditions and data characteristics. By dynamically adjusting \n",
      "execution plans based on real-time statistics, it optimizes query performance, ensuring efficient and \n",
      "scalable processing of large-scale datasets.\n",
      "Next, we will see how Spark does cost-based optimizations.\n",
      "Cost-based optimization\n",
      "Spark estimates the cost of executing different query plans based on factors such as data size, join \n",
      "operations, and shuffle stages. It utilizes cost estimates to select the most efficient query execution plan.\n",
      "Here are the benefits:\n",
      "•\t Optimal plan selection: Cost-based optimization chooses the most cost-effective execution \n",
      "plan while considering factors such as join strategies and data distribution\n",
      "•\t Performance improvement: Minimizing unnecessary shuffling and computations improves \n",
      "query performance\n",
      "Next, we will see how Spark utilizes memory management and tuning for optimizations.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6480 ---\n",
      "By examining the query plan and understanding how the Catalyst optimizer enhances it, you can gain \n",
      "valuable insights into the inner workings of Spark’s optimization engine.\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "This section provided a solid introduction to the Catalyst optimizer, its components, and a practical \n",
      "example. You can expand on this foundation by delving deeper into rule-based and cost-based \n",
      "optimization techniques, as well as discussing real-world scenarios where the Catalyst optimizer can \n",
      "have a substantial impact on query performance.\n",
      "Next, we will see how AQE takes optimizations to the next level in Spark.\n",
      "Adaptive Query Execution (AQE)\n",
      "Apache Spark, a powerful distributed computing framework, offers a multitude of optimization \n",
      "techniques to enhance the performance of data processing jobs. One such advanced optimization \n",
      "feature is AQE, a dynamic approach that significantly improves query processing efficiency.\n",
      "AQE dynamically adjusts execution plans during runtime based on actual data statistics and hardware \n",
      "conditions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ask_question_with_options(questions_to_evaluate[0], response_mode = \"tree_summarize\")\n",
    "display_question_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9628a1a-53b4-42e1-9811-280c842ab641",
   "metadata": {},
   "source": [
    "# Tasa de acierto\n",
    "\n",
    "Se le enfrenta a un Test completo, por ejemplo el Test 1, y se calculará el porcentaje de acierto del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec70f570-b51d-4e6d-9b41-859bfdbb775b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejecutando evaluación del Test 2 con 60 preguntas...\n",
      "\n",
      "➡️ Pregunta 1/60 - ID: Test_2_1\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 2/60 - ID: Test_2_2\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 3/60 - ID: Test_2_3\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 4/60 - ID: Test_2_4\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 5/60 - ID: Test_2_5\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 6/60 - ID: Test_2_6\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 7/60 - ID: Test_2_7\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 8/60 - ID: Test_2_8\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 9/60 - ID: Test_2_9\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 10/60 - ID: Test_2_10\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 11/60 - ID: Test_2_11\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 12/60 - ID: Test_2_12\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 13/60 - ID: Test_2_13\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 14/60 - ID: Test_2_14\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 15/60 - ID: Test_2_15\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 16/60 - ID: Test_2_16\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 17/60 - ID: Test_2_17\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 18/60 - ID: Test_2_18\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 19/60 - ID: Test_2_19\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 20/60 - ID: Test_2_20\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 21/60 - ID: Test_2_21\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 22/60 - ID: Test_2_22\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 23/60 - ID: Test_2_23\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 24/60 - ID: Test_2_24\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 25/60 - ID: Test_2_25\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 26/60 - ID: Test_2_26\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 27/60 - ID: Test_2_27\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 28/60 - ID: Test_2_28\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 29/60 - ID: Test_2_29\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 30/60 - ID: Test_2_30\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 31/60 - ID: Test_2_31\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 32/60 - ID: Test_2_32\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 33/60 - ID: Test_2_33\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 34/60 - ID: Test_2_34\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 35/60 - ID: Test_2_35\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 36/60 - ID: Test_2_36\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 37/60 - ID: Test_2_37\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 38/60 - ID: Test_2_38\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 39/60 - ID: Test_2_39\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 40/60 - ID: Test_2_40\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 41/60 - ID: Test_2_41\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 42/60 - ID: Test_2_42\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 43/60 - ID: Test_2_43\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 44/60 - ID: Test_2_44\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 45/60 - ID: Test_2_45\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 46/60 - ID: Test_2_46\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 47/60 - ID: Test_2_47\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 48/60 - ID: Test_2_48\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 49/60 - ID: Test_2_49\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 50/60 - ID: Test_2_50\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 51/60 - ID: Test_2_51\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 52/60 - ID: Test_2_52\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 53/60 - ID: Test_2_53\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 54/60 - ID: Test_2_54\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 55/60 - ID: Test_2_55\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 56/60 - ID: Test_2_56\n",
      "¡Correcta!\n",
      "\n",
      "➡️ Pregunta 57/60 - ID: Test_2_57\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 58/60 - ID: Test_2_58\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 59/60 - ID: Test_2_59\n",
      "Incorrecta\n",
      "\n",
      "➡️ Pregunta 60/60 - ID: Test_2_60\n",
      "Incorrecta\n",
      "\n",
      "\n",
      "Tasa de acierto en el Test 2: 41.67% (25/60)\n"
     ]
    }
   ],
   "source": [
    "if tasa_acierto:\n",
    "\n",
    "    # Filtrar preguntas por test (Test_1_ o Test_2_)\n",
    "    prefix = f\"Test_{test_exam}_\"\n",
    "    preguntas_test = [q for q in questions if q[\"question_id\"].startswith(prefix)]\n",
    "\n",
    "    total = len(preguntas_test)\n",
    "    aciertos = 0\n",
    "    resultados = []  # Para análisis posterior si se desea\n",
    "\n",
    "    print(f\"\\nEjecutando evaluación del Test {test_exam} con {total} preguntas...\\n\")\n",
    "\n",
    "    for i, pregunta in enumerate(preguntas_test):\n",
    "        # print(f\"➡️ Pregunta {i+1}/{total} - ID: {pregunta['question_id']}\")\n",
    "        result = ask_question_with_options(pregunta)\n",
    "        resultados.append(result)\n",
    "\n",
    "        if result[\"is_correct\"]:\n",
    "            aciertos += 1\n",
    "            # print(\"¡Correcta!\\n\")\n",
    "        else:\n",
    "            # print(\"Incorrecta\\n\")\n",
    "\n",
    "        # Puedes descomentar para ver detalles por pregunta\n",
    "        # display_question_result(result)\n",
    "\n",
    "    tasa = aciertos / total\n",
    "    print(f\"\\nTasa de acierto en el Test {test_exam}: {tasa:.2%} ({aciertos}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266596a-c22d-498d-8f8d-447364784ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
