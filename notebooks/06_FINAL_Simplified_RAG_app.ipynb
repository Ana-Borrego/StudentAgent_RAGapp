{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4be3c35-5c6f-4e29-8a0a-7fc49e78e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up enviroment\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Key\n",
    "load_dotenv()\n",
    "# Set embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b2a4a8-b842-4e96-a1b4-725b94d814ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de las preguntas\n",
    "qa_path = \"../data/exams/clean_questions/questions_all_tests.json\"\n",
    "with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de65d67e-bdfe-49ee-8058-798197881894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al índice optimizado\n",
    "storage_path = \"../data/index_storage_200_90/\"\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_path)\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae99a13-8976-467c-968f-683e333fe798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para formatear las opciones:\n",
    "def format_options(options_dict):\n",
    "    clean_options = {}\n",
    "    for key in options_dict:\n",
    "        # Elimina letras errantes como '\\nB' o '\\nC' dentro del texto\n",
    "        value = options_dict[key]\n",
    "        value = re.sub(r'\\n[A-E]\\n?', '', value).strip()\n",
    "        clean_options[key] = value\n",
    "    return \"\\n\".join([f\"{k}. {v}\" for k, v in clean_options.items()])\n",
    "\n",
    "# Validar la repsuesta:\n",
    "def display_question_result(result):\n",
    "    print(\"ID Pregunta:\", result[\"question_id\"])\n",
    "    print(\"\\nPregunta:\\n\", result[\"question\"])\n",
    "    print(\"\\nOpciones:\\n\", result[\"options\"])\n",
    "    print(\"\\nAcertó?:\", result[\"is_correct\"])\n",
    "    print(\"\\nRespuesta generada por el modelo:\\n\", result[\"response_obj\"])\n",
    "    print(\"\\nRespuesta correcta:\", result[\"correct_answer\"])\n",
    "    if \"response_obj\" in result:\n",
    "        print(f\"\\n🔎 Chunks usados para responder la pregunta {result['question_id']}:\\n\")\n",
    "        for i, node in enumerate(result[\"response_obj\"].source_nodes):\n",
    "            print(f\"--- Chunk #{i+1} | Score: {node.score:.4f} ---\")\n",
    "            print(node.node.get_content().strip())\n",
    "            print()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Eres un experto en Apache Spark y PySpark, y estás respondiendo preguntas tipo test usando exclusivamente la información del contexto proporcionado.\n",
    "\n",
    "No puedes utilizar ningún conocimiento previo ni información externa al contexto.\n",
    "\n",
    "Tu tarea es:\n",
    "1. Leer el contexto cuidadosamente.\n",
    "2. Leer la pregunta y las opciones tipo test.\n",
    "3. Seleccionar la opción correcta si está claramente justificada por el contenido del contexto, y justificar tu elección citando fragmentos textuales exactos.\n",
    "4. Si no hay una justificación directa, puedes razonar una respuesta **únicamente si se basa en información presente en el contexto**, indicando exactamente **qué fragmentos usas** y **cómo los conectas**. No está permitido inventar datos.\n",
    "5. En caso de no encontrar ninguna información útil en el contexto, responde exactamente con: **\"No existe información\"**\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "def extract_predicted_letter(response_text):\n",
    "    \"\"\"\n",
    "    Extrae la letra (A, B, C, D o E) de una respuesta generada en el formato:\n",
    "    'Respuesta correcta: <LETRA>\\nJustificación: <...>'\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Respuesta\\s+correcta\\s*:\\s*([A-E])\", response_text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None \n",
    "\n",
    "def ask_question_with_options(question_obj, \n",
    "                              #query_str_template, \n",
    "                              retriever = retriever, response_mode= \"compact\"):\n",
    "    question_text = question_obj[\"question\"]\n",
    "    options_text = format_options(question_obj[\"options\"])\n",
    "\n",
    "    #qa_prompt = PromptTemplate(query_str_template)\n",
    "    #Settings.text_qa_template = qa_prompt\n",
    "\n",
    "    query_str = f\"\"\"\n",
    "    {question_text}\n",
    "\n",
    "    Opciones: \n",
    "    {options_text}\n",
    "        \n",
    "    IMPORTANTE:\n",
    "        - Usa solo información del contexto.\n",
    "        - No respondas con conocimientos externos.\n",
    "        - Aunque sepas la respuesta correcta, si no existe esa información en el contexto devuelve \"No existe información\"\n",
    "    \n",
    "    Responde de la siguiente manera:\n",
    "    \"\n",
    "    Respuesta correcta: <LETRA>\n",
    "    Justificación: <JUSTIFICACIÓN>\n",
    "    \"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usar QueryBundle para pasar la información\n",
    "    query_bundle = QueryBundle(\n",
    "        query_str = query_str,\n",
    "        custom_embedding_strs = [question_text, options_text]\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=response_mode)\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_bundle)\n",
    "    predicted_answer = response.response.strip()\n",
    "    \n",
    "    return {\n",
    "        \"question_id\": question_obj[\"question_id\"],\n",
    "        \"question\" : question_obj[\"question\"],\n",
    "        \"options\": format_options(question_obj[\"options\"]),\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"correct_answer\": question_obj[\"correct_answer\"],\n",
    "        \"is_correct\": extract_predicted_letter(predicted_answer) == question_obj[\"correct_answer\"],\n",
    "        \"response_obj\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417efaee-afb4-4ecb-b5da-74f29540f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_evaluate = random.sample(questions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ad0839-5e83-400a-b71e-7602af5e8557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Pregunta: Test_1_41\n",
      "\n",
      "Pregunta:\n",
      " The following code block contains an error. The code block should return the df DataFrame with \n",
      "employeeID renamed to employeeIdColumn. Find the error.\n",
      "df.withColumn(\"employeeIdColumn\", \"employeeID\")\n",
      "\n",
      "Opciones:\n",
      " A. Instead of withColumn, the withColumnRenamed method should be used\n",
      "B. Instead of withColumn, the withColumnRenamed method should be used and \n",
      "argument \"employeeIdColumn\" should be swapped with argument \"employeeID\"\n",
      "C. Arguments \"employeeIdColumn\" and \"employeeID\" should be swapped\n",
      "D. The withColumn operator should be replaced with the withColumnRenamed operator\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " Respuesta correcta: A  \n",
      "Justificación: El contexto menciona que \"para cambiar el nombre de una columna, se utilizaría la función withColumnRenamed()\". Esto indica que el uso de withColumn en el código proporcionado es incorrecto para renombrar una columna, y por lo tanto, se debe utilizar withColumnRenamed en su lugar.\n",
      "\n",
      "Respuesta correcta: B\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_1_41:\n",
      "\n",
      "--- Chunk #1 | Score: 0.6114 ---\n",
      "This function is \n",
      "used for column-wise operators. If we don’t use this function, our code will return an error.\n",
      "You don’t always have to update a column in a DataFrame if you only need to rename a column. Now, \n",
      "we will see how we can rename columns in a Spark DataFrame.\n",
      "Renaming columns\n",
      "For changing the name of a column, we would use the withColumnRenamed() function in Spark. \n",
      "We would need to provide the column name that needs to be changed along with the new column \n",
      "name.\n",
      "\n",
      "--- Chunk #2 | Score: 0.5986 ---\n",
      "You don’t always have to update a column in a DataFrame if you only need to rename a column. Now, \n",
      "we will see how we can rename columns in a Spark DataFrame.\n",
      "Renaming columns\n",
      "For changing the name of a column, we would use the withColumnRenamed() function in Spark. \n",
      "We would need to provide the column name that needs to be changed along with the new column \n",
      "name. Here’s the code to illustrate this:\n",
      "data_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\n",
      "data_df.show()\n",
      "As a result, we’ll see the following change:\n",
      "+-----+-----+-------------+----------+------+\n",
      "|col_1|col_2|string_col|col_4| col_6|\n",
      "+-----+-----+-------------+----------+------+\n",
      "|100|200.0|string_test_1|2023-01-01|A|\n",
      "|200|300.0|string_test_2|2023-02-01|A|\n",
      "|300|400.0|string_test_3|2023-03-01|A|\n",
      "+-----+-----+-------------+----------+------+\n",
      "Notice that col_3 is now called string_col after making the change.\n",
      "\n",
      "--- Chunk #3 | Score: 0.5129 ---\n",
      "withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n",
      "# Display the result\n",
      "df_with_capitalized_names.show()\n",
      "\n",
      "Advanced Spark SQL operations\n",
      "\n",
      "The output will be the following:\n",
      "+---+--------+----------+------+---+----------------+\n",
      "| ID|Employee|Department|Salary|Age|capitalized_name|\n",
      "+---+--------+----------+------+---+----------------+\n",
      "|1|John| Field-eng|3500| 40|JOHN|\n",
      "|2|Robert|Sales|4000| 38|ROBERT|\n",
      "|3|Maria|Finance|3500| 28|MARIA|\n",
      "|4| Michael|Sales|3000| 20|MICHAEL|\n",
      "|5|Kelly|Finance|3500| 35|KELLY|\n",
      "|6|Kate|Finance|3000| 45|KATE|\n",
      "|7|Martin|Finance|3500|\n",
      "\n",
      "--- Chunk #4 | Score: 0.4814 ---\n",
      "The following code illustrates how to use a right join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"right\").show()\n",
      "The resulting DataFrame contains all the data from the right-hand DataFrame – that is, employee_\n",
      "data. It looks like this:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "Notice the how parameter has changed and now says right.\n",
      "\n",
      "--- Chunk #5 | Score: 0.4717 ---\n",
      "3500), \\\n",
      "(4, \"Nate\", \"Sales\", 3000), \\\n",
      "]\n",
      "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
      "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_\n",
      "id_2, schema = columns2)\n",
      "salary_data_with_id_2.printSchema()\n",
      "salary_data_with_id_2.show(truncate=False)\n",
      "As a result, you will see the schema of the DataFrame first,\n",
      "\n",
      "ID Pregunta: Test_1_46\n",
      "\n",
      "Pregunta:\n",
      " Which of the following code blocks concatenates the rows of the salaryDf and employeeDf \n",
      "DataFrames without any duplicates (assuming the columns of both DataFrames are similar)?\n",
      "\n",
      "Opciones:\n",
      " A. salaryDf.concat(employeeDf).unique()\n",
      "B. spark.union(salaryDf, employeeDf).distinct()\n",
      "C. salaryDf.union(employeeDf).unique()\n",
      "D. salaryDf.union(employeeDf).distinct()\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe información\n",
      "\n",
      "Respuesta correcta: D\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_1_46:\n",
      "\n",
      "--- Chunk #1 | Score: 0.6619 ---\n",
      "we can use the union() function to join the salary_data_with_\n",
      "id and salary_data_with_id_2 DataFrames together. The following example illustrates this:\n",
      "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
      "unionDF.show(truncate=False)\n",
      "The resulting DataFrame, named unionDF, looks like this:\n",
      "+---+--------+----------+------+\n",
      "|ID |Employee|Department|Salary|\n",
      "+---+--------+----------+------+\n",
      "|1|John|Field-eng |3500|\n",
      "|2|Robert|Sales|4000|\n",
      "|3|Maria|Finance|3500|\n",
      "|4|Michael |Sales|3000|\n",
      "|5|Kelly|Finance|3500|\n",
      "|6|Kate|Finance|3000|\n",
      "|7|Martin|Finance|3500|\n",
      "|8|Kiran|Sales|2200|\n",
      "|1|John|Field-eng |3500|\n",
      "|2|Robert|Sales|4000|\n",
      "|3|Aliya|Finance|3500|\n",
      "|4|Nate|Sales|3000|\n",
      "+---+--------+----------+------+\n",
      "As you can see,\n",
      "\n",
      "--- Chunk #2 | Score: 0.6529 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames – salary_data_with_\n",
      "id and employee_data – joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6434 ---\n",
      "DataFrames or datasets can be joined based on common columns within a DataFrame, and the result \n",
      "of a join query is a new DataFrame.\n",
      "We will demonstrate the join operation on two new DataFrames. First, let’s create these DataFrames. \n",
      "The first DataFrame is called salary_data_with_id:\n",
      "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
      "(2, \"Robert\", \"Sales\", 4000), \\\n",
      "(3, \"Maria\", \"Finance\", 3500), \\\n",
      "\n",
      "Joining DataFrames in Spark\n",
      "\n",
      "(4, \"Michael\", \"Sales\", 3000), \\\n",
      "(5, \"Kelly\", \"Finance\", 3500), \\\n",
      "(6, \"Kate\", \"Finance\", 3000), \\\n",
      "(7, \"Martin\", \"Finance\", 3500), \\\n",
      "(8, \"Kiran\", \"Sales\", 2200), \\\n",
      "]\n",
      "columns= [\"ID\", \"Employee\", \"Department\",\n",
      "\n",
      "--- Chunk #4 | Score: 0.6430 ---\n",
      "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\n",
      "salary_data_with_id.join(employee_data,salary_data_with_id.ID \n",
      "==employee_data.ID,\"inner\").show()\n",
      "The resulting DataFrame now contains all the columns of both DataFrames – salary_data_with_\n",
      "id and employee_data – joined together in a single DataFrame. It only includes rows that are \n",
      "common in both DataFrames. Here’s what it looks like:\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "| ID|Employee|Department|Salary| ID|State|Gender|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "|1|John| Field-eng|3500|1|NY|M|\n",
      "|2|Robert|Sales|4000|2|NC|M|\n",
      "|3|Maria|Finance|3500|3|NY|F|\n",
      "|4| Michael|Sales|3000|4|TX|M|\n",
      "|5|Kelly|Finance|3500|5|NY|F|\n",
      "|6|Kate|Finance|3000|6|AZ|F|\n",
      "+---+--------+----------+------+---+-----+------+\n",
      "\n",
      "Joining DataFrames in Spark\n",
      "\n",
      "You will notice that the how parameter defines the type of join that is being done in this statement.\n",
      "\n",
      "--- Chunk #5 | Score: 0.6358 ---\n",
      "we can use the union() function to join the salary_data_with_\n",
      "id and salary_data_with_id_2 DataFrames together. The following example illustrates this:\n",
      "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
      "unionDF.show(truncate=False)\n",
      "The resulting DataFrame, named unionDF,\n",
      "\n",
      "ID Pregunta: Test_2_50\n",
      "\n",
      "Pregunta:\n",
      " Which of the following code blocks returns a DataFrame with the total count of employees in each \n",
      "department from the df DataFrame?\n",
      "\n",
      "Opciones:\n",
      " A. df.groupBy(\"department\").agg(count(\"*\").alias(\"total_\n",
      "employees\"))\n",
      "B. df.filter(\"department\").agg(count(\"*\").alias(\"total_\n",
      "employees\"))\n",
      "C. df.groupBy(\"department\").agg(sum(\"*\").alias(\"total_employees\"))\n",
      "D. df.filter(\"department\").agg(sum(\"*\").alias(\"total_employees\"))\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe información\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_50:\n",
      "\n",
      "--- Chunk #1 | Score: 0.6271 ---\n",
      "First, we group employees based on the Department column. We take the average salary \n",
      "of each department from the employees table.\n",
      "The resulting data are stored in the grouped_data variable and displayed using the show() method.\n",
      "\n",
      "Advanced Spark SQL operations\n",
      "\n",
      "Aggregating data\n",
      "Spark SQL provides a wide range of aggregate functions to calculate summary statistics on grouped \n",
      "data. Let’s consider another example where we want to calculate the total salary and the maximum \n",
      "salary for each department:\n",
      "# Perform grouping and multiple aggregations\n",
      "aggregated_data = spark.sql(\"SELECT Department, sum(Salary) AS total_\n",
      "salary, max(Salary) AS max_salary FROM employees GROUP BY Department\")\n",
      "# Display the results\n",
      "aggregated_data.show()\n",
      "The output will be the following:\n",
      "+----------+-----------+-----------+\n",
      "|Department|sum(Salary)|max(Salary)|\n",
      "+----------+-----------+-----------+\n",
      "| Field-eng|3500|3500|\n",
      "|Sales|9200|4000|\n",
      "|Finance|13500|3500|\n",
      "+----------+-----------+-----------+\n",
      "In this example, we combine and group the data based on different transformations to the employees \n",
      "table using Spark SQL.\n",
      "\n",
      "--- Chunk #2 | Score: 0.6144 ---\n",
      "First, we group employees based on the Department column. We take the average salary \n",
      "of each department from the employees table.\n",
      "The resulting data are stored in the grouped_data variable and displayed using the show() method.\n",
      "\n",
      "Advanced Spark SQL operations\n",
      "\n",
      "Aggregating data\n",
      "Spark SQL provides a wide range of aggregate functions to calculate summary statistics on grouped \n",
      "data.\n",
      "\n",
      "--- Chunk #3 | Score: 0.6093 ---\n",
      "In the following code snippet, we are going to use groupBy by taking a sum of the salary column \n",
      "for each department. Then, we will round off the sum(Salary) column that we just created to two \n",
      "digits after a decimal. After, we will rename the sum(Salary) column back to Salary. All of these \n",
      "operations are being done in a single groupBy statement:\n",
      "from pyspark.sql.functions import col, round\n",
      "salary_data.groupBy('Department')\\\n",
      ".sum('Salary')\\\n",
      ".withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\\\n",
      ".withColumnRenamed('sum(Salary)', 'Salary')\\\n",
      ".orderBy('Department')\\\n",
      ".show()\n",
      "As a result, we will see the following DataFrame showing the aggregated sum of the Salary column \n",
      "based on each department:\n",
      "+----------+------------------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+------------------+\n",
      "| null|7500|\n",
      "| Field-eng|12500|\n",
      "| Finance|10000|\n",
      "| Sales|5200|\n",
      "+----------+------------------+\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "In this example, we can see that each department’s total salary is calculated in a new column named \n",
      "sum(Salary), after which we round this total up to two decimal places.\n",
      "\n",
      "--- Chunk #4 | Score: 0.5901 ---\n",
      "All four departments, including null (since we had null \n",
      "values in our DataFrame), are included in the resulting DataFrame.\n",
      "Now, let’s take a look at how we can apply complex groupBy operations to data in PySpark DataFrames.\n",
      "A complex groupBy statement\n",
      "groupBy can be used in complex data operations, such as multiple aggregations within a single \n",
      "groupBy statement.\n",
      "In the following code snippet, we are going to use groupBy by taking a sum of the salary column \n",
      "for each department. Then, we will round off the sum(Salary) column that we just created to two \n",
      "digits after a decimal. After, we will rename the sum(Salary) column back to Salary.\n",
      "\n",
      "--- Chunk #5 | Score: 0.5880 ---\n",
      "All the \n",
      "aggregate operations can also be used for different groups of a DataFrame.\n",
      "We will use the following statement to get the average salary across different departments in our \n",
      "salary_data DataFrame:\n",
      "salary_data.groupby(‘Department’).avg().show()\n",
      "\n",
      "Grouping data in Spark and different Spark joins\n",
      "\n",
      "Here’s the result:\n",
      "+----------+------------------+\n",
      "|Department| avg(Salary)|\n",
      "+----------+------------------+\n",
      "| null|3750.0|\n",
      "| Sales|2600.0|\n",
      "| Field-eng| 4166.666666666667|\n",
      "| Finance|3333.3333333333335|\n",
      "+----------+------------------+ \n",
      "In this example, we can see that each department’s average salary is calculated based on the salary \n",
      "column of the salary_data DataFrame. All four departments, including null (since we had null \n",
      "values in our DataFrame), are included in the resulting DataFrame.\n",
      "\n",
      "ID Pregunta: Test_1_18\n",
      "\n",
      "Pregunta:\n",
      " Which of the following statements about broadcast variables is accurate?\n",
      "\n",
      "Opciones:\n",
      " A. Broadcast variables are only present on driver nodes\n",
      "B. Broadcast variables can only be used for tables that fit into memory\n",
      "C. Broadcast variables are not immutable, meaning they can be shared across clusters\n",
      "D. Broadcast variables are not shared across the worker nodes\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe información\n",
      "\n",
      "Respuesta correcta: B\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_1_18:\n",
      "\n",
      "--- Chunk #1 | Score: 0.5110 ---\n",
      "In this \n",
      "approach, the smaller DataFrame is broadcast to all worker nodes, eliminating the need for shuffling \n",
      "data across the network. A broadcast join is a specific optimization technique that can be applied \n",
      "when one of the DataFrames is small enough to fit in memory. In this case, the small DataFrame is \n",
      "broadcast to all worker nodes, avoiding costly shuffling.\n",
      "Let’s look at some of the key characteristics of broadcast joins:\n",
      "•\t Small DataFrame broadcast: The smaller DataFrame is broadcast to all worker nodes, ensuring \n",
      "that it is available locally\n",
      "•\t Reduced network overhead: Broadcast joins significantly reduce network and disk I/O because \n",
      "they avoid data shuffling\n",
      "•\t Ideal for dimension tables: Broadcast joins are commonly used when joining a fact table with \n",
      "smaller dimension tables, such as in data warehousing scenarios\n",
      "•\t Efficient for small-to-large joins: They are efficient for joins where one DataFrame is significantly \n",
      "smaller than the other\n",
      "Use case\n",
      "Broadcast joins are useful when you’re joining a large DataFrame with a much smaller one, such as \n",
      "joining a fact table with dimension tables in a data warehouse.\n",
      "\n",
      "--- Chunk #2 | Score: 0.5065 ---\n",
      "Broadcast hash joins\n",
      "A specific type of broadcast join is the broadcast hash join. In this variant, the smaller DataFrame is \n",
      "broadcast as a hash table to all worker nodes, which allows for efficient lookups in the larger DataFrame.\n",
      "Use case\n",
      "Broadcast hash joins are suitable for scenarios where one DataFrame is small enough to be broadcast, \n",
      "and you need to perform equality-based joins.\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "In this section, we discussed two fundamental join techniques in Spark – shuffle joins and broadcast \n",
      "joins – including specific variants, such as the broadcast hash join and the shuffle sort-merge join. \n",
      "Choosing the right join method depends on the size of your DataFrames, data distribution, and network \n",
      "considerations, and it’s essential to make informed decisions to optimize your Spark applications. In \n",
      "the next section, we will cover different types of transformations that exist in Spark.\n",
      "\n",
      "--- Chunk #3 | Score: 0.4752 ---\n",
      "Let’s look at broadcast joins next.\n",
      "Broadcast joins\n",
      "Broadcast joins are a highly efficient technique for joining a small DataFrame with a larger one. In this \n",
      "approach, the smaller DataFrame is broadcast to all worker nodes, eliminating the need for shuffling \n",
      "data across the network. A broadcast join is a specific optimization technique that can be applied \n",
      "when one of the DataFrames is small enough to fit in memory. In this case, the small DataFrame is \n",
      "broadcast to all worker nodes, avoiding costly shuffling.\n",
      "\n",
      "--- Chunk #4 | Score: 0.4534 ---\n",
      "focusing only on relevant subsets of data\n",
      "•\t Join optimization:\n",
      "\t Broadcast joins: Utilize broadcast joins for smaller datasets to replicate them across nodes,\n",
      "\n",
      "--- Chunk #5 | Score: 0.3988 ---\n",
      "It leverages the same programming model as batch \n",
      "processing, allowing users to seamlessly transition between batch and real-time processing. \n",
      "This unified processing model simplifies development and reduces the learning curve for users \n",
      "familiar with Spark.\n",
      "•\t High-level abstractions: Spark Streaming provides high-level abstractions such as DStreams \n",
      "to represent streaming data. DStreams are designed to handle continuous data streams and \n",
      "enable easy integration with existing Spark APIs, libraries, and data sources. These abstractions \n",
      "provide a familiar and expressive programming interface to process real-time data.\n",
      "•\t Fault tolerance and scalability: Spark Streaming offers fault-tolerant processing by leveraging \n",
      "Spark’s RDD abstraction. It automatically recovers from failures by recomputing lost data, \n",
      "ensuring the processing pipeline remains resilient and robust. Additionally, Spark Streaming \n",
      "can scale horizontally by distributing a workload across a cluster of machines, allowing it to \n",
      "handle large-scale data streams effectively.\n",
      "\n",
      "ID Pregunta: Test_2_34\n",
      "\n",
      "Pregunta:\n",
      " Which of the following code blocks returns a copy of the df DataFrame where the name of the state \n",
      "column has been changed to stateID?\n",
      "\n",
      "Opciones:\n",
      " A. df.withColumnRenamed(\"state\", \"stateID\")\n",
      "B. df.withColumnRenamed(\"stateID\", \"state\")\n",
      "C. df.withColumn(\"state\", \"stateID\")\n",
      "D. df.withColumn(\"stateID\", \"state\")\n",
      "\n",
      "Acertó?: False\n",
      "\n",
      "Respuesta generada por el modelo:\n",
      " No existe información\n",
      "\n",
      "Respuesta correcta: A\n",
      "\n",
      "🔎 Chunks usados para responder la pregunta Test_2_34:\n",
      "\n",
      "--- Chunk #1 | Score: 0.5560 ---\n",
      "You don’t always have to update a column in a DataFrame if you only need to rename a column. Now, \n",
      "we will see how we can rename columns in a Spark DataFrame.\n",
      "Renaming columns\n",
      "For changing the name of a column, we would use the withColumnRenamed() function in Spark. \n",
      "We would need to provide the column name that needs to be changed along with the new column \n",
      "name. Here’s the code to illustrate this:\n",
      "data_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\n",
      "data_df.show()\n",
      "As a result, we’ll see the following change:\n",
      "+-----+-----+-------------+----------+------+\n",
      "|col_1|col_2|string_col|col_4| col_6|\n",
      "+-----+-----+-------------+----------+------+\n",
      "|100|200.0|string_test_1|2023-01-01|A|\n",
      "|200|300.0|string_test_2|2023-02-01|A|\n",
      "|300|400.0|string_test_3|2023-03-01|A|\n",
      "+-----+-----+-------------+----------+------+\n",
      "Notice that col_3 is now called string_col after making the change.\n",
      "\n",
      "--- Chunk #2 | Score: 0.5085 ---\n",
      "The following code shows how to delete the columns:\n",
      "drop_column_list = [\"Id\", \"MSZoning\",\"LotConfig\",\"BldgType\", \n",
      "\"Exterior1st\"]\n",
      "df_dropped_cols = df_transformed.select([column for column in df_\n",
      "transformed.columns if column not in drop_column_list])\n",
      "df_dropped_cols.columns\n",
      "Here’s the result:\n",
      "['MSSubClass',\n",
      " 'LotArea',\n",
      " 'OverallCond',\n",
      " 'YearBuilt',\n",
      " 'YearRemodAdd',\n",
      " 'BsmtFinSF2',\n",
      " 'TotalBsmtSF',\n",
      " 'SalePrice',\n",
      " 'MSZoningIndex',\n",
      " 'LotConfigIndex',\n",
      " 'BldgTypeIndex',\n",
      " 'Exterior1stIndex',\n",
      " 'MSZoningVector',\n",
      " 'LotConfigVector',\n",
      " 'BldgTypeVector',\n",
      "\n",
      "--- Chunk #3 | Score: 0.4976 ---\n",
      "This function is \n",
      "used for column-wise operators. If we don’t use this function, our code will return an error.\n",
      "You don’t always have to update a column in a DataFrame if you only need to rename a column. Now, \n",
      "we will see how we can rename columns in a Spark DataFrame.\n",
      "Renaming columns\n",
      "For changing the name of a column, we would use the withColumnRenamed() function in Spark. \n",
      "We would need to provide the column name that needs to be changed along with the new column \n",
      "name.\n",
      "\n",
      "--- Chunk #4 | Score: 0.4924 ---\n",
      "we’ll create another DataFrame named employee_data:\n",
      "employee_data = [(1, \"NY\", \"M\"), \\\n",
      "(2, \"NC\", \"M\"), \\\n",
      "(3, \"NY\", \"F\"), \\\n",
      "(4, \"TX\", \"M\"), \\\n",
      "(5, \"NY\", \"F\"), \\\n",
      "(6, \"AZ\", \"F\") \\\n",
      "]\n",
      "columns= [\"ID\", \"State\", \"Gender\"]\n",
      "employee_data = spark.createDataFrame(data = employee_data, schema = \n",
      "columns)\n",
      "employee_data.show()\n",
      "\n",
      "Advanced Operations and Optimizations in Spark\n",
      "\n",
      "The resulting DataFrame, named employee_data, looks like this:\n",
      "+---+-----+------+\n",
      "| ID|State|Gender|\n",
      "+---+-----+------+\n",
      "|1|NY|M|\n",
      "|2|NC|M|\n",
      "|3|NY|F|\n",
      "|4|TX|M|\n",
      "|5|NY|F|\n",
      "|6|AZ|F|\n",
      "+---+-----+------+\n",
      "Now,\n",
      "\n",
      "--- Chunk #5 | Score: 0.4811 ---\n",
      "named employee_data, looks like this:\n",
      "+---+-----+------+\n",
      "| ID|State|Gender|\n",
      "+---+-----+------+\n",
      "|1|NY|M|\n",
      "|2|NC|M|\n",
      "|3|NY|F|\n",
      "|4|TX|M|\n",
      "|5|NY|F|\n",
      "|6|AZ|F|\n",
      "+---+-----+------+\n",
      "Now, let’s suppose we want to join these two DataFrames together based on the ID column.\n",
      "As we mentioned earlier, Spark offers different types of join operations. We will explore some of them \n",
      "in this chapter. Let’s start with inner joins.\n",
      "Inner joins\n",
      "An inner join is used when we want to join two DataFrames based on values that are common in \n",
      "both DataFrames. Any value that doesn’t exist in any one of the DataFrames would not be part of the \n",
      "resulting DataFrame. By default, the join type is an inner join in Spark.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions_to_evaluate:\n",
    "    response = ask_question_with_options(q, response_mode = \"tree_summarize\")\n",
    "    display_question_result(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
