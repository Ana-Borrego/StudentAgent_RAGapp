{"docstore/data": {"2afa0d72-7f52-4451-a38d-3322fb2a08cf": {"__data__": {"id_": "2afa0d72-7f52-4451-a38d-3322fb2a08cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6917fa1-934d-4975-8793-ef67d7c9acf9", "node_type": "1", "metadata": {}, "hash": "073e59c91492867a2a5a73597cd4160dbf0bf3d0121cb25f8b2e203855ffc2a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding Apache Spark \nand Its Applications\nWith the advent of machine learning and data science, the world is seeing a paradigm shift. A tremendous \namount of data is being collected every second, and it\u2019s hard for computing power to keep up with \nthis pace of rapid data growth. To make use of all this data, Spark has become a de facto standard for \nbig data processing. Migrating data processing to Spark is not only a question of saving resources that \nwill allow you to focus on your business; it\u2019s also a means of modernizing your workloads to leverage \nthe capabilities of Spark and the modern technology stack to create new business opportunities.\nIn this chapter, we will cover the following topics:\n\u2022\t What is Apache Spark?\n\u2022\t Why choose Apache Spark?\n\u2022\t Different components of Spark\n\u2022\t What are the Spark use cases?\n\u2022\t Who are the Spark users?\nWhat is Apache Spark?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6917fa1-934d-4975-8793-ef67d7c9acf9": {"__data__": {"id_": "a6917fa1-934d-4975-8793-ef67d7c9acf9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2afa0d72-7f52-4451-a38d-3322fb2a08cf", "node_type": "1", "metadata": {}, "hash": "c6a422d8e54dd401d26baf9fcfab3d088d33681947d2a30cba851925ec0a1e18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9faf2d21-870f-4215-bdd2-9b39f53126cb", "node_type": "1", "metadata": {}, "hash": "389c42887948d229b1a302b7b66f8c8998881eba9272698cf8d9e1e98a042aa3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this chapter, we will cover the following topics:\n\u2022\t What is Apache Spark?\n\u2022\t Why choose Apache Spark?\n\u2022\t Different components of Spark\n\u2022\t What are the Spark use cases?\n\u2022\t Who are the Spark users?\nWhat is Apache Spark?\nApache Spark is an open-source big data framework that is used for multiple big data applications. The \nstrength of Spark lies in its superior parallel processing capabilities that makes it a leader in its domain.\nAccording to its website (https://spark.apache.org/), \u201cThe most widely-used engine for \nscalable computing.\u201d\n\nUnderstanding Apache Spark and Its Applications\n\nThe history of Apache Spark\nApache Spark started as a research project at the UC Berkeley AMPLab in 2009 and moved to an open \nsource license in 2010. Later, in 2013, it came under the Apache Software Foundation (https://\nspark.apache.org/).", "mimetype": "text/plain", "start_char_idx": 662, "end_char_idx": 1498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9faf2d21-870f-4215-bdd2-9b39f53126cb": {"__data__": {"id_": "9faf2d21-870f-4215-bdd2-9b39f53126cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6917fa1-934d-4975-8793-ef67d7c9acf9", "node_type": "1", "metadata": {}, "hash": "073e59c91492867a2a5a73597cd4160dbf0bf3d0121cb25f8b2e203855ffc2a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f", "node_type": "1", "metadata": {}, "hash": "b96f843f53185f1aaaa5a6530703eec76e2101e782701dfbd22985aba8a9b40f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Later, in 2013, it came under the Apache Software Foundation (https://\nspark.apache.org/). It gained popularity after 2013, and today, it serves as a backbone for \na large number of big data products across various Fortune 500 companies and has thousands of \ndevelopers actively working on it.\nSpark came into being because of limitations in the Hadoop MapReduce framework. MapReduce\u2019s main \npremise was to read data from disk, distribute that data for parallel processing, apply map functions \nto the data, and then reduce those functions and save them back to disk. This back-and-forth reading \nand saving to disk becomes time-consuming and costly very quickly.\nTo overcome this limitation, Spark introduced the concept of in-memory computation. On top of \nthat, Spark has several capabilities that came as a result of different research initiatives. You will read \nmore about them in the next section.", "mimetype": "text/plain", "start_char_idx": 1408, "end_char_idx": 2312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f": {"__data__": {"id_": "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9faf2d21-870f-4215-bdd2-9b39f53126cb", "node_type": "1", "metadata": {}, "hash": "389c42887948d229b1a302b7b66f8c8998881eba9272698cf8d9e1e98a042aa3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fccfcde2-2c44-4979-9a61-b81bcfea9602", "node_type": "1", "metadata": {}, "hash": "5f665659380bf01e48a71b0b719f25b2c4325867beca1f86b5f8465902668bac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This back-and-forth reading \nand saving to disk becomes time-consuming and costly very quickly.\nTo overcome this limitation, Spark introduced the concept of in-memory computation. On top of \nthat, Spark has several capabilities that came as a result of different research initiatives. You will read \nmore about them in the next section.\nUnderstanding Spark differentiators\nSpark\u2019s foundation lies in its major capabilities such as in-memory computation, lazy evaluation, fault \ntolerance, and support for multiple languages such as Python, SQL, Scala, and R. We will discuss each \none of them in detail in the following section.\nLet\u2019s start with in-memory computation.\nIn-memory computation\nThe first major differentiator technology that Spark\u2019s foundation is built on is that it utilizes in-memory \ncomputations. Remember when we discussed Hadoop MapReduce technology? One of its major \nlimitations is to write back to disk at each step.", "mimetype": "text/plain", "start_char_idx": 1976, "end_char_idx": 2914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fccfcde2-2c44-4979-9a61-b81bcfea9602": {"__data__": {"id_": "fccfcde2-2c44-4979-9a61-b81bcfea9602", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f", "node_type": "1", "metadata": {}, "hash": "b96f843f53185f1aaaa5a6530703eec76e2101e782701dfbd22985aba8a9b40f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42fe3c04-6a3a-4434-9e7d-3ad7cda93957", "node_type": "1", "metadata": {}, "hash": "9cac4befd4b659917540578c67fb3deb0d7bd6ee887b5417b334bb9cc70c18b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s start with in-memory computation.\nIn-memory computation\nThe first major differentiator technology that Spark\u2019s foundation is built on is that it utilizes in-memory \ncomputations. Remember when we discussed Hadoop MapReduce technology? One of its major \nlimitations is to write back to disk at each step. Spark saw this as an opportunity for improvement and \nintroduced the concept of in-memory computation. The main idea is that the data remains in memory \nas long as it is worked on. If we can work with the size of data that can be stored in the memory at once, \nwe can eliminate the need to write to disk at each step. As a result, the complete computation cycle \ncan be done in memory if we can work with all computations on that amount of data. Now, the thing \nto note here is that with the advent of big data, it\u2019s hard to contain all the data in memory.", "mimetype": "text/plain", "start_char_idx": 2605, "end_char_idx": 3471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42fe3c04-6a3a-4434-9e7d-3ad7cda93957": {"__data__": {"id_": "42fe3c04-6a3a-4434-9e7d-3ad7cda93957", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fccfcde2-2c44-4979-9a61-b81bcfea9602", "node_type": "1", "metadata": {}, "hash": "5f665659380bf01e48a71b0b719f25b2c4325867beca1f86b5f8465902668bac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0345a04c-8d7e-4d6c-b5b5-9935453a1423", "node_type": "1", "metadata": {}, "hash": "42b24c40da872e350cfb71883b4eed7b366f54de3e86d1fa794c54fb9cc665b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a result, the complete computation cycle \ncan be done in memory if we can work with all computations on that amount of data. Now, the thing \nto note here is that with the advent of big data, it\u2019s hard to contain all the data in memory. Even if we \nlook at heavyweight servers and clusters in the cloud computing world, memory remains finite. This \nis where Spark\u2019s internal framework of parallel processing comes into play. Spark framework utilizes \nthe underlying hardware resources in the most efficient manner. It distributes the computations across \nmultiple cores and utilizes the hardware capabilities to the maximum.\nThis tremendously reduces the computation time, since the overhead of writing to disk and reading it \nback for the subsequent step is minimized as long as the data can be fit in the memory of Spark compute.\n\nWhat is Apache Spark?\n\nLazy evaluation\nGenerally, when we work with programming frameworks, the backend compilers look at each \nstatement and execute it.", "mimetype": "text/plain", "start_char_idx": 3233, "end_char_idx": 4221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0345a04c-8d7e-4d6c-b5b5-9935453a1423": {"__data__": {"id_": "0345a04c-8d7e-4d6c-b5b5-9935453a1423", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42fe3c04-6a3a-4434-9e7d-3ad7cda93957", "node_type": "1", "metadata": {}, "hash": "9cac4befd4b659917540578c67fb3deb0d7bd6ee887b5417b334bb9cc70c18b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f2511fc-3a2b-4d8b-884a-e14b56568452", "node_type": "1", "metadata": {}, "hash": "b9516663300986de68fab27b62a5b675d146b843d5162f52dfdb0da641d2ffe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It distributes the computations across \nmultiple cores and utilizes the hardware capabilities to the maximum.\nThis tremendously reduces the computation time, since the overhead of writing to disk and reading it \nback for the subsequent step is minimized as long as the data can be fit in the memory of Spark compute.\n\nWhat is Apache Spark?\n\nLazy evaluation\nGenerally, when we work with programming frameworks, the backend compilers look at each \nstatement and execute it. While this works great for programming paradigms, with big data and \nparallel processing, we need to shift to a look-ahead kind of model. Spark is well known for its parallel \nprocessing capabilities. To achieve even better performance, Spark doesn\u2019t execute code as it reads it, \nbut once the code is there and we submit a Spark statement to execute, the first step is that Spark builds \na logical map of the queries. Once that map is built, then it plans what the best path of execution is.", "mimetype": "text/plain", "start_char_idx": 3750, "end_char_idx": 4714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f2511fc-3a2b-4d8b-884a-e14b56568452": {"__data__": {"id_": "8f2511fc-3a2b-4d8b-884a-e14b56568452", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0345a04c-8d7e-4d6c-b5b5-9935453a1423", "node_type": "1", "metadata": {}, "hash": "42b24c40da872e350cfb71883b4eed7b366f54de3e86d1fa794c54fb9cc665b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8421a5b-6167-46d5-ad75-d1cba902608c", "node_type": "1", "metadata": {}, "hash": "f889ca2f15029b43933ddb14e72ed87557efe39850693d42eeb7a2eab458f339", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark is well known for its parallel \nprocessing capabilities. To achieve even better performance, Spark doesn\u2019t execute code as it reads it, \nbut once the code is there and we submit a Spark statement to execute, the first step is that Spark builds \na logical map of the queries. Once that map is built, then it plans what the best path of execution is. \nYou will read more about its intricacies in the Spark architecture chapters. Once the plan is established, \nonly then will the execution begin. Once the execution begins, even then, Spark holds off executing \nall statements until it hits an \u201caction\u201d statement. There are two types of statements in Spark:\n\u2022\t Transformations\n\u2022\t Actions\nYou will learn more about the different types of Spark statements in detail in Chapter 3, where we \ndiscuss Spark architecture.", "mimetype": "text/plain", "start_char_idx": 4360, "end_char_idx": 5178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8421a5b-6167-46d5-ad75-d1cba902608c": {"__data__": {"id_": "f8421a5b-6167-46d5-ad75-d1cba902608c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f2511fc-3a2b-4d8b-884a-e14b56568452", "node_type": "1", "metadata": {}, "hash": "b9516663300986de68fab27b62a5b675d146b843d5162f52dfdb0da641d2ffe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea93af9a-efce-40a1-8bf2-0f0168c5505a", "node_type": "1", "metadata": {}, "hash": "3faef5411e3f50a94aeeb7c0faf10dc26c66983033b982c63c5fdfd43801e1a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the plan is established, \nonly then will the execution begin. Once the execution begins, even then, Spark holds off executing \nall statements until it hits an \u201caction\u201d statement. There are two types of statements in Spark:\n\u2022\t Transformations\n\u2022\t Actions\nYou will learn more about the different types of Spark statements in detail in Chapter 3, where we \ndiscuss Spark architecture. Here are a few advantages of lazy evaluation:\n\u2022\t Efficiency\n\u2022\t Code manageability\n\u2022\t Query and resource optimization\n\u2022\t Reduced complexities\nResilient datasets/fault tolerance\nSpark\u2019s foundation is built on resilient distributed datasets (RDDs). It is an immutable distributed \ncollection of objects that represent a set of records. RDDs are distributed across a number of servers, \nand they are computed in parallel across multiple cluster nodes. RDDs can be generated with code.", "mimetype": "text/plain", "start_char_idx": 4793, "end_char_idx": 5659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea93af9a-efce-40a1-8bf2-0f0168c5505a": {"__data__": {"id_": "ea93af9a-efce-40a1-8bf2-0f0168c5505a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8421a5b-6167-46d5-ad75-d1cba902608c", "node_type": "1", "metadata": {}, "hash": "f889ca2f15029b43933ddb14e72ed87557efe39850693d42eeb7a2eab458f339", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0", "node_type": "1", "metadata": {}, "hash": "f49d8616b4452a1b8a658cf0cf3cedd0debe9f2b20d64ed9d1754511d089e91b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is an immutable distributed \ncollection of objects that represent a set of records. RDDs are distributed across a number of servers, \nand they are computed in parallel across multiple cluster nodes. RDDs can be generated with code. \nWhen we read data from an external storage location into Spark, RDDs hold that data. This data can \nbe shared across multiple clusters and can be computed in parallel, thus giving Spark a very efficient \nway of running computations on RDD data. RDDs are loaded in memory for processing; therefore, \nloading to and from memory computations is not required, unlike Hadoop.\nRDDs are fault-tolerant. This means that if there are failures, RDDs have the ability to self-recover. \nSpark achieves that by distributing these RDDs to different worker nodes while keeping in view what \ntask is performed by which worker node. This handling of worker nodes is done by the Spark driver.", "mimetype": "text/plain", "start_char_idx": 5425, "end_char_idx": 6335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0": {"__data__": {"id_": "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea93af9a-efce-40a1-8bf2-0f0168c5505a", "node_type": "1", "metadata": {}, "hash": "3faef5411e3f50a94aeeb7c0faf10dc26c66983033b982c63c5fdfd43801e1a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f186c54-f6c2-44a5-b000-8454a7cd2583", "node_type": "1", "metadata": {}, "hash": "78092afcbbdd060ddb9ef2754e0a218dc12c193bfb89fb18126b67a4bebf67c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RDDs are fault-tolerant. This means that if there are failures, RDDs have the ability to self-recover. \nSpark achieves that by distributing these RDDs to different worker nodes while keeping in view what \ntask is performed by which worker node. This handling of worker nodes is done by the Spark driver. \nWe will discuss this in detail in upcoming chapters.\nRDDs give a lot of power to Spark in terms of resilience and fault-tolerance. This capability, along with \nother features, makes Spark the tool of choice for any production-grade applications.\n\nUnderstanding Apache Spark and Its Applications\n\nMultiple language support\nSpark supports multiple languages for development such as Java, R, Scala, and Python. This gives \nusers the flexibility to use any language of choice to build applications in Spark.\nThe components of Spark\nLet\u2019s talk about the different components Spark has.", "mimetype": "text/plain", "start_char_idx": 6032, "end_char_idx": 6917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f186c54-f6c2-44a5-b000-8454a7cd2583": {"__data__": {"id_": "0f186c54-f6c2-44a5-b000-8454a7cd2583", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0", "node_type": "1", "metadata": {}, "hash": "f49d8616b4452a1b8a658cf0cf3cedd0debe9f2b20d64ed9d1754511d089e91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46aa50c7-6504-4f14-a013-f630ee4f702e", "node_type": "1", "metadata": {}, "hash": "832225c52925e437e8179b41257464eb7dfa8db0803ae70e489f513caf64ea6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This capability, along with \nother features, makes Spark the tool of choice for any production-grade applications.\n\nUnderstanding Apache Spark and Its Applications\n\nMultiple language support\nSpark supports multiple languages for development such as Java, R, Scala, and Python. This gives \nusers the flexibility to use any language of choice to build applications in Spark.\nThe components of Spark\nLet\u2019s talk about the different components Spark has. As you can see in Figure 1.1, Spark Core is the \nbackbone of operations in Spark and spans across all the other components that Spark has. Other \ncomponents that we\u2019re going to discuss in this section are Spark SQL, Spark Streaming, Spark MLlib, \nand GraphX.\nFigure 2.1: Spark components\nLet\u2019s look at the first component of Spark.\nSpark Core\nSpark Core is central to all the other components of Spark. It provides functionalities and core features \nfor all the different components.", "mimetype": "text/plain", "start_char_idx": 6468, "end_char_idx": 7401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46aa50c7-6504-4f14-a013-f630ee4f702e": {"__data__": {"id_": "46aa50c7-6504-4f14-a013-f630ee4f702e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f186c54-f6c2-44a5-b000-8454a7cd2583", "node_type": "1", "metadata": {}, "hash": "78092afcbbdd060ddb9ef2754e0a218dc12c193bfb89fb18126b67a4bebf67c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85d2fc81-458b-4d94-9e69-621021c8db1e", "node_type": "1", "metadata": {}, "hash": "d60a8ba83800c7886db3e908317e6bdea535d5cc3bf40db6d23713ad4addbb6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other \ncomponents that we\u2019re going to discuss in this section are Spark SQL, Spark Streaming, Spark MLlib, \nand GraphX.\nFigure 2.1: Spark components\nLet\u2019s look at the first component of Spark.\nSpark Core\nSpark Core is central to all the other components of Spark. It provides functionalities and core features \nfor all the different components. Spark SQL, Spark Streaming, Spark MLlib, and GraphX all make \nuse of Spark Core as their base. All the functionality and features of Spark are controlled by Spark \nCore. It provides in-memory computing capabilities to deliver speed, a generalized execution model \nto support a wide variety of applications, and Java, Scala, and Python APIs for ease of development.\nIn all of these different components, you can write queries in supported languages. Spark will then \nconvert these queries to directed acyclic graphs (DAGs), and Spark Core has the responsibility of \nexecuting them.", "mimetype": "text/plain", "start_char_idx": 7057, "end_char_idx": 7982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85d2fc81-458b-4d94-9e69-621021c8db1e": {"__data__": {"id_": "85d2fc81-458b-4d94-9e69-621021c8db1e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46aa50c7-6504-4f14-a013-f630ee4f702e", "node_type": "1", "metadata": {}, "hash": "832225c52925e437e8179b41257464eb7dfa8db0803ae70e489f513caf64ea6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "625aafe0-01af-4e9d-876f-a9b193273e54", "node_type": "1", "metadata": {}, "hash": "cc141c13b7ce4dd94eacc130ab5c5dfbc7468aa029461c55598dbf943afa54b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It provides in-memory computing capabilities to deliver speed, a generalized execution model \nto support a wide variety of applications, and Java, Scala, and Python APIs for ease of development.\nIn all of these different components, you can write queries in supported languages. Spark will then \nconvert these queries to directed acyclic graphs (DAGs), and Spark Core has the responsibility of \nexecuting them.\nThe key responsibilities of Spark Core are as follows:\n\u2022\t Interacting with storage systems\n\u2022\t Memory management\n\u2022\t Task distribution\n\nWhat is Apache Spark?\n\n\u2022\t Task scheduling\n\u2022\t Task monitoring\n\u2022\t In-memory computation\n\u2022\t Fault tolerance\n\u2022\t Optimization\nSpark Core contains an API for RDDs which are an integral part of Spark. It also provides different \nAPIs to interact and work with RDDs. All the components of Spark work with underlying RDDs for \ndata manipulation and processing.", "mimetype": "text/plain", "start_char_idx": 7572, "end_char_idx": 8468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "625aafe0-01af-4e9d-876f-a9b193273e54": {"__data__": {"id_": "625aafe0-01af-4e9d-876f-a9b193273e54", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85d2fc81-458b-4d94-9e69-621021c8db1e", "node_type": "1", "metadata": {}, "hash": "d60a8ba83800c7886db3e908317e6bdea535d5cc3bf40db6d23713ad4addbb6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "581d5a9f-fe03-4dca-adbb-836bd8a8daad", "node_type": "1", "metadata": {}, "hash": "b33d2f6c59890f1f83bd697b5e37b23078e7fc14838975254d799ad20f9d9003", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Task scheduling\n\u2022\t Task monitoring\n\u2022\t In-memory computation\n\u2022\t Fault tolerance\n\u2022\t Optimization\nSpark Core contains an API for RDDs which are an integral part of Spark. It also provides different \nAPIs to interact and work with RDDs. All the components of Spark work with underlying RDDs for \ndata manipulation and processing. RDDs make it possible for Spark to have a lineage for data, since \nthey are immutable. This means that every time an operation is run on an RDD that requires changes \nin it, Spark will create a new RDD for it. Hence, it maintains the lineage information of RDDs and \ntheir corresponding operations.\nSpark SQL\nSQL is the most popular language for database and data warehouse applications. Analysts use this \nlanguage for all their exploratory data analysis on relational databases and their counterparts in \ntraditional data warehouses. Spark SQL adds this advantage to the Spark ecosystem.", "mimetype": "text/plain", "start_char_idx": 8140, "end_char_idx": 9058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "581d5a9f-fe03-4dca-adbb-836bd8a8daad": {"__data__": {"id_": "581d5a9f-fe03-4dca-adbb-836bd8a8daad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "625aafe0-01af-4e9d-876f-a9b193273e54", "node_type": "1", "metadata": {}, "hash": "cc141c13b7ce4dd94eacc130ab5c5dfbc7468aa029461c55598dbf943afa54b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d474804-d117-42ce-9cc9-e77f5f16dadb", "node_type": "1", "metadata": {}, "hash": "e5e5fa5410dc343a1a9333a97eca351409657c4ad465e68bad25ba0ace3c00c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hence, it maintains the lineage information of RDDs and \ntheir corresponding operations.\nSpark SQL\nSQL is the most popular language for database and data warehouse applications. Analysts use this \nlanguage for all their exploratory data analysis on relational databases and their counterparts in \ntraditional data warehouses. Spark SQL adds this advantage to the Spark ecosystem. Spark SQL is \nused to query structured data in SQL using the DataFrame API.\nAs its name represents, Spark SQL gives SQL support to Spark. This means we can query the data \npresent in RDDs and other external sources, such as Parquet files. This is a powerful capability of Spark, \nsince it gives developers the flexibility to use a relational table structure on top of RDDs and other file \nformats and write SQL queries on top of it. This also adds the capabilities of using SQL where necessary \nand unifies it with analytics applications and use cases, thus providing unification of the platforms.", "mimetype": "text/plain", "start_char_idx": 8679, "end_char_idx": 9656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d474804-d117-42ce-9cc9-e77f5f16dadb": {"__data__": {"id_": "7d474804-d117-42ce-9cc9-e77f5f16dadb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "581d5a9f-fe03-4dca-adbb-836bd8a8daad", "node_type": "1", "metadata": {}, "hash": "b33d2f6c59890f1f83bd697b5e37b23078e7fc14838975254d799ad20f9d9003", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afb5be51-ce36-454a-9eaa-6c70f3a2103e", "node_type": "1", "metadata": {}, "hash": "730ea9fb949ba2e3210980858ef5b7bf4a1a403594cc26f0465419e3f59e5d60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is a powerful capability of Spark, \nsince it gives developers the flexibility to use a relational table structure on top of RDDs and other file \nformats and write SQL queries on top of it. This also adds the capabilities of using SQL where necessary \nand unifies it with analytics applications and use cases, thus providing unification of the platforms.\nWith Spark SQL, developers are able to do the following with ease:\n\u2022\t They can read data from different file formats and different sources into RDDs and DataFrames\n\u2022\t They can run SQL queries on top of the data present in DataFrames, thus giving flexibility to \nthe developers to use programming languages or SQL to process data\n\u2022\t Once they\u2019re done with the processing of the data, they have the capability to write RDDs and \nDataFrames to external sources\nSpark SQL consists of a cost-based optimizer that optimizes queries, keeping in view the resources; it \nalso has the capability to generate code for these optimizations, which makes these queries very fast \nand efficient.", "mimetype": "text/plain", "start_char_idx": 9298, "end_char_idx": 10336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afb5be51-ce36-454a-9eaa-6c70f3a2103e": {"__data__": {"id_": "afb5be51-ce36-454a-9eaa-6c70f3a2103e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d474804-d117-42ce-9cc9-e77f5f16dadb", "node_type": "1", "metadata": {}, "hash": "e5e5fa5410dc343a1a9333a97eca351409657c4ad465e68bad25ba0ace3c00c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e03dfe0d-b27b-4dec-b027-02ef8723ee8d", "node_type": "1", "metadata": {}, "hash": "6b780f7b2333de969c0d2f67600dd19429efa5c374facf76cfa831930c81b032", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To support even faster query times, it can scale to multiple nodes with the help of Spark \nCore and also provides features such as fault tolerance and resiliency. This is known as the Catalyst \noptimizer. We will read more about it in Chapter 5.", "mimetype": "text/plain", "start_char_idx": 10337, "end_char_idx": 10582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e03dfe0d-b27b-4dec-b027-02ef8723ee8d": {"__data__": {"id_": "e03dfe0d-b27b-4dec-b027-02ef8723ee8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afb5be51-ce36-454a-9eaa-6c70f3a2103e", "node_type": "1", "metadata": {}, "hash": "730ea9fb949ba2e3210980858ef5b7bf4a1a403594cc26f0465419e3f59e5d60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413", "node_type": "1", "metadata": {}, "hash": "a086b59e1d05f71e8212848b8ef98c9aa08c3ecdf0eef73dfc4ea0450fcb28c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To support even faster query times, it can scale to multiple nodes with the help of Spark \nCore and also provides features such as fault tolerance and resiliency. This is known as the Catalyst \noptimizer. We will read more about it in Chapter 5.\n\nUnderstanding Apache Spark and Its Applications\n\nThe most noticeable features of Sparks SQL are as follows:\n\u2022\t It provides an engine for high-level structured APIs\n\u2022\t Reads/writes data to and from a large number of file formats such as Avro, Delta, Comma-\nSeparated Values (CSV), and Parquet\n\u2022\t Provides Open Database Connectivity (ODBC)/Java Database Connectivity (JDBC) \nconnectors to business intelligence (BI) tools such as PowerBI and Tableau, as well as popular \nrelational databases (RDBMs)\n\u2022\t Provides a way to query structured data in files as tables and views\n\u2022\t It supports ANSI SQL:2003-compliant commands and HiveQL\nNow that we have covered SparkSQL, let\u2019s discuss the Spark Streaming component.", "mimetype": "text/plain", "start_char_idx": 10337, "end_char_idx": 11292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413": {"__data__": {"id_": "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e03dfe0d-b27b-4dec-b027-02ef8723ee8d", "node_type": "1", "metadata": {}, "hash": "6b780f7b2333de969c0d2f67600dd19429efa5c374facf76cfa831930c81b032", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71194748-3662-41af-abf5-46b3bcf89469", "node_type": "1", "metadata": {}, "hash": "6d70e3d80fe146f9ef3b7f26f5b369fa6e1279c26b3011842502b68e983985cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Streaming\nWe have talked about the rapid growth of data in today\u2019s times. If we were to divide this data into \ngroups, there are two types of datasets in practice, batch and streaming:\n\u2022\t Batch data is when there\u2019s a chunk of data present that you have to ingest and then transform \nall at once. Think of when you want to get a sales report of all the sales in a month. You would \nhave the monthly data available as a batch and process it all at once.\n\u2022\t Streaming data is when you need output of that data in real time. To serve this requirement, \nyou would have to ingest and process that data in real time. This means every data point can \nbe ingested as a single data element, and we would not wait for it to be ingested after a block \nof data is collected.", "mimetype": "text/plain", "start_char_idx": 11293, "end_char_idx": 12060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71194748-3662-41af-abf5-46b3bcf89469": {"__data__": {"id_": "71194748-3662-41af-abf5-46b3bcf89469", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413", "node_type": "1", "metadata": {}, "hash": "a086b59e1d05f71e8212848b8ef98c9aa08c3ecdf0eef73dfc4ea0450fcb28c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77657998-374f-43cd-8fa7-3ab3b8bb6481", "node_type": "1", "metadata": {}, "hash": "96c7cea3b07a556b82bf22e654ee608d0e49e83ba80c6e313d89ffe36398207f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Streaming data is when you need output of that data in real time. To serve this requirement, \nyou would have to ingest and process that data in real time. This means every data point can \nbe ingested as a single data element, and we would not wait for it to be ingested after a block \nof data is collected. Think of when self-driving cars need to make decisions in real time based \non the data they collect. All the data needs to be ingested and processed in real time for the \ncar to make effective decisions in a given moment.\nThere are a large number of industries generating streaming data. To make use of this data, you need \nreal-time ingestion, processing, and management of this data. It has become essential for organizations \nto use streaming data as it arrives for real-time analytics and other use cases.", "mimetype": "text/plain", "start_char_idx": 11751, "end_char_idx": 12570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77657998-374f-43cd-8fa7-3ab3b8bb6481": {"__data__": {"id_": "77657998-374f-43cd-8fa7-3ab3b8bb6481", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71194748-3662-41af-abf5-46b3bcf89469", "node_type": "1", "metadata": {}, "hash": "6d70e3d80fe146f9ef3b7f26f5b369fa6e1279c26b3011842502b68e983985cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9025230-0639-4a62-89e7-ab148d9c4d46", "node_type": "1", "metadata": {}, "hash": "521cf29a6670aa221493be3655f9c7e7adbfd73c9b78dd8f86fcdfa048321aa3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the data needs to be ingested and processed in real time for the \ncar to make effective decisions in a given moment.\nThere are a large number of industries generating streaming data. To make use of this data, you need \nreal-time ingestion, processing, and management of this data. It has become essential for organizations \nto use streaming data as it arrives for real-time analytics and other use cases. This gives them an edge \nover their competitors, as this allows them to make decisions in real time.\nSpark Streaming enables organizations to make use of streaming data. One of the most important \nfactors of Spark Streaming is its ease of use alongside batch data processing. You can combine batch and \nstream data within one framework and use it to augment your analytics applications. Spark Streaming \nalso inherits Spark Core\u2019s features of resilience and fault tolerance, giving it a dominant position in the \nindustry.", "mimetype": "text/plain", "start_char_idx": 12162, "end_char_idx": 13093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9025230-0639-4a62-89e7-ab148d9c4d46": {"__data__": {"id_": "c9025230-0639-4a62-89e7-ab148d9c4d46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77657998-374f-43cd-8fa7-3ab3b8bb6481", "node_type": "1", "metadata": {}, "hash": "96c7cea3b07a556b82bf22e654ee608d0e49e83ba80c6e313d89ffe36398207f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fb75d86-4dd7-44bc-87cb-50028a021caa", "node_type": "1", "metadata": {}, "hash": "f2b3476cd080ac146bb2138db85bdfa0bf325fcbd22a3e12c89035686b1f6ce9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Streaming enables organizations to make use of streaming data. One of the most important \nfactors of Spark Streaming is its ease of use alongside batch data processing. You can combine batch and \nstream data within one framework and use it to augment your analytics applications. Spark Streaming \nalso inherits Spark Core\u2019s features of resilience and fault tolerance, giving it a dominant position in the \nindustry. It integrates with a large number of streaming data sources such as HDFS, Kafka, and Flume.\nThe beauty of Spark Streaming is that batch data can be processed as streams to take advantage of built-in \nparadigms of streaming data and look-back capabilities. There are certain factors that need to be taken \ninto consideration when we work with real-time data. When we work with real-time data streams, \n\nWhat is Apache Spark?\n\nthere\u2019s a chance that some of the data may get missed due to system hiccups or failures altogether.", "mimetype": "text/plain", "start_char_idx": 12672, "end_char_idx": 13618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6fb75d86-4dd7-44bc-87cb-50028a021caa": {"__data__": {"id_": "6fb75d86-4dd7-44bc-87cb-50028a021caa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9025230-0639-4a62-89e7-ab148d9c4d46", "node_type": "1", "metadata": {}, "hash": "521cf29a6670aa221493be3655f9c7e7adbfd73c9b78dd8f86fcdfa048321aa3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6", "node_type": "1", "metadata": {}, "hash": "c0fde22bdb010655252e5949bf7dd3f1783d557a0f92e606045e286075ed6946", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are certain factors that need to be taken \ninto consideration when we work with real-time data. When we work with real-time data streams, \n\nWhat is Apache Spark?\n\nthere\u2019s a chance that some of the data may get missed due to system hiccups or failures altogether. \nSpark Streaming takes care of this in a seamless way. To cater to these requirements, it has a built-in \nmechanism called checkpoints. The purpose of these checkpoints is to keep track of the incoming \ndata, knowing what was processed downstream and which data is still left to be processed in the next \ncycle. We will learn more about this in Chapter 7 when we discuss Spark Streaming in more detail.\nThis makes Spark resilient to failures. If there are any failures, you need minimal work to reprocess old \ndata. You can also define mechanisms and algorithms for missing data or late processed data.", "mimetype": "text/plain", "start_char_idx": 13350, "end_char_idx": 14221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6": {"__data__": {"id_": "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fb75d86-4dd7-44bc-87cb-50028a021caa", "node_type": "1", "metadata": {}, "hash": "f2b3476cd080ac146bb2138db85bdfa0bf325fcbd22a3e12c89035686b1f6ce9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ca3e729-9fdd-45ba-bf62-de9a769a736b", "node_type": "1", "metadata": {}, "hash": "6a00ea447919046b31621047b81625ecbfd9446ecb1ba2b5dc2411825c6e8a64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will learn more about this in Chapter 7 when we discuss Spark Streaming in more detail.\nThis makes Spark resilient to failures. If there are any failures, you need minimal work to reprocess old \ndata. You can also define mechanisms and algorithms for missing data or late processed data. This gives a \nlot of flexibility to the data pipelines and makes them easier to maintain in large production environments.\nSpark MLlib\nSpark provides a framework for distributed and scalable machine learning. It distributes the computations \nacross different nodes, thus resulting in better performance for model training. It also distributes \nhyperparameter tuning. You will learn more about hyperparameter tuning in Chapter 8, where we \ntalk about machine learning. Because Spark can scale to large datasets, it is the framework of choice \nfor machine learning production pipelines. When you build products, execution and computation \nspeed matter a lot.", "mimetype": "text/plain", "start_char_idx": 13931, "end_char_idx": 14878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ca3e729-9fdd-45ba-bf62-de9a769a736b": {"__data__": {"id_": "7ca3e729-9fdd-45ba-bf62-de9a769a736b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6", "node_type": "1", "metadata": {}, "hash": "c0fde22bdb010655252e5949bf7dd3f1783d557a0f92e606045e286075ed6946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89951293-a6a9-4b08-a15f-b1dc99ff4743", "node_type": "1", "metadata": {}, "hash": "0d52279e22ad457fbf6d5d565f0f67aa4aa551b98427447235e69236820c9bb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It distributes the computations \nacross different nodes, thus resulting in better performance for model training. It also distributes \nhyperparameter tuning. You will learn more about hyperparameter tuning in Chapter 8, where we \ntalk about machine learning. Because Spark can scale to large datasets, it is the framework of choice \nfor machine learning production pipelines. When you build products, execution and computation \nspeed matter a lot. Spark gives you the ability to work with large amounts of data and build state-of-\nthe-art machine learning models that can run very efficiently. Instead of working with models that \ntake days to train, Spark reduces that time to hours. In addition, working with more data results in \nbetter-performing models in most cases.\nMost of the commonly used machine learning algorithms are part of Spark\u2019s libraries.", "mimetype": "text/plain", "start_char_idx": 14431, "end_char_idx": 15288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89951293-a6a9-4b08-a15f-b1dc99ff4743": {"__data__": {"id_": "89951293-a6a9-4b08-a15f-b1dc99ff4743", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ca3e729-9fdd-45ba-bf62-de9a769a736b", "node_type": "1", "metadata": {}, "hash": "6a00ea447919046b31621047b81625ecbfd9446ecb1ba2b5dc2411825c6e8a64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c14088ed-de4e-4045-8cb4-15738b17d83a", "node_type": "1", "metadata": {}, "hash": "572c9bbea44109f9cdda8b618a87d696fd18d6c67cf74a66f91635e4743ee2e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark gives you the ability to work with large amounts of data and build state-of-\nthe-art machine learning models that can run very efficiently. Instead of working with models that \ntake days to train, Spark reduces that time to hours. In addition, working with more data results in \nbetter-performing models in most cases.\nMost of the commonly used machine learning algorithms are part of Spark\u2019s libraries. There are two \nmachine learning packages available in Spark:\n\u2022\t Spark MLlib\n\u2022\t Spark ML\nThe major difference between these two is the type of data they work with. Spark MLlib is built on top \nof RDDs while Spark ML works with DataFrames. Spark MLlib is the older library and has now entered \nmaintenance mode. The more up-to-date library is Spark ML.", "mimetype": "text/plain", "start_char_idx": 14879, "end_char_idx": 15639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c14088ed-de4e-4045-8cb4-15738b17d83a": {"__data__": {"id_": "c14088ed-de4e-4045-8cb4-15738b17d83a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89951293-a6a9-4b08-a15f-b1dc99ff4743", "node_type": "1", "metadata": {}, "hash": "0d52279e22ad457fbf6d5d565f0f67aa4aa551b98427447235e69236820c9bb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e615998-dd28-429a-93b9-73dae6d44046", "node_type": "1", "metadata": {}, "hash": "f944ba56d5c452bf495c873c62a081439738ed60a862d610a4421ec4bf4adb12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are two \nmachine learning packages available in Spark:\n\u2022\t Spark MLlib\n\u2022\t Spark ML\nThe major difference between these two is the type of data they work with. Spark MLlib is built on top \nof RDDs while Spark ML works with DataFrames. Spark MLlib is the older library and has now entered \nmaintenance mode. The more up-to-date library is Spark ML. You should also note that Spark ML \nis not the official name of the library itself, but it is commonly used to refer to the DataFrame-based \nAPI in Spark. The official name is still Spark MLlib. However, it\u2019s important to know the differences.\nSpark MLlib contains the most commonly used machine learning libraries for classification, \nregression, clustering, and recommendation systems. It also has some support for frequent pattern \nmining algorithms.\nWhen there is a need to serve these models to millions and billions of users, Spark is also helpful.", "mimetype": "text/plain", "start_char_idx": 15289, "end_char_idx": 16194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e615998-dd28-429a-93b9-73dae6d44046": {"__data__": {"id_": "2e615998-dd28-429a-93b9-73dae6d44046", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c14088ed-de4e-4045-8cb4-15738b17d83a", "node_type": "1", "metadata": {}, "hash": "572c9bbea44109f9cdda8b618a87d696fd18d6c67cf74a66f91635e4743ee2e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ef8788d-aeb8-44d7-ae94-741ac6e7b271", "node_type": "1", "metadata": {}, "hash": "5fec30fef7bf38b58551a7bd1dc97bcebb77b33f7e3929954a9cfd9f11dcc07f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The official name is still Spark MLlib. However, it\u2019s important to know the differences.\nSpark MLlib contains the most commonly used machine learning libraries for classification, \nregression, clustering, and recommendation systems. It also has some support for frequent pattern \nmining algorithms.\nWhen there is a need to serve these models to millions and billions of users, Spark is also helpful. \nYou can distribute and parallelize both data processing (Extract, Transform, Load (ETL)) and model \nscoring with Spark.\n\nUnderstanding Apache Spark and Its Applications\n\nGraphX\nGraphX is Spark\u2019s API for graphs and graph-parallel computation. GraphX extends Spark\u2019s RDD to \nwork with graphs and allows you to run parallel computations with graph objects. This speeds up the \ncomputations significantly.\nHere\u2019s a network graph that represents what a graph looks like.\nFigure 2.2: A network graph\nA graph is an object with vertices and edges.", "mimetype": "text/plain", "start_char_idx": 15795, "end_char_idx": 16735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ef8788d-aeb8-44d7-ae94-741ac6e7b271": {"__data__": {"id_": "0ef8788d-aeb8-44d7-ae94-741ac6e7b271", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e615998-dd28-429a-93b9-73dae6d44046", "node_type": "1", "metadata": {}, "hash": "f944ba56d5c452bf495c873c62a081439738ed60a862d610a4421ec4bf4adb12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "895ea528-317b-4787-82c4-a4222d01dd4f", "node_type": "1", "metadata": {}, "hash": "01bed3781c9228ffc8d0528fe514f49c7caebeb50d8247c8f92e7f344eadd1e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GraphX extends Spark\u2019s RDD to \nwork with graphs and allows you to run parallel computations with graph objects. This speeds up the \ncomputations significantly.\nHere\u2019s a network graph that represents what a graph looks like.\nFigure 2.2: A network graph\nA graph is an object with vertices and edges. Properties are attached to each vertex and edge. There \nare primary graph operations that Spark supports, such as subgraph and joinVertices.\nThe main premise is that you can use GraphX for exploratory analysis and ETL and transform and \njoin graphs with RDDs efficiently. There are two types of operator\u2014 Graph and GraphOps. On \ntop of that, graph aggregation operators are also available. Spark also includes a number of graph \nalgorithms that are used in common use cases.", "mimetype": "text/plain", "start_char_idx": 16438, "end_char_idx": 17210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "895ea528-317b-4787-82c4-a4222d01dd4f": {"__data__": {"id_": "895ea528-317b-4787-82c4-a4222d01dd4f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ef8788d-aeb8-44d7-ae94-741ac6e7b271", "node_type": "1", "metadata": {}, "hash": "5fec30fef7bf38b58551a7bd1dc97bcebb77b33f7e3929954a9cfd9f11dcc07f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "431d04f7-ec0f-438d-bc97-d3ab5d917981", "node_type": "1", "metadata": {}, "hash": "7335965d451feef158ddfda467ca27a70e17237cc75e26f0ff7f983e5c623b03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main premise is that you can use GraphX for exploratory analysis and ETL and transform and \njoin graphs with RDDs efficiently. There are two types of operator\u2014 Graph and GraphOps. On \ntop of that, graph aggregation operators are also available. Spark also includes a number of graph \nalgorithms that are used in common use cases. Some of the most popular algorithms are as follows:\n\u2022\t PageRank\n\u2022\t Connected components\n\u2022\t Label propagation\n\u2022\t SVD++\n\u2022\t Strongly connected components\n\u2022\t Triangle count\nNow, let\u2019s discuss why we want to use Spark in our applications and what some of the features it \nprovides are.\nWhy choose Apache Spark?\nIn this section, we will discuss the applications of Apache Spark and its features, such as speed, \nreusability, in-memory computations, and how Spark is a unified platform.\n\nWhat are the Spark use cases?", "mimetype": "text/plain", "start_char_idx": 16877, "end_char_idx": 17721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "431d04f7-ec0f-438d-bc97-d3ab5d917981": {"__data__": {"id_": "431d04f7-ec0f-438d-bc97-d3ab5d917981", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "895ea528-317b-4787-82c4-a4222d01dd4f", "node_type": "1", "metadata": {}, "hash": "01bed3781c9228ffc8d0528fe514f49c7caebeb50d8247c8f92e7f344eadd1e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d13972a6-39df-4843-96a1-9e4a22242409", "node_type": "1", "metadata": {}, "hash": "e8328061bdf0f7597c1aaed9ffcaae5d149e59ff7fd3592feaa307cc177aa011", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Why choose Apache Spark?\nIn this section, we will discuss the applications of Apache Spark and its features, such as speed, \nreusability, in-memory computations, and how Spark is a unified platform.\n\nWhat are the Spark use cases?\n\nSpeed\nApache Spark is one of the fastest processing frameworks for data available today. It beats Hadoop \nMapReduce by a large margin. The main reason is its in-memory computation capabilities and lazy \nevaluation. We will learn more about this when we discuss Spark architecture in the next chapter.\nReusability\nReusability is a very important consideration for large organizations making use of modern platforms. \nSpark can join batch and stream data seamlessly. Moreover, you can augment datasets with historical \ndata to serve your use cases better. This gives a large historical view of data to run queries or build \nmodern analytical systems.\nIn-memory computation\nWith in-memory computation, all the overhead of reading and writing to disks is eliminated.", "mimetype": "text/plain", "start_char_idx": 17492, "end_char_idx": 18485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d13972a6-39df-4843-96a1-9e4a22242409": {"__data__": {"id_": "d13972a6-39df-4843-96a1-9e4a22242409", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "431d04f7-ec0f-438d-bc97-d3ab5d917981", "node_type": "1", "metadata": {}, "hash": "7335965d451feef158ddfda467ca27a70e17237cc75e26f0ff7f983e5c623b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6600a00e-bfe9-4cb3-8eff-747df138760e", "node_type": "1", "metadata": {}, "hash": "efda4e74a036b746c9844286e32686c1555b6104d88dbbe7993397df12ae57f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reusability\nReusability is a very important consideration for large organizations making use of modern platforms. \nSpark can join batch and stream data seamlessly. Moreover, you can augment datasets with historical \ndata to serve your use cases better. This gives a large historical view of data to run queries or build \nmodern analytical systems.\nIn-memory computation\nWith in-memory computation, all the overhead of reading and writing to disks is eliminated. The \ndata is cached, and at each step, the required data is already present in memory. At the end of the \nprocessing, results are aggregated and sent back to the driver for further steps.\nAll of this is facilitated by the process of DAG creation that Spark performs inherently. Before execution, \nSpark creates a DAG of the necessary steps and prioritizes them based on its internal algorithms. We \nwill learn more about this in the next chapter. These capabilities support in-memory computation, \nresulting in fast processing speeds.", "mimetype": "text/plain", "start_char_idx": 18024, "end_char_idx": 19020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6600a00e-bfe9-4cb3-8eff-747df138760e": {"__data__": {"id_": "6600a00e-bfe9-4cb3-8eff-747df138760e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d13972a6-39df-4843-96a1-9e4a22242409", "node_type": "1", "metadata": {}, "hash": "e8328061bdf0f7597c1aaed9ffcaae5d149e59ff7fd3592feaa307cc177aa011", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd2b5ffb-9dcd-4894-a023-6c32e08d4956", "node_type": "1", "metadata": {}, "hash": "8849c395c453ae98d8997e132916c69aea019a2f921fb271c68c1ac66d5b0b3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the end of the \nprocessing, results are aggregated and sent back to the driver for further steps.\nAll of this is facilitated by the process of DAG creation that Spark performs inherently. Before execution, \nSpark creates a DAG of the necessary steps and prioritizes them based on its internal algorithms. We \nwill learn more about this in the next chapter. These capabilities support in-memory computation, \nresulting in fast processing speeds.\nA unified platform\nSpark provides a unified platform for data engineering, data science, machine learning, analytics, \nstreaming, and graph processing. All of these components are integrated with Spark Core. The core \nengine is very high-speed and generalizes the commonly needed tasks for its other components. This \ngives Spark an advantage over other platforms because of the unification of its different components. \nThese components can work in conjunction with each other, providing a unified experience for software \napplications.", "mimetype": "text/plain", "start_char_idx": 18573, "end_char_idx": 19558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd2b5ffb-9dcd-4894-a023-6c32e08d4956": {"__data__": {"id_": "bd2b5ffb-9dcd-4894-a023-6c32e08d4956", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6600a00e-bfe9-4cb3-8eff-747df138760e", "node_type": "1", "metadata": {}, "hash": "efda4e74a036b746c9844286e32686c1555b6104d88dbbe7993397df12ae57f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7efac5f-e3a5-4130-b855-7072616332fc", "node_type": "1", "metadata": {}, "hash": "a540b931be0012419b9ffd061c47d3f07841a609cfe0c692d168520c6582058f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of these components are integrated with Spark Core. The core \nengine is very high-speed and generalizes the commonly needed tasks for its other components. This \ngives Spark an advantage over other platforms because of the unification of its different components. \nThese components can work in conjunction with each other, providing a unified experience for software \napplications. In modern applications, this unification makes it easy to use, and different parts of the \napplication can make use of the core capabilities of these components without compromising on features.\nNow that you understand the benefits of using Spark, let\u2019s talk about the different use cases of Spark \nin the industry.\nWhat are the Spark use cases?\nIn this section, we will learn about how Spark is used in the industry. There are various use cases of \nSpark prevalent today, some of which include big data processing, machine learning applications, \nnear-real-time and real-time streaming, and using graph analytics.", "mimetype": "text/plain", "start_char_idx": 19173, "end_char_idx": 20173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7efac5f-e3a5-4130-b855-7072616332fc": {"__data__": {"id_": "d7efac5f-e3a5-4130-b855-7072616332fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd2b5ffb-9dcd-4894-a023-6c32e08d4956", "node_type": "1", "metadata": {}, "hash": "8849c395c453ae98d8997e132916c69aea019a2f921fb271c68c1ac66d5b0b3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f3bd974-df10-46dc-92e7-f9341b6adb91", "node_type": "1", "metadata": {}, "hash": "73addad95eef650e800b165c572affcf59e6cab07909f7fb2d13f4c5812cec45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now that you understand the benefits of using Spark, let\u2019s talk about the different use cases of Spark \nin the industry.\nWhat are the Spark use cases?\nIn this section, we will learn about how Spark is used in the industry. There are various use cases of \nSpark prevalent today, some of which include big data processing, machine learning applications, \nnear-real-time and real-time streaming, and using graph analytics.\n\nUnderstanding Apache Spark and Its Applications\n\nBig data processing\nOne of the most popular use cases for Spark is big data processing. You might be wondering what big \ndata is, so let\u2019s take a look at the components that mark data as big data.\nThe first component of big data is the volume of data. By volume, we mean that the data is very \nlarge in size, often amounting to terabytes, petabytes, and beyond in some cases. Organizations have \ncollected a large amount of data over the years.", "mimetype": "text/plain", "start_char_idx": 19754, "end_char_idx": 20668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f3bd974-df10-46dc-92e7-f9341b6adb91": {"__data__": {"id_": "6f3bd974-df10-46dc-92e7-f9341b6adb91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7efac5f-e3a5-4130-b855-7072616332fc", "node_type": "1", "metadata": {}, "hash": "a540b931be0012419b9ffd061c47d3f07841a609cfe0c692d168520c6582058f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d5074aa-b778-4a5f-9a11-d2440f4256c5", "node_type": "1", "metadata": {}, "hash": "d8f6260cd3c72453a598329cf74d5edea53c5cf19c014d9cf62dcdd084a00e60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You might be wondering what big \ndata is, so let\u2019s take a look at the components that mark data as big data.\nThe first component of big data is the volume of data. By volume, we mean that the data is very \nlarge in size, often amounting to terabytes, petabytes, and beyond in some cases. Organizations have \ncollected a large amount of data over the years. This data can be used for analysis. However, the first \nstep in this activity is to process these large amounts of data. Also, it\u2019s only recently that computing \npower has grown to now be able to process such vast volumes of data.\nThe second component of big data is the velocity of data. The velocity of data refers to the speed of its \ngeneration, ingestion, and distribution. This means that the speed with which this data is generated \nhas increased manifold in recent years.", "mimetype": "text/plain", "start_char_idx": 20312, "end_char_idx": 21148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d5074aa-b778-4a5f-9a11-d2440f4256c5": {"__data__": {"id_": "1d5074aa-b778-4a5f-9a11-d2440f4256c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f3bd974-df10-46dc-92e7-f9341b6adb91", "node_type": "1", "metadata": {}, "hash": "73addad95eef650e800b165c572affcf59e6cab07909f7fb2d13f4c5812cec45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f864c98-1fcc-437d-8d6e-33ea59fb02d5", "node_type": "1", "metadata": {}, "hash": "fcd3d07eb7aa52bb558dfcfe57b916d059025a1602dc83ce167915e64d575b1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, it\u2019s only recently that computing \npower has grown to now be able to process such vast volumes of data.\nThe second component of big data is the velocity of data. The velocity of data refers to the speed of its \ngeneration, ingestion, and distribution. This means that the speed with which this data is generated \nhas increased manifold in recent years. Take, for example, data generated by your smart appliance that \nsends data every second to a server. In this process, the server also needs to keep up with the ingestion \nof this data, and then distributing this across different sources might be the next step.\nThe third component of big data is the variety of data. The variety of data refers to the different sources \nthat generate the data. It also refers to different types of data that are generated. Gone are the days when \ndata was only generated in structured formats that could be saved as tables in databases.", "mimetype": "text/plain", "start_char_idx": 20790, "end_char_idx": 21718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f864c98-1fcc-437d-8d6e-33ea59fb02d5": {"__data__": {"id_": "5f864c98-1fcc-437d-8d6e-33ea59fb02d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d5074aa-b778-4a5f-9a11-d2440f4256c5", "node_type": "1", "metadata": {}, "hash": "d8f6260cd3c72453a598329cf74d5edea53c5cf19c014d9cf62dcdd084a00e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d", "node_type": "1", "metadata": {}, "hash": "26e3ee07a1b5ce6101bf203a366e4b75c64daa09398fd6611dc98ce01f8cf46f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The third component of big data is the variety of data. The variety of data refers to the different sources \nthat generate the data. It also refers to different types of data that are generated. Gone are the days when \ndata was only generated in structured formats that could be saved as tables in databases. Currently, \ndata can be structured, semi-structured, or unstructured. The systems now have to work with all \nthese different data types, and tools should be able to manipulate these different data types. Think of \nimages that need to be processed or audio and video files that can be analyzed with advanced analytics.\nSome other components can be added to the original three Vs as well, such as veracity and value. \nHowever, these components are out of the scope of our discussion.\nBig data is too large for regular machines to process it. That\u2019s why it\u2019s called big data.", "mimetype": "text/plain", "start_char_idx": 21410, "end_char_idx": 22291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d": {"__data__": {"id_": "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f864c98-1fcc-437d-8d6e-33ea59fb02d5", "node_type": "1", "metadata": {}, "hash": "fcd3d07eb7aa52bb558dfcfe57b916d059025a1602dc83ce167915e64d575b1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084", "node_type": "1", "metadata": {}, "hash": "1188f884b92ec817724c191bf79d3993e54db15a2f3d17728bfb62a8af6f880d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Think of \nimages that need to be processed or audio and video files that can be analyzed with advanced analytics.\nSome other components can be added to the original three Vs as well, such as veracity and value. \nHowever, these components are out of the scope of our discussion.\nBig data is too large for regular machines to process it. That\u2019s why it\u2019s called big data. Big data that is \nhigh-volume, high-velocity, and high-variety needs to be processed with advanced analytical tools such \nas Spark, which can distribute the workload across different machines or clusters and does processing \nin parallel to make use of all the resources available on a machine. So, instead of using only a single \nmachine and loading all the data into one node, Spark gives us the ability to divide the data up into \ndifferent parts and process them in parallel and across different machines. This massively speeds up \nthe whole process and makes use of all the available resources.", "mimetype": "text/plain", "start_char_idx": 21923, "end_char_idx": 22890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084": {"__data__": {"id_": "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d", "node_type": "1", "metadata": {}, "hash": "26e3ee07a1b5ce6101bf203a366e4b75c64daa09398fd6611dc98ce01f8cf46f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7773923-c9de-490f-bbc2-5f8f24e17f0e", "node_type": "1", "metadata": {}, "hash": "6e7159833674b93e74950549f85d13755744911708ed331af3d50e53a9168bc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, instead of using only a single \nmachine and loading all the data into one node, Spark gives us the ability to divide the data up into \ndifferent parts and process them in parallel and across different machines. This massively speeds up \nthe whole process and makes use of all the available resources.\nFor all of the aforementioned reasons, Spark is one of the most widely used big data processing \ntechnologies. Large organizations make use of Spark to analyze and manipulate their big data stacks. \nSpark serves as a backbone for big data processing in complex analytical use cases.\nSome examples of big data use cases are as follows:\n\u2022\t Business intelligence for reporting and dashboarding\n\u2022\t Data warehousing for complex applications\n\u2022\t Operational analytics for application monitoring\n\nWhat are the Spark use cases?\n\nIt is important to note here that working with Spark requires a mindset shift from single-node \nprocessing to big data-processing paradigms.", "mimetype": "text/plain", "start_char_idx": 22586, "end_char_idx": 23551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7773923-c9de-490f-bbc2-5f8f24e17f0e": {"__data__": {"id_": "d7773923-c9de-490f-bbc2-5f8f24e17f0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084", "node_type": "1", "metadata": {}, "hash": "1188f884b92ec817724c191bf79d3993e54db15a2f3d17728bfb62a8af6f880d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86f4b4ef-75c3-44d3-9db7-fc0059332b91", "node_type": "1", "metadata": {}, "hash": "c5fe10371013bfed0c8c3e556ec063b9f77d2286ade9debd601ce2b24ad1981c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark serves as a backbone for big data processing in complex analytical use cases.\nSome examples of big data use cases are as follows:\n\u2022\t Business intelligence for reporting and dashboarding\n\u2022\t Data warehousing for complex applications\n\u2022\t Operational analytics for application monitoring\n\nWhat are the Spark use cases?\n\nIt is important to note here that working with Spark requires a mindset shift from single-node \nprocessing to big data-processing paradigms. You now have to start thinking about how to best utilize \nand optimize the use of large clusters for processing and what some of the best practices around \nparallel processing are.\nMachine learning applications\nAs data grows, so does the need for machine learning models to make use of more and more data. The \ngeneral understanding in the machine learning community today is that the more data is provided to \nthe models, the better the models will be. This resulted in the need for massive amounts of data to be \ngiven to a model for predictive analytics.", "mimetype": "text/plain", "start_char_idx": 23090, "end_char_idx": 24109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86f4b4ef-75c3-44d3-9db7-fc0059332b91": {"__data__": {"id_": "86f4b4ef-75c3-44d3-9db7-fc0059332b91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7773923-c9de-490f-bbc2-5f8f24e17f0e", "node_type": "1", "metadata": {}, "hash": "6e7159833674b93e74950549f85d13755744911708ed331af3d50e53a9168bc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61eb9bc5-1853-493a-b728-b123ac9cd444", "node_type": "1", "metadata": {}, "hash": "ade5096252ddedc161c52ec1813211ba83ef017f5c5858222acd39dc3f5571cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine learning applications\nAs data grows, so does the need for machine learning models to make use of more and more data. The \ngeneral understanding in the machine learning community today is that the more data is provided to \nthe models, the better the models will be. This resulted in the need for massive amounts of data to be \ngiven to a model for predictive analytics. When we deal with massive amounts of data, the challenges \nfor training machine learning models become more complex than data processing. The reason is that \nmachine learning models crunch data and run statistical estimations to come to a minimum error \npoint. To get to that minimum error, the model must do complex mathematical operations such as \nmatrix multiplication. These computations require large amounts of data to be available in memory \nand then computations to run on it. This serves as the case for parallel processing in machine learning.\nMachine learning adds an element of prediction to products.", "mimetype": "text/plain", "start_char_idx": 23733, "end_char_idx": 24723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61eb9bc5-1853-493a-b728-b123ac9cd444": {"__data__": {"id_": "61eb9bc5-1853-493a-b728-b123ac9cd444", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86f4b4ef-75c3-44d3-9db7-fc0059332b91", "node_type": "1", "metadata": {}, "hash": "c5fe10371013bfed0c8c3e556ec063b9f77d2286ade9debd601ce2b24ad1981c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6128816-6a90-46fb-a9d6-cbdef2f47640", "node_type": "1", "metadata": {}, "hash": "892b2af62417db25b90d3ad8845e3cd01f717585da841114f792ae5e5ff7eaa1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To get to that minimum error, the model must do complex mathematical operations such as \nmatrix multiplication. These computations require large amounts of data to be available in memory \nand then computations to run on it. This serves as the case for parallel processing in machine learning.\nMachine learning adds an element of prediction to products. Instead of reacting to the changes that \nhave already taken place, we can proactively look for ways to improve our products and services based \non historical data and trends. Every aspect of an organization can make use of machine learning \nfor predictive analytics. Machine learning can be applied to a number of industries, from hospitals \nto retail stores to manufacturing organizations. All of us have encountered some kind of machine \nlearning algorithms when we do tasks on the internet, such as online buying and selling, browsing \nand searching for websites, and using social media platforms. Machine learning has become a major \npart of our lives knowingly or unknowingly.", "mimetype": "text/plain", "start_char_idx": 24371, "end_char_idx": 25405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6128816-6a90-46fb-a9d6-cbdef2f47640": {"__data__": {"id_": "c6128816-6a90-46fb-a9d6-cbdef2f47640", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61eb9bc5-1853-493a-b728-b123ac9cd444", "node_type": "1", "metadata": {}, "hash": "ade5096252ddedc161c52ec1813211ba83ef017f5c5858222acd39dc3f5571cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2d4acba-d7c4-4788-9b3c-e928b0c37782", "node_type": "1", "metadata": {}, "hash": "e94b17fb4505f0510a0bf684370c5f13581ec1a21c499e5295b893c2622869ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine learning can be applied to a number of industries, from hospitals \nto retail stores to manufacturing organizations. All of us have encountered some kind of machine \nlearning algorithms when we do tasks on the internet, such as online buying and selling, browsing \nand searching for websites, and using social media platforms. Machine learning has become a major \npart of our lives knowingly or unknowingly.\nAlthough there\u2019s a large number of use cases that an organization can make use of in terms of machine \nlearning, I\u2019m highlighting only a few here:\n\u2022\t Personalized shopping\n\u2022\t Website searches and ranking\n\u2022\t Fraud detection for banking and insurance\n\u2022\t Customer sentiment analysis\n\u2022\t Customer segmentation\n\u2022\t Recommendation engines\n\u2022\t Price optimization\n\u2022\t Predictive maintenance and support\n\u2022\t Text and video analytics\n\u2022\t Customer/patient 360\n\nUnderstanding Apache Spark and Its Applications\n\nLet\u2019s move on to cover real-time streaming next.", "mimetype": "text/plain", "start_char_idx": 24991, "end_char_idx": 25947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2d4acba-d7c4-4788-9b3c-e928b0c37782": {"__data__": {"id_": "e2d4acba-d7c4-4788-9b3c-e928b0c37782", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6128816-6a90-46fb-a9d6-cbdef2f47640", "node_type": "1", "metadata": {}, "hash": "892b2af62417db25b90d3ad8845e3cd01f717585da841114f792ae5e5ff7eaa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2", "node_type": "1", "metadata": {}, "hash": "643667771d43320cf2ee21436dc0aa59311e056d7b739fe030603aa0233b14d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Real-time streaming\nReal-time streaming is one of the use cases where Spark really shines. There are very few competing \nframeworks that offer the flexibility that Spark Streaming has.\nSpark Streaming provides a mechanism to ingest data from multiple streaming data sources, such \nas Kafka and Amazon Kinesis. Once the data is ingested, it can be processed in real time with very \nefficient Spark Streaming processing.\nThere are a large number of real-time use cases that can make use of Spark Streaming.", "mimetype": "text/plain", "start_char_idx": 25948, "end_char_idx": 26452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2": {"__data__": {"id_": "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2d4acba-d7c4-4788-9b3c-e928b0c37782", "node_type": "1", "metadata": {}, "hash": "e94b17fb4505f0510a0bf684370c5f13581ec1a21c499e5295b893c2622869ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48a046e4-1c7a-473e-998d-245b521c1691", "node_type": "1", "metadata": {}, "hash": "993445754f88548a087c910de02a97935bf58cd5123370f6362967d172431686", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are very few competing \nframeworks that offer the flexibility that Spark Streaming has.\nSpark Streaming provides a mechanism to ingest data from multiple streaming data sources, such \nas Kafka and Amazon Kinesis. Once the data is ingested, it can be processed in real time with very \nefficient Spark Streaming processing.\nThere are a large number of real-time use cases that can make use of Spark Streaming. Some of them \nare as follows:\n\u2022\t Self-driving cars\n\u2022\t Real-time reporting and analysis\n\u2022\t Providing updates on stock market data\n\u2022\t Internet of Things (IoT) data ingestion and processing\n\u2022\t Real-time news data processing\n\u2022\t Real-time analytics for optimization of inventory and operations\n\u2022\t Real-time fraud detection systems for credit cards\n\u2022\t Real-time event detection\n\u2022\t Real-time recommendations\nLarge global organizations make use of Spark Streaming to process billion and trillions of data rows \nin real time.", "mimetype": "text/plain", "start_char_idx": 26039, "end_char_idx": 26969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48a046e4-1c7a-473e-998d-245b521c1691": {"__data__": {"id_": "48a046e4-1c7a-473e-998d-245b521c1691", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2", "node_type": "1", "metadata": {}, "hash": "643667771d43320cf2ee21436dc0aa59311e056d7b739fe030603aa0233b14d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699", "node_type": "1", "metadata": {}, "hash": "f426fe4be86449f06bf06ff7ee31b11b8ed2824d80cb2a4857941807111c005a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We see some of this in action in our everyday life. Your credit card blocking a transaction \nwhile you\u2019re out shopping is one such example of real-time fraud detection in action. Netflix and \nYouTube use real-time interactions, with the video platforms recommending users what to watch next.\nAs we move to a world of every device sending data back to its server for analysis, there\u2019s an increased \nneed for streaming and real-time analysis. One of the main advantages of using Spark Streaming for this \nkind of data is the built-in capabilities it has, for look-back and late processing of data. We discussed \nthe usefulness of this approach earlier as well, and a lot of manual pipeline processing work is removed \ndue to these capabilities. We will learn more about this when we discuss Spark Streaming in Chapter 7.\nGraph analytics\nGraph analytics provides a unique way of looking at data by analyzing relationships between different \nentities.", "mimetype": "text/plain", "start_char_idx": 26970, "end_char_idx": 27917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699": {"__data__": {"id_": "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48a046e4-1c7a-473e-998d-245b521c1691", "node_type": "1", "metadata": {}, "hash": "993445754f88548a087c910de02a97935bf58cd5123370f6362967d172431686", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85c9e32b-8614-4f2a-8762-3c97bf6d081f", "node_type": "1", "metadata": {}, "hash": "eec9c2c3c508dad48172df0b482d7029e4408a9a89ce6ba95c8830d1c33a041f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We discussed \nthe usefulness of this approach earlier as well, and a lot of manual pipeline processing work is removed \ndue to these capabilities. We will learn more about this when we discuss Spark Streaming in Chapter 7.\nGraph analytics\nGraph analytics provides a unique way of looking at data by analyzing relationships between different \nentities. The vertices of a graph represent the entities and the edges of a graph represent the relationship \nbetween two entities. Think of your social network on Facebook or Instagram. You represent one \n\nWho are the Spark users?\n\nentity, and the people you are connected to represent another entity. The relationship (connection) \nbetween you and your friends is the edge. Similarly, your interests on your social media could all \nbe different edges. Then, there can be a location category, for which all people who belong to one \nlocation would have an edge (a relationship) with that location, and so on.", "mimetype": "text/plain", "start_char_idx": 27566, "end_char_idx": 28517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85c9e32b-8614-4f2a-8762-3c97bf6d081f": {"__data__": {"id_": "85c9e32b-8614-4f2a-8762-3c97bf6d081f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699", "node_type": "1", "metadata": {}, "hash": "f426fe4be86449f06bf06ff7ee31b11b8ed2824d80cb2a4857941807111c005a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5496aa0c-7d5b-4b69-8313-5c153bd66a8d", "node_type": "1", "metadata": {}, "hash": "5915e13e832c849dddd94d92af86bb2fdc3b7739514723178c28736c3dcd3bca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "entity, and the people you are connected to represent another entity. The relationship (connection) \nbetween you and your friends is the edge. Similarly, your interests on your social media could all \nbe different edges. Then, there can be a location category, for which all people who belong to one \nlocation would have an edge (a relationship) with that location, and so on. Therefore, connections can \nbe made with any different type of entities. The more connected you are, the higher the chance that \nyou are connected to like-minded people or interests. This is one method of measuring relationships \nbetween different entities. There are several uses for these kinds of graphs. The beauty of Spark is the \ndistributed processing of these graphs to find these relationships very quickly. There can be millions \nand billions of connections for billions of entities. Spark has the capability to distribute these workloads \nand compute complex algorithms very fast.", "mimetype": "text/plain", "start_char_idx": 28141, "end_char_idx": 29109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5496aa0c-7d5b-4b69-8313-5c153bd66a8d": {"__data__": {"id_": "5496aa0c-7d5b-4b69-8313-5c153bd66a8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85c9e32b-8614-4f2a-8762-3c97bf6d081f", "node_type": "1", "metadata": {}, "hash": "eec9c2c3c508dad48172df0b482d7029e4408a9a89ce6ba95c8830d1c33a041f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275", "node_type": "1", "metadata": {}, "hash": "a75519c37d712ac912d359b822eb87e63727d019a36d324b69a05e9fe60aaadd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is one method of measuring relationships \nbetween different entities. There are several uses for these kinds of graphs. The beauty of Spark is the \ndistributed processing of these graphs to find these relationships very quickly. There can be millions \nand billions of connections for billions of entities. Spark has the capability to distribute these workloads \nand compute complex algorithms very fast.\nThe following are some use cases of graph analytics:\n\u2022\t Social network analysis\n\u2022\t Fraud detection\n\u2022\t Page ranking based on relevance\n\u2022\t Weather prediction\n\u2022\t Search engine optimization\n\u2022\t Supply chain analysis\n\u2022\t Finding influencers on social media\n\u2022\t Money laundering and fraud detection\nWith a growing number of use cases of graph analytics, this proves to be a critical use case in the \nindustry today where we need to analyze networks of relationships among entities.\nIn the next section, we\u2019re going to discuss who the Spark users are and what their typical role is within \nan organization.", "mimetype": "text/plain", "start_char_idx": 28701, "end_char_idx": 29706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275": {"__data__": {"id_": "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5496aa0c-7d5b-4b69-8313-5c153bd66a8d", "node_type": "1", "metadata": {}, "hash": "5915e13e832c849dddd94d92af86bb2fdc3b7739514723178c28736c3dcd3bca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08fbe868-004f-4339-a7c9-c37576f9a7fb", "node_type": "1", "metadata": {}, "hash": "39ab9c891105eaae7c349414fb31fd361253abe1530c5b09da9baad4de48bb3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next section, we\u2019re going to discuss who the Spark users are and what their typical role is within \nan organization.\nWho are the Spark users?\nAs the world moves toward data-driven decision-making approaches, the role of data and the different \ntypes of users who can leverage it for critical business decisions has become paramount. There are \ndifferent types of users in data who can leverage Spark for different purposes. I will introduce some \nof those different users in this section. This is not an exhaustive list, but it should give you an idea of \nthe different roles that exist in data-driven organizations today. However, as the industry grows, many \nmore new roles are coming up that are similar to the ones present in the following sections, although \neach may have its own separate role.\nWe\u2019ll start with the role of data analysts.\n\nUnderstanding Apache Spark and Its Applications\n\nData analysts\nThe more traditional role in data today is a data analyst.", "mimetype": "text/plain", "start_char_idx": 29583, "end_char_idx": 30557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08fbe868-004f-4339-a7c9-c37576f9a7fb": {"__data__": {"id_": "08fbe868-004f-4339-a7c9-c37576f9a7fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275", "node_type": "1", "metadata": {}, "hash": "a75519c37d712ac912d359b822eb87e63727d019a36d324b69a05e9fe60aaadd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "091a7f7b-ca71-4b81-927a-845f08daf909", "node_type": "1", "metadata": {}, "hash": "778776fb4a5fb75e51a80481f06dff3208569c6cfb8b46541f8acba6e7aba0a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, as the industry grows, many \nmore new roles are coming up that are similar to the ones present in the following sections, although \neach may have its own separate role.\nWe\u2019ll start with the role of data analysts.\n\nUnderstanding Apache Spark and Its Applications\n\nData analysts\nThe more traditional role in data today is a data analyst. The data analyst is typically the first-tier role \nin data. What this means is that data analysts are at the core of decision making in organizations. This \nrole spans across different business units in an organization, and oftentimes, data analysts have to \ninteract with multiple business stakeholders to put across their requirements. This requires knowledge \nof the business domain as well as its processes. When an analyst has an understanding of the business \nand its goals, only then can they perform their duties best.", "mimetype": "text/plain", "start_char_idx": 30213, "end_char_idx": 31084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "091a7f7b-ca71-4b81-927a-845f08daf909": {"__data__": {"id_": "091a7f7b-ca71-4b81-927a-845f08daf909", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08fbe868-004f-4339-a7c9-c37576f9a7fb", "node_type": "1", "metadata": {}, "hash": "39ab9c891105eaae7c349414fb31fd361253abe1530c5b09da9baad4de48bb3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd", "node_type": "1", "metadata": {}, "hash": "19d4092e6c366d4f979083cfad82d0db85cfc6c9ae4a1cee1d895069717e43df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This \nrole spans across different business units in an organization, and oftentimes, data analysts have to \ninteract with multiple business stakeholders to put across their requirements. This requires knowledge \nof the business domain as well as its processes. When an analyst has an understanding of the business \nand its goals, only then can they perform their duties best. Moreover, a lot of times, the requirement \nis to make current processes more efficient, which results in a better bottom line for the business. \nTherefore, having an understanding of not just the business goals but also how it all works together \nis one of the main requirements for this role.\nA typical job role for a data analyst may look as follows:\n1.\t\nWhen data analysts are given a project in an organization, the first step in the project is to gather \nrequirements from multiple stakeholders. Let\u2019s work with an example here. Say you joined an \norganization as a data analyst.", "mimetype": "text/plain", "start_char_idx": 30709, "end_char_idx": 31669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd": {"__data__": {"id_": "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "091a7f7b-ca71-4b81-927a-845f08daf909", "node_type": "1", "metadata": {}, "hash": "778776fb4a5fb75e51a80481f06dff3208569c6cfb8b46541f8acba6e7aba0a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f", "node_type": "1", "metadata": {}, "hash": "67a52246bf036c988a1fc85236eb4ab27a9b83bf1022894b30bc7fb106ec5309", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A typical job role for a data analyst may look as follows:\n1.\t\nWhen data analysts are given a project in an organization, the first step in the project is to gather \nrequirements from multiple stakeholders. Let\u2019s work with an example here. Say you joined an \norganization as a data analyst. This organization makes and sells computer hardware. You are \ngiven the task of reporting on the revenue each month for the last 10 years. The first step for \nyou would be to gather all requirements. It is possible that some stakeholders want to know how \nmany units of certain products are sold each month, while others may want to know whether \nthe revenues are consistently growing or not. Remember, the end users of your reports might \nwork in different business units of the organization.\n2.", "mimetype": "text/plain", "start_char_idx": 31379, "end_char_idx": 32166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f": {"__data__": {"id_": "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd", "node_type": "1", "metadata": {}, "hash": "19d4092e6c366d4f979083cfad82d0db85cfc6c9ae4a1cee1d895069717e43df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc45d385-72c3-4b81-98bf-d87784465900", "node_type": "1", "metadata": {}, "hash": "0d5c7f5e317885010f33f4be6f7d5db383de01b178f09fc5ceb2a5f8491a3e51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first step for \nyou would be to gather all requirements. It is possible that some stakeholders want to know how \nmany units of certain products are sold each month, while others may want to know whether \nthe revenues are consistently growing or not. Remember, the end users of your reports might \nwork in different business units of the organization.\n2.\t\nOnce you have all the requirements gathered from all the concerned stakeholders, then you \nmove on to the next step, which is to look for the relevant data sources to answer the questions \nthat you are tasked with. You may need to talk with database administrators in the organization \nor platform architects to know where the different data sources reside that have relevant \ninformation for you to extract.\n3.\t\nOnce you have all the relevant sources, then you want to connect with those sources \nprogrammatically (in most cases) and clean and join some data together to come up with \nrelevant statistics, based on your requirements.", "mimetype": "text/plain", "start_char_idx": 31809, "end_char_idx": 32802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc45d385-72c3-4b81-98bf-d87784465900": {"__data__": {"id_": "dc45d385-72c3-4b81-98bf-d87784465900", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f", "node_type": "1", "metadata": {}, "hash": "67a52246bf036c988a1fc85236eb4ab27a9b83bf1022894b30bc7fb106ec5309", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2f22521-0da6-4d25-904b-1019b606b2a7", "node_type": "1", "metadata": {}, "hash": "7f6840d12aac308206543c26318c913237068e9cd15896431a28f0cb9c14e376", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You may need to talk with database administrators in the organization \nor platform architects to know where the different data sources reside that have relevant \ninformation for you to extract.\n3.\t\nOnce you have all the relevant sources, then you want to connect with those sources \nprogrammatically (in most cases) and clean and join some data together to come up with \nrelevant statistics, based on your requirements. This is where Spark would help you connect \nto these different data sources and also read and manipulate the data most efficiently. You also \nwant to slice and dice the data based on your business requirements. Once the data is clean and \nstatistics are generated, you want to generate some reports based on these statistics. There are \ndifferent tools in the market to generate reports, such as Qlik and Tableau, that you can work \nwith. Once the reports are generated, you may want to share your results with the stakeholders.", "mimetype": "text/plain", "start_char_idx": 32383, "end_char_idx": 33331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2f22521-0da6-4d25-904b-1019b606b2a7": {"__data__": {"id_": "a2f22521-0da6-4d25-904b-1019b606b2a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc45d385-72c3-4b81-98bf-d87784465900", "node_type": "1", "metadata": {}, "hash": "0d5c7f5e317885010f33f4be6f7d5db383de01b178f09fc5ceb2a5f8491a3e51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcf50002-0242-47e1-8c2d-7bf4523eeeb5", "node_type": "1", "metadata": {}, "hash": "e5b08749b92abbec4737b4126505838bb149280f94ad24c3a1dae53930a29d6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You also \nwant to slice and dice the data based on your business requirements. Once the data is clean and \nstatistics are generated, you want to generate some reports based on these statistics. There are \ndifferent tools in the market to generate reports, such as Qlik and Tableau, that you can work \nwith. Once the reports are generated, you may want to share your results with the stakeholders. \nYou could present your results to them or share the reports with them, depending on what the \npreferred medium is. This will help stakeholders make informed business-critical decisions \nthat are data-driven in nature.\nCollaboration across different roles also plays an important role for data analysts. Since organizations \nhave been collecting data for a long time, the most important thing is working with all the data that \nhas been collected over the years and making sense of it, helping businesses with critical decision \nmaking.", "mimetype": "text/plain", "start_char_idx": 32935, "end_char_idx": 33868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcf50002-0242-47e1-8c2d-7bf4523eeeb5": {"__data__": {"id_": "fcf50002-0242-47e1-8c2d-7bf4523eeeb5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2f22521-0da6-4d25-904b-1019b606b2a7", "node_type": "1", "metadata": {}, "hash": "7f6840d12aac308206543c26318c913237068e9cd15896431a28f0cb9c14e376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8", "node_type": "1", "metadata": {}, "hash": "0ab82ba63f08155b78f2c2c9a112d5d42c6d8234cf36eeda323a5a22c1ef5f0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This will help stakeholders make informed business-critical decisions \nthat are data-driven in nature.\nCollaboration across different roles also plays an important role for data analysts. Since organizations \nhave been collecting data for a long time, the most important thing is working with all the data that \nhas been collected over the years and making sense of it, helping businesses with critical decision \nmaking. Helping with data-driven decision making is the key to being a successful data analyst.\n\nWho are the Spark users?\n\nHere\u2019s a summary of the steps taken in a project, as discussed in the previous paragraphs:\n1.\t\nGather requirements from stakeholders.\n2.\t\nIdentify the relevant data sources.\n3.\t\nCollaborate with subject matter experts (SMEs).\n4.\t\nSlice and dice data.\n5.\t\nGenerate reports.\n6.\t\nShare the results.\nLet\u2019s look at data engineers next. This role is gaining a lot of traction in the industry today.", "mimetype": "text/plain", "start_char_idx": 33448, "end_char_idx": 34376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8": {"__data__": {"id_": "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcf50002-0242-47e1-8c2d-7bf4523eeeb5", "node_type": "1", "metadata": {}, "hash": "e5b08749b92abbec4737b4126505838bb149280f94ad24c3a1dae53930a29d6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c7f513d-77ff-4298-bd80-35e20c97617d", "node_type": "1", "metadata": {}, "hash": "bfda49ae1ce4b3f2d86ccd625e7338354311615fbef5f147f617062b76273772", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gather requirements from stakeholders.\n2.\t\nIdentify the relevant data sources.\n3.\t\nCollaborate with subject matter experts (SMEs).\n4.\t\nSlice and dice data.\n5.\t\nGenerate reports.\n6.\t\nShare the results.\nLet\u2019s look at data engineers next. This role is gaining a lot of traction in the industry today.\nData engineers\nThe next role that is getting more and more prevalent in the industry is a data engineer. This is a \nrelatively new role but has gained immense popularity in recent times. The reason for this is that \ndata is growing at tremendous levels. We have more data being generated per second now than in a \nwhole month a few years ago. Working with all this data requires specialized skills. The data can no \nlonger be contained in the modest memory of most computers, so we have to make use of the massive \nscale of cloud computing to serve this purpose.", "mimetype": "text/plain", "start_char_idx": 34079, "end_char_idx": 34939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c7f513d-77ff-4298-bd80-35e20c97617d": {"__data__": {"id_": "5c7f513d-77ff-4298-bd80-35e20c97617d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8", "node_type": "1", "metadata": {}, "hash": "0ab82ba63f08155b78f2c2c9a112d5d42c6d8234cf36eeda323a5a22c1ef5f0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e23e7096-4355-4676-8497-4f0585b9255c", "node_type": "1", "metadata": {}, "hash": "5d481cbd7bdfce9e0242bf68a9f0dc399a4552f807d94dc43480ecb35ecd3f8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reason for this is that \ndata is growing at tremendous levels. We have more data being generated per second now than in a \nwhole month a few years ago. Working with all this data requires specialized skills. The data can no \nlonger be contained in the modest memory of most computers, so we have to make use of the massive \nscale of cloud computing to serve this purpose. As data needs are becoming a lot more complex, we \nneed complex architectures to process and use this data for business decision making. This is where \nthe role of the data engineer comes into play. The main job of the data engineer is to prepare data \nfor ingestion for different purposes. The downstream systems that leverage this prepared data could \nbe dashboards that run reports based on this data, or it could be a predictive analytics solution that \nworks with advanced machine learning algorithms to make proactive decisions based on the data.", "mimetype": "text/plain", "start_char_idx": 34564, "end_char_idx": 35492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e23e7096-4355-4676-8497-4f0585b9255c": {"__data__": {"id_": "e23e7096-4355-4676-8497-4f0585b9255c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c7f513d-77ff-4298-bd80-35e20c97617d", "node_type": "1", "metadata": {}, "hash": "bfda49ae1ce4b3f2d86ccd625e7338354311615fbef5f147f617062b76273772", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad757761-937b-46db-a109-75fc66bce2e5", "node_type": "1", "metadata": {}, "hash": "a8632ea92634dfc374a275aac84e74ef5c880fcb3501fb932f208d15de521066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is where \nthe role of the data engineer comes into play. The main job of the data engineer is to prepare data \nfor ingestion for different purposes. The downstream systems that leverage this prepared data could \nbe dashboards that run reports based on this data, or it could be a predictive analytics solution that \nworks with advanced machine learning algorithms to make proactive decisions based on the data.\nMore broadly, data engineers are responsible for creating, maintaining, optimizing, and monitoring \ndata pipelines that serve different use cases in an organization. These pipelines are typically known as \nExtract, Transform, Load (ETL) pipelines. The major differentiator is the sheer scale of data that data \nengineers have to work with. When there are downstream needs for data for BI reporting, advanced \nanalytics, and/or machine learning, that is where data pipelines come into play for large projects.\nA typical job role for a data engineer in an organization may look as follows.", "mimetype": "text/plain", "start_char_idx": 35077, "end_char_idx": 36080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad757761-937b-46db-a109-75fc66bce2e5": {"__data__": {"id_": "ad757761-937b-46db-a109-75fc66bce2e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e23e7096-4355-4676-8497-4f0585b9255c", "node_type": "1", "metadata": {}, "hash": "5d481cbd7bdfce9e0242bf68a9f0dc399a4552f807d94dc43480ecb35ecd3f8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fa74f19-76a5-40f7-8410-3a0c5c8cad31", "node_type": "1", "metadata": {}, "hash": "da47be949adeb873231fc1a29c031b07de745ef0be6748a9ff28293be67de75b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These pipelines are typically known as \nExtract, Transform, Load (ETL) pipelines. The major differentiator is the sheer scale of data that data \nengineers have to work with. When there are downstream needs for data for BI reporting, advanced \nanalytics, and/or machine learning, that is where data pipelines come into play for large projects.\nA typical job role for a data engineer in an organization may look as follows. When data engineers are \ngiven a task to create a data pipeline for a project, the first thing they need to consider is the overall \narchitecture of an application. There might be data architects in some organizations to help with some \nof the architecture requirements, but that might not always be the case. So, a data engineer would ask \nquestions such as the following:\n\u2022\t What are the different sources of data?\n\u2022\t What is the size of the data?\n\u2022\t Where does the data reside today?", "mimetype": "text/plain", "start_char_idx": 35659, "end_char_idx": 36567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fa74f19-76a5-40f7-8410-3a0c5c8cad31": {"__data__": {"id_": "8fa74f19-76a5-40f7-8410-3a0c5c8cad31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad757761-937b-46db-a109-75fc66bce2e5", "node_type": "1", "metadata": {}, "hash": "a8632ea92634dfc374a275aac84e74ef5c880fcb3501fb932f208d15de521066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173", "node_type": "1", "metadata": {}, "hash": "e843d78f826b3fd984fc30abbc677d1c1bf299dbfb36575182cdea2cb034c930", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There might be data architects in some organizations to help with some \nof the architecture requirements, but that might not always be the case. So, a data engineer would ask \nquestions such as the following:\n\u2022\t What are the different sources of data?\n\u2022\t What is the size of the data?\n\u2022\t Where does the data reside today?\n\u2022\t Do we need to migrate the data between different tools?\n\nUnderstanding Apache Spark and Its Applications\n\n\u2022\t How do we connect to the data?\n\u2022\t What kind of transformations are required for the data?\n\u2022\t How often does the data get updated?\n\u2022\t Should we expect a schema change in the new data?\n\u2022\t How do we monitor the pipelines if there are failures?\n\u2022\t Do we need to create a notification system for failures?\n\u2022\t Do we need to add a retry mechanism for failures?\n\u2022\t What is the timeout strategy for failures?\n\u2022\t How do we run back-dated pipelines if there are failures?", "mimetype": "text/plain", "start_char_idx": 36246, "end_char_idx": 37140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173": {"__data__": {"id_": "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fa74f19-76a5-40f7-8410-3a0c5c8cad31", "node_type": "1", "metadata": {}, "hash": "da47be949adeb873231fc1a29c031b07de745ef0be6748a9ff28293be67de75b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df43e7e7-f667-4f2d-8c64-2b4a22816bf7", "node_type": "1", "metadata": {}, "hash": "ea9c65241f77ac1b7b4973dc1be5d91fab49dbf0036af06d0c2bb6a2013e243c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t How often does the data get updated?\n\u2022\t Should we expect a schema change in the new data?\n\u2022\t How do we monitor the pipelines if there are failures?\n\u2022\t Do we need to create a notification system for failures?\n\u2022\t Do we need to add a retry mechanism for failures?\n\u2022\t What is the timeout strategy for failures?\n\u2022\t How do we run back-dated pipelines if there are failures?\n\u2022\t How do we deal with bad data?\n\u2022\t What strategy we should follow \u2013 ETL or ELT?\n\u2022\t How can we save the costs of computation?\nOnce they have answers to these questions, then they start working on a resilient architecture to build \ndata pipelines. Once those pipelines are run and tested, the next step is to maintain these pipelines \nand make the processing more efficient and visible for failure detection.", "mimetype": "text/plain", "start_char_idx": 36770, "end_char_idx": 37548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df43e7e7-f667-4f2d-8c64-2b4a22816bf7": {"__data__": {"id_": "df43e7e7-f667-4f2d-8c64-2b4a22816bf7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173", "node_type": "1", "metadata": {}, "hash": "e843d78f826b3fd984fc30abbc677d1c1bf299dbfb36575182cdea2cb034c930", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7695727-f089-4ed4-9b73-724a5998cd38", "node_type": "1", "metadata": {}, "hash": "278ae7174b4d11c1d3c16b19bb2c53c5d71e8ca2343b424a44c030eddcc9466f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t How do we deal with bad data?\n\u2022\t What strategy we should follow \u2013 ETL or ELT?\n\u2022\t How can we save the costs of computation?\nOnce they have answers to these questions, then they start working on a resilient architecture to build \ndata pipelines. Once those pipelines are run and tested, the next step is to maintain these pipelines \nand make the processing more efficient and visible for failure detection. The goal is to build these \npipelines so that once everything is run, the end state of data is consistent for different downstream \nuse cases. Too often, data engineers have to collaborate with data analysts and data scientists to come \nup with correct data transformation requirements, based on the required use cases.\nLet\u2019s talk about data scientists now, a job that has been advertised as \u201cthe sexiest job of the 21st century\u201d \non multiple forums.", "mimetype": "text/plain", "start_char_idx": 37141, "end_char_idx": 37999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7695727-f089-4ed4-9b73-724a5998cd38": {"__data__": {"id_": "e7695727-f089-4ed4-9b73-724a5998cd38", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df43e7e7-f667-4f2d-8c64-2b4a22816bf7", "node_type": "1", "metadata": {}, "hash": "ea9c65241f77ac1b7b4973dc1be5d91fab49dbf0036af06d0c2bb6a2013e243c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "877fe78c-f363-4c45-8dd7-7b38af52adbc", "node_type": "1", "metadata": {}, "hash": "2b18900696a1621ff75794a7606b1d2a0f73fb8b9e9015c29e2f7e97022416a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Too often, data engineers have to collaborate with data analysts and data scientists to come \nup with correct data transformation requirements, based on the required use cases.\nLet\u2019s talk about data scientists now, a job that has been advertised as \u201cthe sexiest job of the 21st century\u201d \non multiple forums.\nData scientists\nTraditionally, data has been used for decision making based on what has happened in the past. This \nmeans that organizations have been reactive, based on the data. Now, there\u2019s been a paradigm shift \nin advanced and predictive analytics. This means instead of being reactive, organizations can be \nproactive in their decision making. They achieve this with the help of all the data that is available \nto organizations now. To make effective use of this data, data scientists play a major part.", "mimetype": "text/plain", "start_char_idx": 37692, "end_char_idx": 38509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "877fe78c-f363-4c45-8dd7-7b38af52adbc": {"__data__": {"id_": "877fe78c-f363-4c45-8dd7-7b38af52adbc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7695727-f089-4ed4-9b73-724a5998cd38", "node_type": "1", "metadata": {}, "hash": "278ae7174b4d11c1d3c16b19bb2c53c5d71e8ca2343b424a44c030eddcc9466f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd9b58ff-eeda-46df-b657-2564ffa240c7", "node_type": "1", "metadata": {}, "hash": "c00d38f419e236130ea9a3c4378e41735a2a828e0d45e136c1d7c3ed85f8e1fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This \nmeans that organizations have been reactive, based on the data. Now, there\u2019s been a paradigm shift \nin advanced and predictive analytics. This means instead of being reactive, organizations can be \nproactive in their decision making. They achieve this with the help of all the data that is available \nto organizations now. To make effective use of this data, data scientists play a major part. They take \nanalytics to the next level, where instead of just looking at what has happened in the past, they have \nsophisticated machine learning algorithms to predict what could take place in the future as well. All \nthis is based on the huge amounts of data that is available to them.\nA typical job role for a data scientist in an organization may look as follows.\n\nWho are the Spark users?\n\nThe data scientist is given a problem to solve or a question to answer.", "mimetype": "text/plain", "start_char_idx": 38110, "end_char_idx": 38975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd9b58ff-eeda-46df-b657-2564ffa240c7": {"__data__": {"id_": "bd9b58ff-eeda-46df-b657-2564ffa240c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "877fe78c-f363-4c45-8dd7-7b38af52adbc", "node_type": "1", "metadata": {}, "hash": "2b18900696a1621ff75794a7606b1d2a0f73fb8b9e9015c29e2f7e97022416a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b97d233-1317-4e1e-a8c2-1850424a8396", "node_type": "1", "metadata": {}, "hash": "e59c1696bd822698e2f513ddf18fb5662cdaa6abfd493bafaf9258b35927cbe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All \nthis is based on the huge amounts of data that is available to them.\nA typical job role for a data scientist in an organization may look as follows.\n\nWho are the Spark users?\n\nThe data scientist is given a problem to solve or a question to answer. The first task is to see what \nkind of data is available to them that would help them answer this question. They would create a few \nhypotheses to test with the given data. If the results are positive and the data is able to answer some \nof the problem statements, then they move on to experimenting with the data and seek ways to more \neffectively answer the questions at hand. For this purpose, they would join different datasets together, \nand they would also transform the data to make it ready for some machine learning algorithms to \nconsume. At this stage, they would also need to decide what kind of machine learning problem they \naim to solve.", "mimetype": "text/plain", "start_char_idx": 38723, "end_char_idx": 39628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b97d233-1317-4e1e-a8c2-1850424a8396": {"__data__": {"id_": "1b97d233-1317-4e1e-a8c2-1850424a8396", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd9b58ff-eeda-46df-b657-2564ffa240c7", "node_type": "1", "metadata": {}, "hash": "c00d38f419e236130ea9a3c4378e41735a2a828e0d45e136c1d7c3ed85f8e1fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da1c3963-f8b5-4712-8366-c7c4a6f5efeb", "node_type": "1", "metadata": {}, "hash": "cebc10a020ad1bf5d58aaec211e7a09a56af7fa9bb9e89474c68444fb32ac34a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this purpose, they would join different datasets together, \nand they would also transform the data to make it ready for some machine learning algorithms to \nconsume. At this stage, they would also need to decide what kind of machine learning problem they \naim to solve.\nThere are three major types of machine learning techniques that they can use:\n\u2022\t Regression\n\u2022\t Classification\n\u2022\t Clustering\nBased on the technique decided and data transformations, they would then move to prototype \nwith a few machine learning algorithms to create a baseline model. A baseline model is a very basic \nmodel that serves to answer the original question. Based on this baseline model, other models can \nbe created that would be able to answer the question better. In some cases, some predefined rules \ncan also serve as a baseline model. What this means is that the business might already be operating \non some predefined rules that can serve as a baseline to compare the machine learning model.", "mimetype": "text/plain", "start_char_idx": 39355, "end_char_idx": 40337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da1c3963-f8b5-4712-8366-c7c4a6f5efeb": {"__data__": {"id_": "da1c3963-f8b5-4712-8366-c7c4a6f5efeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b97d233-1317-4e1e-a8c2-1850424a8396", "node_type": "1", "metadata": {}, "hash": "e59c1696bd822698e2f513ddf18fb5662cdaa6abfd493bafaf9258b35927cbe7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c61e2a43-19d3-4ce5-b28e-bf5911134da0", "node_type": "1", "metadata": {}, "hash": "995d9aeb94aac629e59c9bc3e5b1362b5377b0bbe1af80ab4390d4296a28523f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A baseline model is a very basic \nmodel that serves to answer the original question. Based on this baseline model, other models can \nbe created that would be able to answer the question better. In some cases, some predefined rules \ncan also serve as a baseline model. What this means is that the business might already be operating \non some predefined rules that can serve as a baseline to compare the machine learning model. Once \nthe initial prototyping is done, then the data scientist moves on to more advanced optimizations in \nterms of models. They can work with different hyperparameters of the model or experiment with \ndifferent data transformations and sample sizes. All of this can be done in Spark or other tools and \nlanguages, depending on their preference. Spark has the edge to run these algorithms in a parallel \nfashion, making the whole process very efficient.", "mimetype": "text/plain", "start_char_idx": 39912, "end_char_idx": 40791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c61e2a43-19d3-4ce5-b28e-bf5911134da0": {"__data__": {"id_": "c61e2a43-19d3-4ce5-b28e-bf5911134da0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da1c3963-f8b5-4712-8366-c7c4a6f5efeb", "node_type": "1", "metadata": {}, "hash": "cebc10a020ad1bf5d58aaec211e7a09a56af7fa9bb9e89474c68444fb32ac34a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7", "node_type": "1", "metadata": {}, "hash": "aad58222f7807840fe177daadb57eacd730ceead6ff764cbde4c0816278c0cb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They can work with different hyperparameters of the model or experiment with \ndifferent data transformations and sample sizes. All of this can be done in Spark or other tools and \nlanguages, depending on their preference. Spark has the edge to run these algorithms in a parallel \nfashion, making the whole process very efficient. Once the data scientist is happy with the model \nresults based on different metrics, they would then move that model to a production environment \nwhere these models can be served to customers solving specific problems. At this point, they would \nhand over these models to machine learning engineers to start incorporating them into the pipelines.\nHere\u2019s a summary of the steps taken in a project, as discussed in the previous paragraph:\n1.\t\nCreate and test a hypothesis.\n2.\t\nTransform the data.\n3.\t\nDecide on a machine learning algorithm.\n4.\t\nPrototype with different machine learning models.\n5.\t\nCreate a baseline model.\n6.", "mimetype": "text/plain", "start_char_idx": 40462, "end_char_idx": 41416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7": {"__data__": {"id_": "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c61e2a43-19d3-4ce5-b28e-bf5911134da0", "node_type": "1", "metadata": {}, "hash": "995d9aeb94aac629e59c9bc3e5b1362b5377b0bbe1af80ab4390d4296a28523f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87", "node_type": "1", "metadata": {}, "hash": "06f01cdcdd12646c841719fa9ad128d48af3f81ff9ecfabaea7eb164b5f34b3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At this point, they would \nhand over these models to machine learning engineers to start incorporating them into the pipelines.\nHere\u2019s a summary of the steps taken in a project, as discussed in the previous paragraph:\n1.\t\nCreate and test a hypothesis.\n2.\t\nTransform the data.\n3.\t\nDecide on a machine learning algorithm.\n4.\t\nPrototype with different machine learning models.\n5.\t\nCreate a baseline model.\n6.\t\nTune the model.\n7.\t\nTune the data.\n8.\t\nTransition models to production.\n\nUnderstanding Apache Spark and Its Applications\n\nLet\u2019s discuss the role of machine learning engineers next.\nMachine learning engineers\nLike data engineers, machine learning engineers also build pipelines, but these pipelines are primarily \nbuilt for machine learning model deployment. Machine learning engineers typically take prototyped \nmodels created by data scientists and build machine learning pipelines around them.", "mimetype": "text/plain", "start_char_idx": 41011, "end_char_idx": 41913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87": {"__data__": {"id_": "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7", "node_type": "1", "metadata": {}, "hash": "aad58222f7807840fe177daadb57eacd730ceead6ff764cbde4c0816278c0cb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e489e613-2c89-4859-a726-340a417e5eda", "node_type": "1", "metadata": {}, "hash": "2a2e4a14b4cca5caf9d07ba325fea453b92eedc7101da4a1a25a5cbc337a000a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.\t\nTune the data.\n8.\t\nTransition models to production.\n\nUnderstanding Apache Spark and Its Applications\n\nLet\u2019s discuss the role of machine learning engineers next.\nMachine learning engineers\nLike data engineers, machine learning engineers also build pipelines, but these pipelines are primarily \nbuilt for machine learning model deployment. Machine learning engineers typically take prototyped \nmodels created by data scientists and build machine learning pipelines around them. We will discuss \nwhat machine learning pipelines are and what some of the questions that need to be answered to \nbuild these pipelines are.\nMachine learning models are built to solve complex problems and provide advanced analytic methods \nto serve a business. After prototyping, these models need to run in the production environments of the \norganizations and be deployed to serve customers. For deployment, there are several considerations \nthat need to be taken into account:\n\u2022\t How much data is there for model training?\n\u2022\t How many customers do we plan to serve concurrently?", "mimetype": "text/plain", "start_char_idx": 41434, "end_char_idx": 42494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e489e613-2c89-4859-a726-340a417e5eda": {"__data__": {"id_": "e489e613-2c89-4859-a726-340a417e5eda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87", "node_type": "1", "metadata": {}, "hash": "06f01cdcdd12646c841719fa9ad128d48af3f81ff9ecfabaea7eb164b5f34b3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4df34045-d2fe-4848-9b30-b1676fb1309b", "node_type": "1", "metadata": {}, "hash": "0ddb196bd7670172361dfd7336488f160d9204fcb0beb0bcf37ba73066d1642c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine learning models are built to solve complex problems and provide advanced analytic methods \nto serve a business. After prototyping, these models need to run in the production environments of the \norganizations and be deployed to serve customers. For deployment, there are several considerations \nthat need to be taken into account:\n\u2022\t How much data is there for model training?\n\u2022\t How many customers do we plan to serve concurrently?\n\u2022\t How often do we need to retrain the models?\n\u2022\t How often do we expect the data to change?\n\u2022\t How do we scale the pipeline up and down based on demand?\n\u2022\t How do we monitor failures in model training?\n\u2022\t Do we need notifications for failures?\n\u2022\t Do we need to add a retry mechanism for failures?\n\u2022\t What is the timeout strategy for failures?\n\u2022\t How do we measure model performance in production?\n\u2022\t How do we tackle data drift?\n\u2022\t How do we tackle model drift?", "mimetype": "text/plain", "start_char_idx": 42054, "end_char_idx": 42957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4df34045-d2fe-4848-9b30-b1676fb1309b": {"__data__": {"id_": "4df34045-d2fe-4848-9b30-b1676fb1309b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e489e613-2c89-4859-a726-340a417e5eda", "node_type": "1", "metadata": {}, "hash": "2a2e4a14b4cca5caf9d07ba325fea453b92eedc7101da4a1a25a5cbc337a000a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1060dad7-138a-45e5-bba5-fe384bc7569b", "node_type": "1", "metadata": {}, "hash": "6e89b62835d2b9eefa085f889183bb8f0eb10a368a495f900f24451b9b17dcd8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t How do we scale the pipeline up and down based on demand?\n\u2022\t How do we monitor failures in model training?\n\u2022\t Do we need notifications for failures?\n\u2022\t Do we need to add a retry mechanism for failures?\n\u2022\t What is the timeout strategy for failures?\n\u2022\t How do we measure model performance in production?\n\u2022\t How do we tackle data drift?\n\u2022\t How do we tackle model drift?\nOnce these questions are answered, the next step is to build a pipeline around these models. The main \npurpose of the pipeline would be such that when new data comes in, the pre-trained models are able \nto answer questions based on new datasets.\nLet\u2019s use an example to better understand these pipelines. We\u2019ll continue with the first example of an \norganization selling computer hardware:\n1.\t\nSuppose the organization wants to build a recommender system on its website that recommends \nto users which products to buy.\n2.", "mimetype": "text/plain", "start_char_idx": 42588, "end_char_idx": 43479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1060dad7-138a-45e5-bba5-fe384bc7569b": {"__data__": {"id_": "1060dad7-138a-45e5-bba5-fe384bc7569b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4df34045-d2fe-4848-9b30-b1676fb1309b", "node_type": "1", "metadata": {}, "hash": "0ddb196bd7670172361dfd7336488f160d9204fcb0beb0bcf37ba73066d1642c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02330974-dbba-4011-aba2-0e6b4f1c0a67", "node_type": "1", "metadata": {}, "hash": "c9ef5157db66f3f4c7517cf6ae2926cc949cc577e2fdd455e0bc52c6ad15042b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main \npurpose of the pipeline would be such that when new data comes in, the pre-trained models are able \nto answer questions based on new datasets.\nLet\u2019s use an example to better understand these pipelines. We\u2019ll continue with the first example of an \norganization selling computer hardware:\n1.\t\nSuppose the organization wants to build a recommender system on its website that recommends \nto users which products to buy.\n2.\t\nThe data scientists have built a prototype model that works well with test data. Now, they want \nto deploy it to production.\n\nSummary\n\n3.\t\nTo deploy this model, the machine learning engineers would have to see how they can incorporate \nthis model on the website.\n4.\t\nThey would start by getting the data ingested from the website to get the user information.\n5.\t\nOnce they have the information, they pass it through the data pipeline to clean and join the data.\n6.", "mimetype": "text/plain", "start_char_idx": 43051, "end_char_idx": 43945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02330974-dbba-4011-aba2-0e6b4f1c0a67": {"__data__": {"id_": "02330974-dbba-4011-aba2-0e6b4f1c0a67", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1060dad7-138a-45e5-bba5-fe384bc7569b", "node_type": "1", "metadata": {}, "hash": "6e89b62835d2b9eefa085f889183bb8f0eb10a368a495f900f24451b9b17dcd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "994ea49c-462d-46f7-abb0-6b29b08749dc", "node_type": "1", "metadata": {}, "hash": "1760ce83010024e40df914db3006ce2f1eeb74e0c02952cf2844e8643ff24a4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, they want \nto deploy it to production.\n\nSummary\n\n3.\t\nTo deploy this model, the machine learning engineers would have to see how they can incorporate \nthis model on the website.\n4.\t\nThey would start by getting the data ingested from the website to get the user information.\n5.\t\nOnce they have the information, they pass it through the data pipeline to clean and join the data.\n6.\t\nThey might also want to add some precomputed features to the model, such as the time of the \nyear, to get a better idea of whether it\u2019s a holiday season and some special deals are going on.\n7.\t\nThen, they would need a REST API endpoint to get the latest recommendations for each user \non the website.\n8.\t\nAfter that, the website needs to be connected to the REST endpoint to serve the actual customers.\n9.", "mimetype": "text/plain", "start_char_idx": 43562, "end_char_idx": 44352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "994ea49c-462d-46f7-abb0-6b29b08749dc": {"__data__": {"id_": "994ea49c-462d-46f7-abb0-6b29b08749dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02330974-dbba-4011-aba2-0e6b4f1c0a67", "node_type": "1", "metadata": {}, "hash": "c9ef5157db66f3f4c7517cf6ae2926cc949cc577e2fdd455e0bc52c6ad15042b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "342c4650-0350-4b78-a2bb-7f325b5aa8fa", "node_type": "1", "metadata": {}, "hash": "bb7053cf4d90d478b41f0654a49b5b7ca063c65f914c7bc83e18144ef301575d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.\t\nThen, they would need a REST API endpoint to get the latest recommendations for each user \non the website.\n8.\t\nAfter that, the website needs to be connected to the REST endpoint to serve the actual customers.\n9.\t\nOnce these models are deployed on live systems (the website, in our example), there needs \nto be a monitoring system for any errors and changes in either the model or the data. This is \nknown as model drift and data drift, respectively.\nData drift\nData may change over time. In our example, people\u2019s preferences may change with time, or with \nseasonality, data may be different. For example, during a holiday season, people\u2019s preferences might \nslightly change because they are looking to get presents for their friends and family, so recommending \nrelevant products based on these preferences is of paramount importance for a business.", "mimetype": "text/plain", "start_char_idx": 44137, "end_char_idx": 44990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "342c4650-0350-4b78-a2bb-7f325b5aa8fa": {"__data__": {"id_": "342c4650-0350-4b78-a2bb-7f325b5aa8fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "994ea49c-462d-46f7-abb0-6b29b08749dc", "node_type": "1", "metadata": {}, "hash": "1760ce83010024e40df914db3006ce2f1eeb74e0c02952cf2844e8643ff24a4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29dfc462-225b-4b63-909f-08375cfdfbe0", "node_type": "1", "metadata": {}, "hash": "9d6a3e863d168ed2844344a76ae194f9f78632ab76faf43b4c16e6c36739f3b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data drift\nData may change over time. In our example, people\u2019s preferences may change with time, or with \nseasonality, data may be different. For example, during a holiday season, people\u2019s preferences might \nslightly change because they are looking to get presents for their friends and family, so recommending \nrelevant products based on these preferences is of paramount importance for a business. Monitoring \nthese trends and changes in the data would result in better models over time and would ultimately \nbenefit the business.\nModel drift\nSimilar to data drift, we also have the concept of model drift. This means that the model changes over \ntime, and the old model that was initially built is not the most performant in terms of recommending \nitems to website visitors. With changing data, the model also needs to be updated from time to time.", "mimetype": "text/plain", "start_char_idx": 44591, "end_char_idx": 45442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29dfc462-225b-4b63-909f-08375cfdfbe0": {"__data__": {"id_": "29dfc462-225b-4b63-909f-08375cfdfbe0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "342c4650-0350-4b78-a2bb-7f325b5aa8fa", "node_type": "1", "metadata": {}, "hash": "bb7053cf4d90d478b41f0654a49b5b7ca063c65f914c7bc83e18144ef301575d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c", "node_type": "1", "metadata": {}, "hash": "a076bcc2c77d44e3e29fcb9d562cc9c5a1e20221ed37f66a662de71aadd9f896", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model drift\nSimilar to data drift, we also have the concept of model drift. This means that the model changes over \ntime, and the old model that was initially built is not the most performant in terms of recommending \nitems to website visitors. With changing data, the model also needs to be updated from time to time. \nTo get a sense of when a model needs to be updated, we need to have monitoring in place for models as \nwell. This monitoring would constantly compare old model results with the new data and see whether \nmodel performance is degrading. If that\u2019s the case, it\u2019s time to update the model.\nThis whole life cycle of model deployment is typically the responsibility of machine learning engineers. \nNote that the process would slightly vary for different problems, but the overall idea remains the same.", "mimetype": "text/plain", "start_char_idx": 45124, "end_char_idx": 45940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c": {"__data__": {"id_": "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29dfc462-225b-4b63-909f-08375cfdfbe0", "node_type": "1", "metadata": {}, "hash": "9d6a3e863d168ed2844344a76ae194f9f78632ab76faf43b4c16e6c36739f3b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7e6b31b-969e-475c-9ac4-e3369d1e290e", "node_type": "1", "metadata": {}, "hash": "28b0c0e570ea1d096f1584e39d7954a754bd0031df3f3911f0d42d91ea182fe2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This monitoring would constantly compare old model results with the new data and see whether \nmodel performance is degrading. If that\u2019s the case, it\u2019s time to update the model.\nThis whole life cycle of model deployment is typically the responsibility of machine learning engineers. \nNote that the process would slightly vary for different problems, but the overall idea remains the same.\nSummary\nIn this chapter, we learned about the basics of Apache Spark and why Spark is becoming a lot more \nprevalent in the industry for big data applications. We also learned about the different components of \nSpark and how these components are helpful in terms of application development. Then, we discussed \nthe different roles that are present in the industry today and who can make use of Spark\u2019s capabilities. \nFinally, we discussed the modern-day uses of Spark in different industry use cases.\n\nUnderstanding Apache Spark and Its Applications\n\nSpark Architecture and \nTransformations\nSpark approaches data processing differently than traditional tools and technologies.", "mimetype": "text/plain", "start_char_idx": 45553, "end_char_idx": 46617, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7e6b31b-969e-475c-9ac4-e3369d1e290e": {"__data__": {"id_": "a7e6b31b-969e-475c-9ac4-e3369d1e290e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c", "node_type": "1", "metadata": {}, "hash": "a076bcc2c77d44e3e29fcb9d562cc9c5a1e20221ed37f66a662de71aadd9f896", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d4439fb-acc2-4330-83e6-29c175f72014", "node_type": "1", "metadata": {}, "hash": "c9f3b52c7fe687850a97f863fef7090b255c5d0060a1ecc9dad04ccec563cee0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we discussed \nthe different roles that are present in the industry today and who can make use of Spark\u2019s capabilities. \nFinally, we discussed the modern-day uses of Spark in different industry use cases.\n\nUnderstanding Apache Spark and Its Applications\n\nSpark Architecture and \nTransformations\nSpark approaches data processing differently than traditional tools and technologies. To understand \nSpark\u2019s unique approach, we will have to understand its basic architecture. A deep dive into Spark\u2019s \narchitecture and its components will give you an idea of how Spark achieves its ground-breaking \nprocessing speeds for big data analytics.", "mimetype": "text/plain", "start_char_idx": 46232, "end_char_idx": 46873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d4439fb-acc2-4330-83e6-29c175f72014": {"__data__": {"id_": "4d4439fb-acc2-4330-83e6-29c175f72014", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7e6b31b-969e-475c-9ac4-e3369d1e290e", "node_type": "1", "metadata": {}, "hash": "28b0c0e570ea1d096f1584e39d7954a754bd0031df3f3911f0d42d91ea182fe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7405b6be-90e1-4a29-8eda-84001617cc06", "node_type": "1", "metadata": {}, "hash": "c93ac39a06c440f4af8cafa2e75549985c598d99ba5c048f33ee13b67b20d6a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding Apache Spark and Its Applications\n\nSpark Architecture and \nTransformations\nSpark approaches data processing differently than traditional tools and technologies. To understand \nSpark\u2019s unique approach, we will have to understand its basic architecture. A deep dive into Spark\u2019s \narchitecture and its components will give you an idea of how Spark achieves its ground-breaking \nprocessing speeds for big data analytics.\nIn this chapter, you will learn about the following broader topics:\n\u2022\t Spark architecture and execution hierarchy\n\u2022\t Different Spark components\n\u2022\t The roles of the Spark driver and Spark executor\n\u2022\t Different deployment modes in Spark\n\u2022\t Transformations and actions as Spark operations\nBy the end of this chapter, you will have valuable insights into Spark\u2019s inner workings and know how \nto apply this knowledge effectively for your certification test.\nSpark architecture\nIn the previous chapters, we discussed that Apache Spark is an open source, distributed computing \nframework designed for big data processing and analytics.", "mimetype": "text/plain", "start_char_idx": 46443, "end_char_idx": 47502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7405b6be-90e1-4a29-8eda-84001617cc06": {"__data__": {"id_": "7405b6be-90e1-4a29-8eda-84001617cc06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d4439fb-acc2-4330-83e6-29c175f72014", "node_type": "1", "metadata": {}, "hash": "c9f3b52c7fe687850a97f863fef7090b255c5d0060a1ecc9dad04ccec563cee0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64de67c2-506d-48dd-afaa-326decdcc837", "node_type": "1", "metadata": {}, "hash": "8c9538c4556356dd7d343e5a8383c5dd6078f955d471e170076e0d069cd53e08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark architecture\nIn the previous chapters, we discussed that Apache Spark is an open source, distributed computing \nframework designed for big data processing and analytics. Its architecture is built to handle various \nworkloads efficiently, offering speed, scalability, and fault tolerance. Understanding the architecture \nof Spark is crucial for comprehending its capabilities in processing large volumes of data.\n\nSpark Architecture and Transformations\n\nThe components of Spark architecture work in collaboration to process data efficiently. The following \nmajor components are involved:\n\u2022\t Spark driver\n\u2022\t SparkContext\n\u2022\t Cluster manager\n\u2022\t Worker node\n\u2022\t Spark executor\n\u2022\t Task\nBefore we talk about any of these components, it\u2019s important to understand their execution hierarchy \nto know how each component interacts when a Spark program starts.", "mimetype": "text/plain", "start_char_idx": 47327, "end_char_idx": 48179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64de67c2-506d-48dd-afaa-326decdcc837": {"__data__": {"id_": "64de67c2-506d-48dd-afaa-326decdcc837", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7405b6be-90e1-4a29-8eda-84001617cc06", "node_type": "1", "metadata": {}, "hash": "c93ac39a06c440f4af8cafa2e75549985c598d99ba5c048f33ee13b67b20d6a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb6eb391-0300-40f8-a61e-b107bbbc0c2f", "node_type": "1", "metadata": {}, "hash": "ee8026bf655b8d75259b99c63f618ad748307b2239ad6a012b855f43bc0e607f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Architecture and Transformations\n\nThe components of Spark architecture work in collaboration to process data efficiently. The following \nmajor components are involved:\n\u2022\t Spark driver\n\u2022\t SparkContext\n\u2022\t Cluster manager\n\u2022\t Worker node\n\u2022\t Spark executor\n\u2022\t Task\nBefore we talk about any of these components, it\u2019s important to understand their execution hierarchy \nto know how each component interacts when a Spark program starts.\nExecution hierarchy\nLet\u2019s look at the execution flow of a Spark application with the help of the architecture depicted in \nFigure 3.1:\nFigure 3.1: Spark architecture\n\nSpark components\n\nThese steps outline the flow from submitting a Spark job to freeing up resources when the job is completed:\n1.\t\nSpark executions start with a user submitting a spark-submit request to the Spark engine. This \nwill create a Spark application. Once an action is performed, it will result in a job being created.\n2.", "mimetype": "text/plain", "start_char_idx": 47746, "end_char_idx": 48676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb6eb391-0300-40f8-a61e-b107bbbc0c2f": {"__data__": {"id_": "eb6eb391-0300-40f8-a61e-b107bbbc0c2f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64de67c2-506d-48dd-afaa-326decdcc837", "node_type": "1", "metadata": {}, "hash": "8c9538c4556356dd7d343e5a8383c5dd6078f955d471e170076e0d069cd53e08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32b284e1-bb6f-4eaa-933a-ebe480aa6dee", "node_type": "1", "metadata": {}, "hash": "bed1d20b63dd2e1faab5218ec415e3192691bce005886f8b6638aa442e52d83f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark executions start with a user submitting a spark-submit request to the Spark engine. This \nwill create a Spark application. Once an action is performed, it will result in a job being created.\n2.\t\nThis request will initiate communication with the cluster manager. In turn, the cluster manager \ninitializes the Spark driver to execute the main() method of the Spark application. To execute \nthis method, SparkSession is created.\n3.\t\nThe driver starts communicating with the cluster manager and asks for resources to start \nplanning for execution.\n4.\t\nThe cluster manager then starts the executors, which can communicate with the driver directly.\n5.\t\nThe driver creates a logical plan, known as a directed acyclic graph (DAG), and physical plan \nfor execution based on the total number of tasks required to be executed.\n6.\t\nThe driver also divides data to be run on each executor, along with tasks.\n7.", "mimetype": "text/plain", "start_char_idx": 48477, "end_char_idx": 49380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32b284e1-bb6f-4eaa-933a-ebe480aa6dee": {"__data__": {"id_": "32b284e1-bb6f-4eaa-933a-ebe480aa6dee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb6eb391-0300-40f8-a61e-b107bbbc0c2f", "node_type": "1", "metadata": {}, "hash": "ee8026bf655b8d75259b99c63f618ad748307b2239ad6a012b855f43bc0e607f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5be36c37-bd33-4489-9e82-30ec2f71e715", "node_type": "1", "metadata": {}, "hash": "4b8ab0cf9f31fdfb0bf453c187c8fadf9cce30bdf072b9875ea3add52ce4c798", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.\t\nThe cluster manager then starts the executors, which can communicate with the driver directly.\n5.\t\nThe driver creates a logical plan, known as a directed acyclic graph (DAG), and physical plan \nfor execution based on the total number of tasks required to be executed.\n6.\t\nThe driver also divides data to be run on each executor, along with tasks.\n7.\t\nOnce each task finishes running, the driver gets the results.\n8.\t\nWhen the program finishes running, the main() method exits and Spark frees all executors \nand driver resources.\nNow that you understand the execution hierarchy, let\u2019s discuss each of Spark\u2019s components in detail.\nSpark components\nLet\u2019s dive into the inner workings of each Spark component to understand how each of them plays a \ncrucial role in empowering efficient distributed data processing.\nSpark driver\nThe Spark driver is the core of the intelligent and efficient computations in Spark.", "mimetype": "text/plain", "start_char_idx": 49027, "end_char_idx": 49940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5be36c37-bd33-4489-9e82-30ec2f71e715": {"__data__": {"id_": "5be36c37-bd33-4489-9e82-30ec2f71e715", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b284e1-bb6f-4eaa-933a-ebe480aa6dee", "node_type": "1", "metadata": {}, "hash": "bed1d20b63dd2e1faab5218ec415e3192691bce005886f8b6638aa442e52d83f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb", "node_type": "1", "metadata": {}, "hash": "e3211bd82aa68a1fce12c2b55aafb16b0c1c2dbbb2387b5aca3913e4b5b6792b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now that you understand the execution hierarchy, let\u2019s discuss each of Spark\u2019s components in detail.\nSpark components\nLet\u2019s dive into the inner workings of each Spark component to understand how each of them plays a \ncrucial role in empowering efficient distributed data processing.\nSpark driver\nThe Spark driver is the core of the intelligent and efficient computations in Spark. Spark follows an \narchitecture that is commonly known as the master-worker architecture in network topology. Consider \nthe Spark driver as a master and Spark executors as slaves. The driver has control and knowledge of \nall the executors at any given time. It is the responsibility of the driver to know how many executors \nare present and if any executor has failed so that it can fall back on its alternative. The Spark driver \nalso maintains communication with executors all the time. The driver runs on the master node of a \nmachine or cluster.", "mimetype": "text/plain", "start_char_idx": 49560, "end_char_idx": 50489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb": {"__data__": {"id_": "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5be36c37-bd33-4489-9e82-30ec2f71e715", "node_type": "1", "metadata": {}, "hash": "4b8ab0cf9f31fdfb0bf453c187c8fadf9cce30bdf072b9875ea3add52ce4c798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5bd8d71-1d42-48a6-b126-30be2ae64025", "node_type": "1", "metadata": {}, "hash": "1deed967ff0520ec26cde0749d11e426b6078f1f1cb705c5caae487aae5e12d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The driver has control and knowledge of \nall the executors at any given time. It is the responsibility of the driver to know how many executors \nare present and if any executor has failed so that it can fall back on its alternative. The Spark driver \nalso maintains communication with executors all the time. The driver runs on the master node of a \nmachine or cluster. When a Spark application starts running, the driver keeps up with all the required \ninformation that is needed to run the application successfully.\nAs shown in Figure 3.1, the driver node contains SparkSession, which is the entry point of the \nSpark application. Previously, this was known as the SparkContext object, but in Spark 2.0, \nSparkSession handles all contexts to start execution. The application\u2019s main method runs on the \ndriver to coordinate the whole application. It runs on its own Java Virtual Machine (JVM).", "mimetype": "text/plain", "start_char_idx": 50120, "end_char_idx": 51014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5bd8d71-1d42-48a6-b126-30be2ae64025": {"__data__": {"id_": "a5bd8d71-1d42-48a6-b126-30be2ae64025", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb", "node_type": "1", "metadata": {}, "hash": "e3211bd82aa68a1fce12c2b55aafb16b0c1c2dbbb2387b5aca3913e4b5b6792b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5db9014d-9b3a-404d-8196-e20299ef608c", "node_type": "1", "metadata": {}, "hash": "d04f81005c16594dc5f83df6c120fef7553e65bed6b996dbdc857a9f13d5d790", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figure 3.1, the driver node contains SparkSession, which is the entry point of the \nSpark application. Previously, this was known as the SparkContext object, but in Spark 2.0, \nSparkSession handles all contexts to start execution. The application\u2019s main method runs on the \ndriver to coordinate the whole application. It runs on its own Java Virtual Machine (JVM). Spark \ndriver can run as an independent process or it can run on one of the worker nodes, depending on \nthe architecture.\n\nSpark Architecture and Transformations\n\nThe Spark driver is responsible for dividing the application into smaller entities for execution. These \nentities are known as tasks. You will learn more about tasks in the upcoming sections of this chapter. \nThe Spark driver also decides what data the executor will work on and what tasks are run on which \nexecutor.", "mimetype": "text/plain", "start_char_idx": 50638, "end_char_idx": 51495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5db9014d-9b3a-404d-8196-e20299ef608c": {"__data__": {"id_": "5db9014d-9b3a-404d-8196-e20299ef608c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5bd8d71-1d42-48a6-b126-30be2ae64025", "node_type": "1", "metadata": {}, "hash": "1deed967ff0520ec26cde0749d11e426b6078f1f1cb705c5caae487aae5e12d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa", "node_type": "1", "metadata": {}, "hash": "bd9b72f27517c2df5604a97d1010bae92b6f0efcb0b379217a536e8ec7c52820", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Architecture and Transformations\n\nThe Spark driver is responsible for dividing the application into smaller entities for execution. These \nentities are known as tasks. You will learn more about tasks in the upcoming sections of this chapter. \nThe Spark driver also decides what data the executor will work on and what tasks are run on which \nexecutor. These tasks are scheduled to run on the executor nodes with the help of the cluster manager. \nThis information that is driven by the driver enables fault tolerance. Since the driver has all the \ninformation about the number of available workers and the tasks that are running on each of them \nalongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \ntaking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \nexecutor returns the task earlier would prevail.", "mimetype": "text/plain", "start_char_idx": 51138, "end_char_idx": 52054, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa": {"__data__": {"id_": "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5db9014d-9b3a-404d-8196-e20299ef608c", "node_type": "1", "metadata": {}, "hash": "d04f81005c16594dc5f83df6c120fef7553e65bed6b996dbdc857a9f13d5d790", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82281413-8084-44d1-8d81-fc1b08032af0", "node_type": "1", "metadata": {}, "hash": "916c2a1950339c90498d1f6a78f88cfc8a81cf0ab54fdba6726bf76de388661e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the driver has all the \ninformation about the number of available workers and the tasks that are running on each of them \nalongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is \ntaking too long to run, it can be assigned to another executor if that gets free. In that case, whichever \nexecutor returns the task earlier would prevail. The Spark driver also maintains metadata about the \nResilient Distributed Dataset (RDD) and its partitions.\nIt is the responsibility of the Spark driver to design the complete execution map. It determines which \ntasks run on which executors, as well as how the data is distributed across these executors. This is \ndone by creating RDDs internally. Based on this distribution of data, the operations that are required \nare determined, such as transformations and actions that are defined in the program. A DAG is \ncreated based on these decisions.", "mimetype": "text/plain", "start_char_idx": 51661, "end_char_idx": 52601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82281413-8084-44d1-8d81-fc1b08032af0": {"__data__": {"id_": "82281413-8084-44d1-8d81-fc1b08032af0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa", "node_type": "1", "metadata": {}, "hash": "bd9b72f27517c2df5604a97d1010bae92b6f0efcb0b379217a536e8ec7c52820", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82c0aeef-ca56-4431-81df-3627966fc03e", "node_type": "1", "metadata": {}, "hash": "b3beebaf1d13c777fdc08217e693df63a08327a7ab7f2eb0bf52c4981549ffbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It determines which \ntasks run on which executors, as well as how the data is distributed across these executors. This is \ndone by creating RDDs internally. Based on this distribution of data, the operations that are required \nare determined, such as transformations and actions that are defined in the program. A DAG is \ncreated based on these decisions. The Spark driver optimizes the logical plan (DAG) and finds the \nbest possible execution strategy for the DAG, in addition to determining the most optimal location for \nthe execution of a particular task. These executions are done in parallel. The executors simply follow \nthese commands without doing any optimization on their end.\nFor performance considerations, it is optimal to have the Spark driver work close to the executor. This \nreduces the latency by a great deal. This means that there would be less delay in the response time of \nthe processes.", "mimetype": "text/plain", "start_char_idx": 52246, "end_char_idx": 53158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82c0aeef-ca56-4431-81df-3627966fc03e": {"__data__": {"id_": "82c0aeef-ca56-4431-81df-3627966fc03e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82281413-8084-44d1-8d81-fc1b08032af0", "node_type": "1", "metadata": {}, "hash": "916c2a1950339c90498d1f6a78f88cfc8a81cf0ab54fdba6726bf76de388661e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38507cad-f8a5-4c49-8a94-afd7c4ab52e1", "node_type": "1", "metadata": {}, "hash": "e084338573f45bca10bb97df750b546b904a328285584f7f25256d468781ca47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These executions are done in parallel. The executors simply follow \nthese commands without doing any optimization on their end.\nFor performance considerations, it is optimal to have the Spark driver work close to the executor. This \nreduces the latency by a great deal. This means that there would be less delay in the response time of \nthe processes. Another point to note here is that this is true for the data as well. The executor reading \nthe data close to it would have better performance than otherwise. Ideally, the driver and worker nodes \nshould be run in the same local area network (LAN) for the best performance.\nThe Spark driver also creates a web UI for the execution details. This UI is very helpful in determining \nthe performance of the application. In cases where troubleshooting is required and some bottlenecks \nneed to be identified in the Spark process, this UI is very helpful.", "mimetype": "text/plain", "start_char_idx": 52807, "end_char_idx": 53708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38507cad-f8a5-4c49-8a94-afd7c4ab52e1": {"__data__": {"id_": "38507cad-f8a5-4c49-8a94-afd7c4ab52e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82c0aeef-ca56-4431-81df-3627966fc03e", "node_type": "1", "metadata": {}, "hash": "b3beebaf1d13c777fdc08217e693df63a08327a7ab7f2eb0bf52c4981549ffbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7931d982-c024-41bc-b292-15c647a18a57", "node_type": "1", "metadata": {}, "hash": "0aeb91cdb8a2ec736e791dba0b715582330cddb24b1b7a7fcebd421f0f657412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ideally, the driver and worker nodes \nshould be run in the same local area network (LAN) for the best performance.\nThe Spark driver also creates a web UI for the execution details. This UI is very helpful in determining \nthe performance of the application. In cases where troubleshooting is required and some bottlenecks \nneed to be identified in the Spark process, this UI is very helpful.\nSparkSession\nSparkSession is the main point of entry and interaction with Spark. As discussed earlier, in the \nprevious versions of Spark, SparkContext used to play this role, but in Spark 2.0, SparkSession \ncan be created for this purpose. The Spark driver creates a SparkSession object to interact with \nthe cluster manager and get resource allocation through it.\nIn the lifetime of the application, SparkSession is also used to interact with all the underlying Spark \nAPIs.", "mimetype": "text/plain", "start_char_idx": 53318, "end_char_idx": 54185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7931d982-c024-41bc-b292-15c647a18a57": {"__data__": {"id_": "7931d982-c024-41bc-b292-15c647a18a57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38507cad-f8a5-4c49-8a94-afd7c4ab52e1", "node_type": "1", "metadata": {}, "hash": "e084338573f45bca10bb97df750b546b904a328285584f7f25256d468781ca47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcf22dec-41ed-4d55-9432-d48f0b4acbae", "node_type": "1", "metadata": {}, "hash": "7994ad8c82e0ef898450e598faf7cc4037c3f4fec28dec61f4d2b0574d6e56e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As discussed earlier, in the \nprevious versions of Spark, SparkContext used to play this role, but in Spark 2.0, SparkSession \ncan be created for this purpose. The Spark driver creates a SparkSession object to interact with \nthe cluster manager and get resource allocation through it.\nIn the lifetime of the application, SparkSession is also used to interact with all the underlying Spark \nAPIs. We talked about different Spark APIs in Chapter 2 namely, SparkSQL, Spark Streaming, MLlib, \nand GraphX. All of these APIs use SparkSession from its core to interact with the Spark application.\nSparkSession keeps track of Spark executors throughout the application\u2019s execution.\n\nSpark components\n\nCluster manager\nSpark is a distributed framework, which requires it to have access to computing resources. This access is \ngoverned and controlled by a process known as the cluster manager.", "mimetype": "text/plain", "start_char_idx": 53790, "end_char_idx": 54672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcf22dec-41ed-4d55-9432-d48f0b4acbae": {"__data__": {"id_": "fcf22dec-41ed-4d55-9432-d48f0b4acbae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7931d982-c024-41bc-b292-15c647a18a57", "node_type": "1", "metadata": {}, "hash": "0aeb91cdb8a2ec736e791dba0b715582330cddb24b1b7a7fcebd421f0f657412", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afea7bf3-6755-4d28-aa13-10523385b153", "node_type": "1", "metadata": {}, "hash": "63bb62414cfc8229c5a19362ba9ca9882c0c9e310d3f0aad2f44015d264e5465", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of these APIs use SparkSession from its core to interact with the Spark application.\nSparkSession keeps track of Spark executors throughout the application\u2019s execution.\n\nSpark components\n\nCluster manager\nSpark is a distributed framework, which requires it to have access to computing resources. This access is \ngoverned and controlled by a process known as the cluster manager. It is the responsibility of the cluster \nmanager to allocate computing resources for the Spark application when the application execution \nstarts. These resources become available at the request of the application master. In the Apache Spark \necosystem, the application master plays a crucial role in managing and coordinating the execution \nof Spark applications within a distributed cluster environment. It\u2019s an essential component that\u2019s \nresponsible for negotiating resources, scheduling tasks, and monitoring the application\u2019s execution.\nOnce the resources are available, the driver is made aware of those resources.", "mimetype": "text/plain", "start_char_idx": 54291, "end_char_idx": 55294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afea7bf3-6755-4d28-aa13-10523385b153": {"__data__": {"id_": "afea7bf3-6755-4d28-aa13-10523385b153", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcf22dec-41ed-4d55-9432-d48f0b4acbae", "node_type": "1", "metadata": {}, "hash": "7994ad8c82e0ef898450e598faf7cc4037c3f4fec28dec61f4d2b0574d6e56e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6d10179-db9e-492b-a0c5-6966a7837c79", "node_type": "1", "metadata": {}, "hash": "623de37adf0454d3277819fbe38a10f6af0a7f2be6a02cb399d3379e8ff8f15e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These resources become available at the request of the application master. In the Apache Spark \necosystem, the application master plays a crucial role in managing and coordinating the execution \nof Spark applications within a distributed cluster environment. It\u2019s an essential component that\u2019s \nresponsible for negotiating resources, scheduling tasks, and monitoring the application\u2019s execution.\nOnce the resources are available, the driver is made aware of those resources. It\u2019s the responsibility of \nthe driver to manage these resources based on tasks that need to be executed by the Spark application. \nOnce the application has finished execution, these resources are released back to the cluster manager.\nApplications have their dedicated executor processes that parallelize how tasks are run. The advantage \nis that each application is independent of the other and runs on its own schedule. Data also becomes \nindependent for each of these applications, so data sharing can only take place by writing data to disk \nso that it can be shared across applications.", "mimetype": "text/plain", "start_char_idx": 54820, "end_char_idx": 55886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6d10179-db9e-492b-a0c5-6966a7837c79": {"__data__": {"id_": "b6d10179-db9e-492b-a0c5-6966a7837c79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afea7bf3-6755-4d28-aa13-10523385b153", "node_type": "1", "metadata": {}, "hash": "63bb62414cfc8229c5a19362ba9ca9882c0c9e310d3f0aad2f44015d264e5465", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a4d8153-2591-4e70-8dff-a92c0dc0496d", "node_type": "1", "metadata": {}, "hash": "e704461c00fc15a484ca3ae26417044b6c85ff46ba19e204f520cbd4f1be9fde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the application has finished execution, these resources are released back to the cluster manager.\nApplications have their dedicated executor processes that parallelize how tasks are run. The advantage \nis that each application is independent of the other and runs on its own schedule. Data also becomes \nindependent for each of these applications, so data sharing can only take place by writing data to disk \nso that it can be shared across applications.\nCluster modes\nCluster modes define how Spark applications utilize cluster resources, manage task execution, and \ninteract with cluster managers for resource allocation.\nIf there is more than one user sharing resources on the cluster, be it Spark applications or other \napplications that need cluster resources, they have to be managed based on different modes. There \nare two types of modes available for cluster managers \u2013 standalone client mode and cluster mode.", "mimetype": "text/plain", "start_char_idx": 55427, "end_char_idx": 56351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a4d8153-2591-4e70-8dff-a92c0dc0496d": {"__data__": {"id_": "0a4d8153-2591-4e70-8dff-a92c0dc0496d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6d10179-db9e-492b-a0c5-6966a7837c79", "node_type": "1", "metadata": {}, "hash": "623de37adf0454d3277819fbe38a10f6af0a7f2be6a02cb399d3379e8ff8f15e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d75e662-c547-4ba3-8f2e-4d05ad991f10", "node_type": "1", "metadata": {}, "hash": "5378d7c1ec485685eed94d092f6f306adadcd63567757f463e8684578e9c5974", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cluster modes\nCluster modes define how Spark applications utilize cluster resources, manage task execution, and \ninteract with cluster managers for resource allocation.\nIf there is more than one user sharing resources on the cluster, be it Spark applications or other \napplications that need cluster resources, they have to be managed based on different modes. There \nare two types of modes available for cluster managers \u2013 standalone client mode and cluster mode. \nThe following table highlights some of the differences between the two:\nClient Mode\nCluster Mode\nIn client mode, the driver program runs on \nthe machine where the Spark application \nis submitted.\nIn cluster mode, the driver program runs within \nthe cluster, on one of the worker nodes.\nThe driver program is responsible \nfor orchestrating the execution of the \nSpark application, including creating \nSparkContext and coordinating tasks.\nThe cluster manager is responsible for launching \nthe driver program and allocating resources \nfor execution.", "mimetype": "text/plain", "start_char_idx": 55887, "end_char_idx": 56899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d75e662-c547-4ba3-8f2e-4d05ad991f10": {"__data__": {"id_": "5d75e662-c547-4ba3-8f2e-4d05ad991f10", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a4d8153-2591-4e70-8dff-a92c0dc0496d", "node_type": "1", "metadata": {}, "hash": "e704461c00fc15a484ca3ae26417044b6c85ff46ba19e204f520cbd4f1be9fde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90baf5a8-651b-4fb1-8eef-d90220395627", "node_type": "1", "metadata": {}, "hash": "2b99bf457f4b2777dc8a8cf1ec8927fdc1ec199b9418d3ca652ef42da08f32f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In cluster mode, the driver program runs within \nthe cluster, on one of the worker nodes.\nThe driver program is responsible \nfor orchestrating the execution of the \nSpark application, including creating \nSparkContext and coordinating tasks.\nThe cluster manager is responsible for launching \nthe driver program and allocating resources \nfor execution.\nThe client machine interacts directly with the \ncluster manager to request resources and launch \nexecutors on worker nodes.\nOnce the driver program is launched, it \ncoordinates with the cluster manager to request \nresources and distribute tasks to worker nodes.\nIt may not be suitable for production \ndeployments with large-scale applications.\nIt is commonly used for production deployments \nas it allows for better resource utilization and \nscalability. It also ensures fault tolerance.", "mimetype": "text/plain", "start_char_idx": 56549, "end_char_idx": 57387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90baf5a8-651b-4fb1-8eef-d90220395627": {"__data__": {"id_": "90baf5a8-651b-4fb1-8eef-d90220395627", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d75e662-c547-4ba3-8f2e-4d05ad991f10", "node_type": "1", "metadata": {}, "hash": "5378d7c1ec485685eed94d092f6f306adadcd63567757f463e8684578e9c5974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5084e129-7b29-4991-b338-355406171b1a", "node_type": "1", "metadata": {}, "hash": "143f9cc037b3328dc67a5206258d687931c198b82322d90e98145626d7f46396", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The client machine interacts directly with the \ncluster manager to request resources and launch \nexecutors on worker nodes.\nOnce the driver program is launched, it \ncoordinates with the cluster manager to request \nresources and distribute tasks to worker nodes.\nIt may not be suitable for production \ndeployments with large-scale applications.\nIt is commonly used for production deployments \nas it allows for better resource utilization and \nscalability. It also ensures fault tolerance.\nTable 3.1: Client mode versus cluster mode\n\nSpark Architecture and Transformations\n\nNow, we will talk about different deployment modes and their corresponding managers in Spark:\n\u2022\t Built-in standalone mode (Spark\u2019s native manager): A simple cluster manager bundled with \nSpark that\u2019s suitable for small to medium-scale deployments without external dependencies.\n\u2022\t Apache YARN (Hadoop\u2019s resource manager): Integrated with Spark, YARN enables Spark \napplications to share Hadoop\u2019s cluster resources efficiently.", "mimetype": "text/plain", "start_char_idx": 56900, "end_char_idx": 57898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5084e129-7b29-4991-b338-355406171b1a": {"__data__": {"id_": "5084e129-7b29-4991-b338-355406171b1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90baf5a8-651b-4fb1-8eef-d90220395627", "node_type": "1", "metadata": {}, "hash": "2b99bf457f4b2777dc8a8cf1ec8927fdc1ec199b9418d3ca652ef42da08f32f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db0373e-93b6-415a-94cf-0493a82f1ba0", "node_type": "1", "metadata": {}, "hash": "036e225e863f62201e06bfedb05409aede9148e67fddb3d2fa733a1fb4b33653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Apache YARN (Hadoop\u2019s resource manager): Integrated with Spark, YARN enables Spark \napplications to share Hadoop\u2019s cluster resources efficiently.\n\u2022\t Apache Mesos (resource sharing platform): Mesos offers efficient resource sharing across \nmultiple applications, allowing Spark to run alongside other frameworks.\nWe will talk more about deployment modes later in this chapter.\nSpark executors\nSpark executors are the processes that run on the worker node and execute tasks sent by the driver. \nThe data is stored in memory primarily but can also be written to disk storage closest to them. Driver \nlaunches the executors based on the DAG that Spark generates for its execution. Once the tasks have \nfinished executing, executors send the results back to the driver.\nSince the driver is the main controller of the Spark application, if an executor fails or takes too long to \nexecute a task, the driver can choose to send that task over to other available executors.", "mimetype": "text/plain", "start_char_idx": 57750, "end_char_idx": 58717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4db0373e-93b6-415a-94cf-0493a82f1ba0": {"__data__": {"id_": "4db0373e-93b6-415a-94cf-0493a82f1ba0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5084e129-7b29-4991-b338-355406171b1a", "node_type": "1", "metadata": {}, "hash": "143f9cc037b3328dc67a5206258d687931c198b82322d90e98145626d7f46396", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd0039d0-ab32-45f3-aa93-10834e76228e", "node_type": "1", "metadata": {}, "hash": "4a5be4a3bd1fc26fa024da6afb18a5cb88479b974bfd20ffc1afc9761dcc6a72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Driver \nlaunches the executors based on the DAG that Spark generates for its execution. Once the tasks have \nfinished executing, executors send the results back to the driver.\nSince the driver is the main controller of the Spark application, if an executor fails or takes too long to \nexecute a task, the driver can choose to send that task over to other available executors. This ensures \nreliability and fault tolerance in Spark. We will read more about this later in this chapter.\nIt is the responsibility of the executor to read data from external sources that are needed to run the \ntasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by \nthe executor.", "mimetype": "text/plain", "start_char_idx": 58342, "end_char_idx": 59050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd0039d0-ab32-45f3-aa93-10834e76228e": {"__data__": {"id_": "cd0039d0-ab32-45f3-aa93-10834e76228e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db0373e-93b6-415a-94cf-0493a82f1ba0", "node_type": "1", "metadata": {}, "hash": "036e225e863f62201e06bfedb05409aede9148e67fddb3d2fa733a1fb4b33653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293", "node_type": "1", "metadata": {}, "hash": "1d4158759afa2e6fdcf645a795b71ad437c4534256f1d8cf2985797f1bf83adf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This ensures \nreliability and fault tolerance in Spark. We will read more about this later in this chapter.\nIt is the responsibility of the executor to read data from external sources that are needed to run the \ntasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by \nthe executor.\nThe key functions of an executor are as follows:\n\u2022\t Task execution: Executors run tasks assigned by the Spark application, processing data stored \nin RDDs or DataFrames\n\u2022\t Resource allocation: Each Spark application has a set of executors allocated by the cluster \nmanager for managing resources such as CPU cores and memory\nIn Apache Spark, the concepts of job, stage, and task form the fundamental building blocks of its \ndistributed computing framework. Understanding these components is essential to grasp the core \nworkings of Spark\u2019s parallel processing and task execution.", "mimetype": "text/plain", "start_char_idx": 58718, "end_char_idx": 59630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293": {"__data__": {"id_": "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd0039d0-ab32-45f3-aa93-10834e76228e", "node_type": "1", "metadata": {}, "hash": "4a5be4a3bd1fc26fa024da6afb18a5cb88479b974bfd20ffc1afc9761dcc6a72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e6c009-d0f2-4faa-ac24-fd1786c0443e", "node_type": "1", "metadata": {}, "hash": "3d61acc5aee4cfdf7e1af0c06cd25126768ada9645551e8e7efeee2f9b036fd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding these components is essential to grasp the core \nworkings of Spark\u2019s parallel processing and task execution. See Figure 3.2 to understand the relationship \nbetween these concepts while we discuss them in detail:\nFigure 3.2: Interaction between jobs, stages, and tasks\n\nPartitioning in Spark\n\nLet\u2019s take a closer look:\n\u2022\t Job: The Spark application will initiate multiple jobs when the application starts running. These \njobs can be executed in parallel, wherein each job can consist of multiple tasks. A job gets \ninitiated when a Spark action is called (such as collect). We will learn more about actions \nlater. When an action (such as collect or count) is invoked on a dataset, it triggers the \nexecution of one or more jobs.\nA job consists of several stages, each containing tasks that execute a set of transformations on \ndata partitions.", "mimetype": "text/plain", "start_char_idx": 59508, "end_char_idx": 60365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82e6c009-d0f2-4faa-ac24-fd1786c0443e": {"__data__": {"id_": "82e6c009-d0f2-4faa-ac24-fd1786c0443e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293", "node_type": "1", "metadata": {}, "hash": "1d4158759afa2e6fdcf645a795b71ad437c4534256f1d8cf2985797f1bf83adf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b", "node_type": "1", "metadata": {}, "hash": "2c2f69d0191faaf64434784a383d28c773541f7760a1703d68454e974c53ddf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A job gets \ninitiated when a Spark action is called (such as collect). We will learn more about actions \nlater. When an action (such as collect or count) is invoked on a dataset, it triggers the \nexecution of one or more jobs.\nA job consists of several stages, each containing tasks that execute a set of transformations on \ndata partitions.\n\u2022\t Stage: Each job is divided into stages that may depend on other stages. Stages act as transformation \nboundaries \u2013 they are created at the boundaries of wide transformations that require data \nshuffling across partitions. If a stage is dependent on outputs from a previous stage, then this \nstage would not begin execution until the previous dependent stages have finished execution.\nEach stage is divided into a set of tasks to be executed on the cluster nodes, processing data \nin parallel.\n\u2022\t Task: A task is the smallest unit of execution in Spark.", "mimetype": "text/plain", "start_char_idx": 60024, "end_char_idx": 60921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b": {"__data__": {"id_": "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e6c009-d0f2-4faa-ac24-fd1786c0443e", "node_type": "1", "metadata": {}, "hash": "3d61acc5aee4cfdf7e1af0c06cd25126768ada9645551e8e7efeee2f9b036fd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18da6f96-247f-4fec-b2c7-74777312f774", "node_type": "1", "metadata": {}, "hash": "c1648d847a715109df490d82fb140eff7f48be6c7be58301336383a3ddd03b11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a stage is dependent on outputs from a previous stage, then this \nstage would not begin execution until the previous dependent stages have finished execution.\nEach stage is divided into a set of tasks to be executed on the cluster nodes, processing data \nin parallel.\n\u2022\t Task: A task is the smallest unit of execution in Spark. It is the smallest object compiled and \nrun by Spark to perform a group of operations. It is executed on a Spark executor. Tasks are \nessentially a series of operations such as filter, groupBy, and others.\nTasks run in parallel across executors. They can be run on multiple nodes and are independent of \neach other. This is done with the help of slots. Each task processes a portion of the data partition. \nOccasionally, a group of these tasks has to finish execution to begin the next task\u2019s execution.", "mimetype": "text/plain", "start_char_idx": 60591, "end_char_idx": 61425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18da6f96-247f-4fec-b2c7-74777312f774": {"__data__": {"id_": "18da6f96-247f-4fec-b2c7-74777312f774", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b", "node_type": "1", "metadata": {}, "hash": "2c2f69d0191faaf64434784a383d28c773541f7760a1703d68454e974c53ddf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4", "node_type": "1", "metadata": {}, "hash": "af6a7e1572cef1263c997133d29e4b7095d59763f612e4d1a655ffa08e3ee007", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tasks are \nessentially a series of operations such as filter, groupBy, and others.\nTasks run in parallel across executors. They can be run on multiple nodes and are independent of \neach other. This is done with the help of slots. Each task processes a portion of the data partition. \nOccasionally, a group of these tasks has to finish execution to begin the next task\u2019s execution.\nNow that we understand these concepts, let\u2019s see why they are significant in Spark:\n\u2022\t Parallel processing: Executors, jobs, stages, and tasks collaborate to enable parallel execution \nof computations, optimizing performance by leveraging distributed computing\n\u2022\t Task granularity and efficiency: Tasks divide computations into smaller units, facilitating \nefficient resource utilization and parallelism across cluster nodes\nNext, we will move on to discuss a significant concept that enhances efficiency in computation.", "mimetype": "text/plain", "start_char_idx": 61045, "end_char_idx": 61946, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4": {"__data__": {"id_": "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18da6f96-247f-4fec-b2c7-74777312f774", "node_type": "1", "metadata": {}, "hash": "c1648d847a715109df490d82fb140eff7f48be6c7be58301336383a3ddd03b11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9", "node_type": "1", "metadata": {}, "hash": "671972d965099b582a0ce34259a26205249aef9b014635e77c6e288def9d2b96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now that we understand these concepts, let\u2019s see why they are significant in Spark:\n\u2022\t Parallel processing: Executors, jobs, stages, and tasks collaborate to enable parallel execution \nof computations, optimizing performance by leveraging distributed computing\n\u2022\t Task granularity and efficiency: Tasks divide computations into smaller units, facilitating \nefficient resource utilization and parallelism across cluster nodes\nNext, we will move on to discuss a significant concept that enhances efficiency in computation.\nPartitioning in Spark\nIn Apache Spark, partitioning is a critical concept that\u2019s used to divide data across multiple nodes \nin a cluster for parallel processing. Partitioning improves data locality, enhances performance, and \nenables efficient computation by distributing data in a structured manner. Spark supports both static \nand dynamic partitioning strategies to organize data across the cluster nodes:\n\u2022\t Static partitioning of resources: Static partitioning is available on all cluster managers.", "mimetype": "text/plain", "start_char_idx": 61426, "end_char_idx": 62449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9": {"__data__": {"id_": "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4", "node_type": "1", "metadata": {}, "hash": "af6a7e1572cef1263c997133d29e4b7095d59763f612e4d1a655ffa08e3ee007", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de3a88b4-0bab-4599-8b00-c305d8aa693f", "node_type": "1", "metadata": {}, "hash": "e78c01ee80d89646d7cb9356b4f86cace9f1dee48e82d8b3dc035100f76919d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Partitioning improves data locality, enhances performance, and \nenables efficient computation by distributing data in a structured manner. Spark supports both static \nand dynamic partitioning strategies to organize data across the cluster nodes:\n\u2022\t Static partitioning of resources: Static partitioning is available on all cluster managers. With \nstatic partitioning, maximum resources are allocated to each application and these resources \nremain dedicated to these applications during their lifetime.\n\nSpark Architecture and Transformations\n\n\u2022\t Dynamic sharing of resources: Dynamic partitioning is only available on Mesos. When \ndynamically sharing resources, the Spark application gets fixed and independent memory \nallocation, such as static partitioning. The major difference is that when the tasks are not being \nrun by an application, these cores can be used by other applications as well.", "mimetype": "text/plain", "start_char_idx": 62109, "end_char_idx": 63006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de3a88b4-0bab-4599-8b00-c305d8aa693f": {"__data__": {"id_": "de3a88b4-0bab-4599-8b00-c305d8aa693f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9", "node_type": "1", "metadata": {}, "hash": "671972d965099b582a0ce34259a26205249aef9b014635e77c6e288def9d2b96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9075e7da-a7c3-49d3-86e3-98a8f728c72c", "node_type": "1", "metadata": {}, "hash": "292a380384d919838a95bca7ca20bb9dfb907981a97535d25262904b45a01029", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Architecture and Transformations\n\n\u2022\t Dynamic sharing of resources: Dynamic partitioning is only available on Mesos. When \ndynamically sharing resources, the Spark application gets fixed and independent memory \nallocation, such as static partitioning. The major difference is that when the tasks are not being \nrun by an application, these cores can be used by other applications as well.\nLet\u2019s discuss why partitioning is significant:\n\u2022\t Performance optimization: Effective partitioning strategies, whether static or dynamic, \nsignificantly impact Spark\u2019s performance by improving data locality and reducing data shuffle\n\u2022\t Adaptability and flexibility: Dynamic partitioning provides adaptability to varying data sizes \nor distribution patterns without manual intervention\n\u2022\t Control and predictability: Static partitioning offers control and predictability over data \ndistribution, which can be advantageous in specific use cases\nIn summary, partitioning strategies \u2013 whether static or dynamic \u2013 in Spark play a crucial role in \noptimizing data distribution across cluster nodes, improving performance, and ensuring efficient \nparallel processing of data.", "mimetype": "text/plain", "start_char_idx": 62613, "end_char_idx": 63775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9075e7da-a7c3-49d3-86e3-98a8f728c72c": {"__data__": {"id_": "9075e7da-a7c3-49d3-86e3-98a8f728c72c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de3a88b4-0bab-4599-8b00-c305d8aa693f", "node_type": "1", "metadata": {}, "hash": "e78c01ee80d89646d7cb9356b4f86cace9f1dee48e82d8b3dc035100f76919d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d22501c-1b2b-4249-9a81-22f372250627", "node_type": "1", "metadata": {}, "hash": "5754724d781bb2446474b04398cbf473e7a878b8362384cbb5d0535ca549d8f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Apache Spark offers different cluster and deployment modes to run applications across distributed \ncomputing environments. We\u2019ll take a look at them in the next section.\nDeployment modes\nThere are different deployment modes available in Spark. These deployment modes define how Spark \napplications are launched, executed, and managed in diverse computing infrastructures. Based on \nthese different deployment modes, it gets decided where the Spark driver, executor, and cluster \nmanager will run.\nThe different deployment modes that are available in Spark are as follows:\n\u2022\t Local: In local mode, the Spark driver and executor run on a single JVM and the cluster manager \nruns on the same host as the driver and executor.\n\u2022\t Standalone: In standalone mode, the driver can run on any node of the cluster and the executor \nwill launch its own independent JVM. The cluster manager can remain on any of the hosts in \nthe cluster.", "mimetype": "text/plain", "start_char_idx": 63776, "end_char_idx": 64701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d22501c-1b2b-4249-9a81-22f372250627": {"__data__": {"id_": "6d22501c-1b2b-4249-9a81-22f372250627", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9075e7da-a7c3-49d3-86e3-98a8f728c72c", "node_type": "1", "metadata": {}, "hash": "292a380384d919838a95bca7ca20bb9dfb907981a97535d25262904b45a01029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e003a72-00e0-4457-9fd8-b24ade8c6215", "node_type": "1", "metadata": {}, "hash": "3cf6d656bfc8078d28460fd4127d11e95ab084dc8cd31010e26e7ba8ec926add", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Standalone: In standalone mode, the driver can run on any node of the cluster and the executor \nwill launch its own independent JVM. The cluster manager can remain on any of the hosts in \nthe cluster.\n\u2022\t YARN (client): In this mode, the Spark driver runs on the client and YARN\u2019s resource manager \nallocates containers for executors on NodeManagers.\n\u2022\t YARN (cluster): In this mode, the Spark driver runs with the YARN application master while \nYARN\u2019s resource manager allocates containers for executors on NodeManagers.\n\u2022\t Kubernetes: In this mode, the driver runs in Kubernetes pods. Executors have their own pods.\n\nRDDs\n\nLet\u2019s look at some points of significance regarding the different deployment modes:\n\u2022\t Resource utilization: Different deployment modes optimize resource utilization by determining \nwhere the driver program runs and how resources are allocated between the client and the cluster.", "mimetype": "text/plain", "start_char_idx": 64498, "end_char_idx": 65404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e003a72-00e0-4457-9fd8-b24ade8c6215": {"__data__": {"id_": "9e003a72-00e0-4457-9fd8-b24ade8c6215", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d22501c-1b2b-4249-9a81-22f372250627", "node_type": "1", "metadata": {}, "hash": "5754724d781bb2446474b04398cbf473e7a878b8362384cbb5d0535ca549d8f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "367f703b-3d43-45ea-aaa9-7e6c1fd00231", "node_type": "1", "metadata": {}, "hash": "aa5737b647d84accc0e6203d87d6e77946e33ad7e0b05af207a635b5b757851e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Kubernetes: In this mode, the driver runs in Kubernetes pods. Executors have their own pods.\n\nRDDs\n\nLet\u2019s look at some points of significance regarding the different deployment modes:\n\u2022\t Resource utilization: Different deployment modes optimize resource utilization by determining \nwhere the driver program runs and how resources are allocated between the client and the cluster.\n\u2022\t Accessibility and control: Client mode offers easy accessibility to driver logs and outputs, \nfacilitating development and debugging, while cluster mode utilizes cluster resources more \nefficiently for production workloads.\n\u2022\t Integration with container orchestration: Kubernetes deployment mode enables seamless \nintegration with containerized environments, leveraging Kubernetes\u2019 orchestration capabilities \nfor efficient resource management.", "mimetype": "text/plain", "start_char_idx": 65022, "end_char_idx": 65852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "367f703b-3d43-45ea-aaa9-7e6c1fd00231": {"__data__": {"id_": "367f703b-3d43-45ea-aaa9-7e6c1fd00231", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e003a72-00e0-4457-9fd8-b24ade8c6215", "node_type": "1", "metadata": {}, "hash": "3cf6d656bfc8078d28460fd4127d11e95ab084dc8cd31010e26e7ba8ec926add", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b096f3e-379b-4790-8e1b-20c4050d6c04", "node_type": "1", "metadata": {}, "hash": "da57571903b31c641598ba5afe79b8878dd652ea5099d92a6e7799c668b61d2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Accessibility and control: Client mode offers easy accessibility to driver logs and outputs, \nfacilitating development and debugging, while cluster mode utilizes cluster resources more \nefficiently for production workloads.\n\u2022\t Integration with container orchestration: Kubernetes deployment mode enables seamless \nintegration with containerized environments, leveraging Kubernetes\u2019 orchestration capabilities \nfor efficient resource management.\nThere are some considerations to keep in mind while choosing deployment modes:\n\u2022\t Development versus production: Client mode is suitable for development and debugging, \nwhile cluster mode is ideal for production workloads\n\u2022\t Resource management: Evaluate the allocation of resources between client and cluster nodes \nbased on the application\u2019s requirements\n\u2022\t Containerization needs: Consider Kubernetes deployment for containerized environments, \nleveraging Kubernetes features for efficient container management\nIn summary, deployment modes in Apache Spark provide flexibility in how Spark applications are launched \nand executed, catering to different development, production, and containerized deployment scenarios.", "mimetype": "text/plain", "start_char_idx": 65405, "end_char_idx": 66572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b096f3e-379b-4790-8e1b-20c4050d6c04": {"__data__": {"id_": "8b096f3e-379b-4790-8e1b-20c4050d6c04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "367f703b-3d43-45ea-aaa9-7e6c1fd00231", "node_type": "1", "metadata": {}, "hash": "aa5737b647d84accc0e6203d87d6e77946e33ad7e0b05af207a635b5b757851e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32b30a21-99c5-4984-84e3-8aa7fb3da840", "node_type": "1", "metadata": {}, "hash": "af25fac70d0cae35693e8ab971f921a03718577842104200af2c16da418dc048", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, we will look at RDDs, which serve as foundational data abstractions in Apache Spark, enabling \ndistributed processing, fault tolerance, and flexibility in handling large-scale data operations. While \nRDDs continue to be a fundamental concept, Spark\u2019s DataFrame and Dataset APIs offer advancements \nin structured data processing and performance optimization.\nRDDs\nApache Spark\u2019s RDD stands as a foundational abstraction that underpins the distributed computing \ncapabilities within the Spark framework. RDDs serve as the core data structure in Spark, enabling \nfault-tolerant and parallel operations on large-scale distributed datasets and they are immutable. This \nmeans that they cannot be changed over time. For any operations, a new RDD has to be generated from \nthe existing RDD. When a new RDD originates from the original RDD, the new RDD has a pointer to \nthe RDD it is generated from. This is the way Spark documents the lineage for all the transformations \ntaking place on an RDD.", "mimetype": "text/plain", "start_char_idx": 66573, "end_char_idx": 67568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32b30a21-99c5-4984-84e3-8aa7fb3da840": {"__data__": {"id_": "32b30a21-99c5-4984-84e3-8aa7fb3da840", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b096f3e-379b-4790-8e1b-20c4050d6c04", "node_type": "1", "metadata": {}, "hash": "da57571903b31c641598ba5afe79b8878dd652ea5099d92a6e7799c668b61d2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1473f48-3fd4-4d9d-ae71-6ef716f6a443", "node_type": "1", "metadata": {}, "hash": "633db2af74ce02401cbc679774fa76a1d2beb45e02e04b74552508f9d4186fa2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This \nmeans that they cannot be changed over time. For any operations, a new RDD has to be generated from \nthe existing RDD. When a new RDD originates from the original RDD, the new RDD has a pointer to \nthe RDD it is generated from. This is the way Spark documents the lineage for all the transformations \ntaking place on an RDD. This lineage enables lazy evaluation in Spark, which generates DAGs for \ndifferent operations.\n\nSpark Architecture and Transformations\n\nThis immutability and lineage gives Spark the ability to reproduce any DataFrame in case of failure \nand it makes fault-tolerant by design. Since RDD is the lowest level of abstraction in Spark, all other \ndatasets built on top of RDDs share these properties. The high-level DataFrame API is built on top of \nthe low-level RDD API as well, so DataFrames also share the same properties.", "mimetype": "text/plain", "start_char_idx": 67238, "end_char_idx": 68090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1473f48-3fd4-4d9d-ae71-6ef716f6a443": {"__data__": {"id_": "c1473f48-3fd4-4d9d-ae71-6ef716f6a443", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b30a21-99c5-4984-84e3-8aa7fb3da840", "node_type": "1", "metadata": {}, "hash": "af25fac70d0cae35693e8ab971f921a03718577842104200af2c16da418dc048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4450f7d-86ea-4489-a8f8-592d9e1106b2", "node_type": "1", "metadata": {}, "hash": "74c87d2b1e5e6a17008276d35cd7523adba48d6d343dd35558ee7cd6f7489c76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Architecture and Transformations\n\nThis immutability and lineage gives Spark the ability to reproduce any DataFrame in case of failure \nand it makes fault-tolerant by design. Since RDD is the lowest level of abstraction in Spark, all other \ndatasets built on top of RDDs share these properties. The high-level DataFrame API is built on top of \nthe low-level RDD API as well, so DataFrames also share the same properties.\nRDDs are also partitioned by Spark and each partition is distributed to multiple nodes in the cluster.\nHere are some of the key characteristics of Spark RDDs:\n\u2022\t Immutable nature: RDDs are immutable, ensuring that once created, they cannot be altered, \nallowing for a lineage of transformations.\n\u2022\t Resilience through lineage: RDDs store lineage information, enabling reconstruction of lost \npartitions in case of failures. Spark is designed to be fault-tolerant.", "mimetype": "text/plain", "start_char_idx": 67665, "end_char_idx": 68554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4450f7d-86ea-4489-a8f8-592d9e1106b2": {"__data__": {"id_": "a4450f7d-86ea-4489-a8f8-592d9e1106b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1473f48-3fd4-4d9d-ae71-6ef716f6a443", "node_type": "1", "metadata": {}, "hash": "633db2af74ce02401cbc679774fa76a1d2beb45e02e04b74552508f9d4186fa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bffc1a77-e752-46ff-98d0-926587199147", "node_type": "1", "metadata": {}, "hash": "ce307cc0c6f043051289ca53868dc3fce335f6ccb2de8143d6a96e0e918d8821", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here are some of the key characteristics of Spark RDDs:\n\u2022\t Immutable nature: RDDs are immutable, ensuring that once created, they cannot be altered, \nallowing for a lineage of transformations.\n\u2022\t Resilience through lineage: RDDs store lineage information, enabling reconstruction of lost \npartitions in case of failures. Spark is designed to be fault-tolerant. Therefore, if an executor \non a worker node fails while calculating an RDD, that RDD can be recomputed by another \nexecutor using the lineage that Spark has created.\n\u2022\t Partitioned data: RDDs divide data into partitions, distributed across multiple nodes in a \ncluster for parallel processing.\n\u2022\t Parallel execution: Spark executes operations on RDDs in parallel across distributed partitions, \nenhancing performance.\nLet\u2019s discuss some more characteristics in detail.\nLazy computation\nRDDs support lazy evaluation, deferring execution of transformations until an action is invoked.", "mimetype": "text/plain", "start_char_idx": 68194, "end_char_idx": 69137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bffc1a77-e752-46ff-98d0-926587199147": {"__data__": {"id_": "bffc1a77-e752-46ff-98d0-926587199147", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4450f7d-86ea-4489-a8f8-592d9e1106b2", "node_type": "1", "metadata": {}, "hash": "74c87d2b1e5e6a17008276d35cd7523adba48d6d343dd35558ee7cd6f7489c76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "730711b2-f2e6-4881-97ae-60f2723fbcbd", "node_type": "1", "metadata": {}, "hash": "436027459b5038dfe317180593acb1cd9d33972cea40c9d279ddf5c544d9e9b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Partitioned data: RDDs divide data into partitions, distributed across multiple nodes in a \ncluster for parallel processing.\n\u2022\t Parallel execution: Spark executes operations on RDDs in parallel across distributed partitions, \nenhancing performance.\nLet\u2019s discuss some more characteristics in detail.\nLazy computation\nRDDs support lazy evaluation, deferring execution of transformations until an action is invoked. \nThe way Spark achieves its efficiency in processing and fault tolerance is through lazy evaluation. \nCode execution in Spark is delayed. Unless an action is called an operation, Spark does not start code \nexecution. This helps Spark achieve optimization as well. For all the transformations and actions, Spark \nkeeps track of the steps in the code that need to be executed by creating a DAG for these operations. \nBecause Spark creates the query plan before execution, it can make smart decisions about the hierarchy \nof execution as well. To achieve this, one of the features Spark uses is called predicate pushdown.", "mimetype": "text/plain", "start_char_idx": 68721, "end_char_idx": 69756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "730711b2-f2e6-4881-97ae-60f2723fbcbd": {"__data__": {"id_": "730711b2-f2e6-4881-97ae-60f2723fbcbd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bffc1a77-e752-46ff-98d0-926587199147", "node_type": "1", "metadata": {}, "hash": "ce307cc0c6f043051289ca53868dc3fce335f6ccb2de8143d6a96e0e918d8821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1710651a-5562-4711-a26a-d5cd21cf1ce5", "node_type": "1", "metadata": {}, "hash": "da0e55105864e5e8d3957106e85fab81d2580a5b7a5b31cdafcf8a30965a8315", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This helps Spark achieve optimization as well. For all the transformations and actions, Spark \nkeeps track of the steps in the code that need to be executed by creating a DAG for these operations. \nBecause Spark creates the query plan before execution, it can make smart decisions about the hierarchy \nof execution as well. To achieve this, one of the features Spark uses is called predicate pushdown.\nPredicate pushdown means that Spark can prioritize the operations to make them the most efficient. \nOne example can be a filter operation. A filter operation would generally reduce the amount of data \nthat the subsequent operations have to work with if the filter operation can be applied before other \ntransformations. This is exactly how Spark operates. It will execute filters as early in the process as \npossible, thus making the next operations more performant.\nThis also implies that Spark jobs would fail only at execution time.", "mimetype": "text/plain", "start_char_idx": 69355, "end_char_idx": 70292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1710651a-5562-4711-a26a-d5cd21cf1ce5": {"__data__": {"id_": "1710651a-5562-4711-a26a-d5cd21cf1ce5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "730711b2-f2e6-4881-97ae-60f2723fbcbd", "node_type": "1", "metadata": {}, "hash": "436027459b5038dfe317180593acb1cd9d33972cea40c9d279ddf5c544d9e9b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff7e4b28-68d9-4b8a-943d-51fc51745a35", "node_type": "1", "metadata": {}, "hash": "ec77cc96e69f64c88b89a2fc0eb3de01501874886dd66a65591a64e0531c6f03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One example can be a filter operation. A filter operation would generally reduce the amount of data \nthat the subsequent operations have to work with if the filter operation can be applied before other \ntransformations. This is exactly how Spark operates. It will execute filters as early in the process as \npossible, thus making the next operations more performant.\nThis also implies that Spark jobs would fail only at execution time. Since Spark uses lazy evaluation, \nuntil an action is called, the code is not executed and certain errors can be missed. To catch these \nerrors, Spark code would need to have an action for execution and hence error handling.\n\nRDDs\n\nTransformations\nTransformations create new RDDs by applying functions to existing RDDs (for example, map, \nfilter, and reduce). Transformations are operations that do not result in any code execution. \nThese statements result in Spark creating a DAG for execution.", "mimetype": "text/plain", "start_char_idx": 69857, "end_char_idx": 70789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff7e4b28-68d9-4b8a-943d-51fc51745a35": {"__data__": {"id_": "ff7e4b28-68d9-4b8a-943d-51fc51745a35", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1710651a-5562-4711-a26a-d5cd21cf1ce5", "node_type": "1", "metadata": {}, "hash": "da0e55105864e5e8d3957106e85fab81d2580a5b7a5b31cdafcf8a30965a8315", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05948547-c438-4254-a7fe-093e21294490", "node_type": "1", "metadata": {}, "hash": "b809e8d70cce357ac077578a41a25d21c10a95dfccc993599f8b43fcd3d60b51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To catch these \nerrors, Spark code would need to have an action for execution and hence error handling.\n\nRDDs\n\nTransformations\nTransformations create new RDDs by applying functions to existing RDDs (for example, map, \nfilter, and reduce). Transformations are operations that do not result in any code execution. \nThese statements result in Spark creating a DAG for execution. Once that DAG is created, Spark would \nneed an action operation in the end to run the code. Due to this, when certain developers try to time \nthe code from Spark, they see that certain operations\u2019 runtime is very fast. The reason could be that \nthe code is only comprised of transformations until that point. Since no action is present, the code \ndoesn\u2019t run. To accurately measure the runtime of each operation, actions have to be called to force \nSpark to execute those statements.", "mimetype": "text/plain", "start_char_idx": 70414, "end_char_idx": 71273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05948547-c438-4254-a7fe-093e21294490": {"__data__": {"id_": "05948547-c438-4254-a7fe-093e21294490", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff7e4b28-68d9-4b8a-943d-51fc51745a35", "node_type": "1", "metadata": {}, "hash": "ec77cc96e69f64c88b89a2fc0eb3de01501874886dd66a65591a64e0531c6f03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92faecb8-d912-4363-9a3c-a1d659aaafcf", "node_type": "1", "metadata": {}, "hash": "1f9122d69b05f85c9e55a734f1b4f455735a2926f35b41e639f36b589c0386c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Due to this, when certain developers try to time \nthe code from Spark, they see that certain operations\u2019 runtime is very fast. The reason could be that \nthe code is only comprised of transformations until that point. Since no action is present, the code \ndoesn\u2019t run. To accurately measure the runtime of each operation, actions have to be called to force \nSpark to execute those statements.\nHere are some of the operations that can be classified as transformations:\n\u2022\t orderBy()\n\u2022\t groupBy()\n\u2022\t filter()\n\u2022\t select()\n\u2022\t join()\nWhen these commands are executed, they are evaluated lazily. This means all these operations on \nDataFrames result in a new DataFrame, but they are not executed until an action is followed by them. \nThis would return a DataFrame or RDD when it is triggered by an action.", "mimetype": "text/plain", "start_char_idx": 70882, "end_char_idx": 71679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92faecb8-d912-4363-9a3c-a1d659aaafcf": {"__data__": {"id_": "92faecb8-d912-4363-9a3c-a1d659aaafcf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05948547-c438-4254-a7fe-093e21294490", "node_type": "1", "metadata": {}, "hash": "b809e8d70cce357ac077578a41a25d21c10a95dfccc993599f8b43fcd3d60b51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4653ce15-0e56-4981-9575-4e1c769d11e4", "node_type": "1", "metadata": {}, "hash": "4aeb9ad7c21db5d40029921d25be0da1da32ed4a3ef3ca1df57459b41c043717", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This means all these operations on \nDataFrames result in a new DataFrame, but they are not executed until an action is followed by them. \nThis would return a DataFrame or RDD when it is triggered by an action.\nActions and computation execution\nActions (for example, collect, count, and saveAsTextFile) prompt the execution of \ntransformations on RDDs. Execution is triggered by actions only, not by transformations. When an \naction is called, this is when Spark starts execution on the DAG it created during the analysis phase of \ncode. With the DAG created, Spark creates multiple query plans based on its internal optimizations. \nThen, it executes the plan that is the most efficient and cost-effective. We will discuss query plans \nlater in this book.", "mimetype": "text/plain", "start_char_idx": 71470, "end_char_idx": 72224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4653ce15-0e56-4981-9575-4e1c769d11e4": {"__data__": {"id_": "4653ce15-0e56-4981-9575-4e1c769d11e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92faecb8-d912-4363-9a3c-a1d659aaafcf", "node_type": "1", "metadata": {}, "hash": "1f9122d69b05f85c9e55a734f1b4f455735a2926f35b41e639f36b589c0386c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64eafbef-8222-4b69-a671-e92a39719c87", "node_type": "1", "metadata": {}, "hash": "8a88d312057a005acaa6e734a1e17f1c441cff9aa9e00a843477d8ecaed3cce3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Execution is triggered by actions only, not by transformations. When an \naction is called, this is when Spark starts execution on the DAG it created during the analysis phase of \ncode. With the DAG created, Spark creates multiple query plans based on its internal optimizations. \nThen, it executes the plan that is the most efficient and cost-effective. We will discuss query plans \nlater in this book.\nHere are some of the operations that can be classified as actions:\n\u2022\t show()\n\u2022\t take()\n\u2022\t count()\n\u2022\t collect()\n\u2022\t save()\n\u2022\t foreach()\n\u2022\t first()\n\nSpark Architecture and Transformations\n\nAll of these operations would result in Spark triggering code execution and thus operations are run.", "mimetype": "text/plain", "start_char_idx": 71822, "end_char_idx": 72511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64eafbef-8222-4b69-a671-e92a39719c87": {"__data__": {"id_": "64eafbef-8222-4b69-a671-e92a39719c87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4653ce15-0e56-4981-9575-4e1c769d11e4", "node_type": "1", "metadata": {}, "hash": "4aeb9ad7c21db5d40029921d25be0da1da32ed4a3ef3ca1df57459b41c043717", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84713e50-4c3a-458f-a6c6-ea140ae0edac", "node_type": "1", "metadata": {}, "hash": "156d70221825077ec76800be2effc0dcd6caccda3e2c9cffe0e8b4c978297619", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will discuss query plans \nlater in this book.\nHere are some of the operations that can be classified as actions:\n\u2022\t show()\n\u2022\t take()\n\u2022\t count()\n\u2022\t collect()\n\u2022\t save()\n\u2022\t foreach()\n\u2022\t first()\n\nSpark Architecture and Transformations\n\nAll of these operations would result in Spark triggering code execution and thus operations are run.\nLet\u2019s take a look at the following code to understand these concepts better:\n# Python\n>>> df = spark.read.text(\"{path_to_data_file}\")\n>>> names_df = df.select(col(\"firstname\"),col(\"lastname\"))\n>>> names_df.show()\nIn the preceding code, until line 2, nothing would be executed. On line 3, an action is triggered and \nthus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the \nwrong column names in line 2, Spark will not detect this until it runs line 3.", "mimetype": "text/plain", "start_char_idx": 72176, "end_char_idx": 73010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84713e50-4c3a-458f-a6c6-ea140ae0edac": {"__data__": {"id_": "84713e50-4c3a-458f-a6c6-ea140ae0edac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64eafbef-8222-4b69-a671-e92a39719c87", "node_type": "1", "metadata": {}, "hash": "8a88d312057a005acaa6e734a1e17f1c441cff9aa9e00a843477d8ecaed3cce3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2d0444c-fb6b-4f0c-940c-8bcd78139052", "node_type": "1", "metadata": {}, "hash": "7a904b4cdb7642b25d3ffdc0d760dbab1e88c63fc09aa2eee6d0e6384d32effc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On line 3, an action is triggered and \nthus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the \nwrong column names in line 2, Spark will not detect this until it runs line 3. This is a different paradigm \nthan most other programming paradigms. This is what we call lazy evaluation in Spark.\nActions bring about computation and collect results to be sent to the driver program.\nNow that we\u2019ve covered the basics of transformations and actions in Spark, let\u2019s move on to understanding \nthe two types of transformations it offers.\nTypes of transformations\nApache Spark\u2019s transformations are broadly categorized into narrow and wide transformations, each \nserving distinct purposes in the context of distributed data processing.\nNarrow transformations\nNarrow transformations, also known as local transformations, operate on individual partitions of \ndata without shuffling or redistributing data across partitions.", "mimetype": "text/plain", "start_char_idx": 72789, "end_char_idx": 73746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2d0444c-fb6b-4f0c-940c-8bcd78139052": {"__data__": {"id_": "e2d0444c-fb6b-4f0c-940c-8bcd78139052", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84713e50-4c3a-458f-a6c6-ea140ae0edac", "node_type": "1", "metadata": {}, "hash": "156d70221825077ec76800be2effc0dcd6caccda3e2c9cffe0e8b4c978297619", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18eaf744-9237-4f6e-b329-07628e71762a", "node_type": "1", "metadata": {}, "hash": "c0b5b71c65d6118a3c15f229c239efc87435aad37b1095c035c24100965179f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Types of transformations\nApache Spark\u2019s transformations are broadly categorized into narrow and wide transformations, each \nserving distinct purposes in the context of distributed data processing.\nNarrow transformations\nNarrow transformations, also known as local transformations, operate on individual partitions of \ndata without shuffling or redistributing data across partitions. These transformations enable Spark to \nprocess data within a single partition independently. In narrow transformations, Spark will work with \na single input partition and a single output partition. This means that these types of transformations \nwould result in an operation that can be performed on a single partition. The data doesn\u2019t have to be \ntaken from multiple partitions or written back to multiple partitions. This results in operations that \ndon\u2019t require shuffle.", "mimetype": "text/plain", "start_char_idx": 73364, "end_char_idx": 74222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18eaf744-9237-4f6e-b329-07628e71762a": {"__data__": {"id_": "18eaf744-9237-4f6e-b329-07628e71762a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2d0444c-fb6b-4f0c-940c-8bcd78139052", "node_type": "1", "metadata": {}, "hash": "7a904b4cdb7642b25d3ffdc0d760dbab1e88c63fc09aa2eee6d0e6384d32effc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f23d879f-3d91-4cfd-b415-f049b77e4ecc", "node_type": "1", "metadata": {}, "hash": "91ce3a4545275f828e257998821d20521966f0f33dc1fcdabef2c758ceec64ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These transformations enable Spark to \nprocess data within a single partition independently. In narrow transformations, Spark will work with \na single input partition and a single output partition. This means that these types of transformations \nwould result in an operation that can be performed on a single partition. The data doesn\u2019t have to be \ntaken from multiple partitions or written back to multiple partitions. This results in operations that \ndon\u2019t require shuffle.\nHere are some of their characteristics:\n\u2022\t Partition-level operation: Narrow transformations process data at the partition level, performing \ncomputations within each partition\n\u2022\t Independence and local processing: They do not require data movement or communication \nacross partitions, allowing parallel execution within partitions\n\u2022\t Examples: Operations such as map, filter, and flatMap are typical examples of \nnarrow transformations\n\nRDDs\n\nNow, let\u2019s look at their significance:\n\u2022\t Efficiency and speed: Narrow transformations are efficient as they involve local processing \nwithin partitions, reducing communication overhead\n\u2022\t Parallelism: They facilitate maximum parallelism by operating on partitions independently, \noptimizing performance\nWide transformations\nWide transformations, also termed global or shuffle-dependent transformations, involve operations that \nrequire data shuffling and redistribution across partitions.", "mimetype": "text/plain", "start_char_idx": 73747, "end_char_idx": 75156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f23d879f-3d91-4cfd-b415-f049b77e4ecc": {"__data__": {"id_": "f23d879f-3d91-4cfd-b415-f049b77e4ecc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18eaf744-9237-4f6e-b329-07628e71762a", "node_type": "1", "metadata": {}, "hash": "c0b5b71c65d6118a3c15f229c239efc87435aad37b1095c035c24100965179f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c288223-633c-432b-8af7-34a92f6d8db0", "node_type": "1", "metadata": {}, "hash": "1ae6895e7ad00d8c6cdb6313d0f374b0457c492a000bceabb51f1ff6569c2ea6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These transformations involve dependencies \nbetween partitions, necessitating data exchange. With wide transformations, Spark will use the data \npresent on multiple partitions and it could also write back the results to multiple partitions. These \ntransformations would force a shuffle operation, so they are also referred to as shuffle transformations.\nWide transformations are complex operations. They would need to write the results out in between \noperations if needed and they also have to aggregate data across different machines in certain cases.\nHere are some of their characteristics:\n\u2022\t Data shuffling: Wide transformations reorganize data across partitions by reshuffling or \naggregating data from multiple partitions\n\u2022\t Dependency on multiple partitions: They depend on data from various partitions, leading to \nthe exchange and reorganization of data across the cluster\n\u2022\t Examples: Operations such as groupBy, join, and sortByKey are typical examples of \nwide transformations\nNow,", "mimetype": "text/plain", "start_char_idx": 75157, "end_char_idx": 76151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c288223-633c-432b-8af7-34a92f6d8db0": {"__data__": {"id_": "9c288223-633c-432b-8af7-34a92f6d8db0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f23d879f-3d91-4cfd-b415-f049b77e4ecc", "node_type": "1", "metadata": {}, "hash": "91ce3a4545275f828e257998821d20521966f0f33dc1fcdabef2c758ceec64ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4", "node_type": "1", "metadata": {}, "hash": "f5aa62ffbd6c7f5bd1b17e0c4dc37ec6947591b4047c61e5a0168f2129ba1abe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here are some of their characteristics:\n\u2022\t Data shuffling: Wide transformations reorganize data across partitions by reshuffling or \naggregating data from multiple partitions\n\u2022\t Dependency on multiple partitions: They depend on data from various partitions, leading to \nthe exchange and reorganization of data across the cluster\n\u2022\t Examples: Operations such as groupBy, join, and sortByKey are typical examples of \nwide transformations\nNow, let\u2019s look at their significance:\n\u2022\t Network and disk overhead: Wide transformations introduce network and disk overhead due \nto data shuffling, impacting performance\n\u2022\t Stage boundary creation: They define stage boundaries within a Spark job, resulting in distinct \nstages during job execution\nThe following are the differences between narrow and wide transformations:\n\u2022\t Data movement: Narrow transformations process data within partitions locally, minimizing data \nmovement,", "mimetype": "text/plain", "start_char_idx": 75711, "end_char_idx": 76629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4": {"__data__": {"id_": "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c288223-633c-432b-8af7-34a92f6d8db0", "node_type": "1", "metadata": {}, "hash": "1ae6895e7ad00d8c6cdb6313d0f374b0457c492a000bceabb51f1ff6569c2ea6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccbc9915-12f6-4cc0-a1b1-0f036101603d", "node_type": "1", "metadata": {}, "hash": "c911a341f647a0ccf3a27df11e3cf063877668a6f0d2d516966d2584e464711d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "let\u2019s look at their significance:\n\u2022\t Network and disk overhead: Wide transformations introduce network and disk overhead due \nto data shuffling, impacting performance\n\u2022\t Stage boundary creation: They define stage boundaries within a Spark job, resulting in distinct \nstages during job execution\nThe following are the differences between narrow and wide transformations:\n\u2022\t Data movement: Narrow transformations process data within partitions locally, minimizing data \nmovement, while wide transformations involve data shuffling and movement across partitions\n\u2022\t Performance impact: Narrow transformations typically offer higher performance due to reduced \ndata movement, whereas wide transformations involve additional overhead due to data shuffling\n\u2022\t Parallelism scope: Narrow transformations enable maximum parallelism within partitions, \nwhile wide transformations might limit parallelism due to dependency on multiple partitions\n\nSpark Architecture and Transformations\n\nIn Apache Spark, understanding the distinction between narrow and wide transformations is crucial.", "mimetype": "text/plain", "start_char_idx": 76152, "end_char_idx": 77225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccbc9915-12f6-4cc0-a1b1-0f036101603d": {"__data__": {"id_": "ccbc9915-12f6-4cc0-a1b1-0f036101603d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4", "node_type": "1", "metadata": {}, "hash": "f5aa62ffbd6c7f5bd1b17e0c4dc37ec6947591b4047c61e5a0168f2129ba1abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3f88296-eaf4-4f23-8b61-b2581e660d97", "node_type": "1", "metadata": {}, "hash": "9bbc1c4b8e8c3137ebcc54bb158918e6e99bb5982413efe9965ae02cb6f2d1e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "whereas wide transformations involve additional overhead due to data shuffling\n\u2022\t Parallelism scope: Narrow transformations enable maximum parallelism within partitions, \nwhile wide transformations might limit parallelism due to dependency on multiple partitions\n\nSpark Architecture and Transformations\n\nIn Apache Spark, understanding the distinction between narrow and wide transformations is crucial. \nNarrow transformations excel in local processing within partitions, optimizing performance, while \nwide transformations, although necessary for certain operations, introduce overhead due to data \nshuffling and global reorganization across partitions.", "mimetype": "text/plain", "start_char_idx": 76823, "end_char_idx": 77477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3f88296-eaf4-4f23-8b61-b2581e660d97": {"__data__": {"id_": "d3f88296-eaf4-4f23-8b61-b2581e660d97", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccbc9915-12f6-4cc0-a1b1-0f036101603d", "node_type": "1", "metadata": {}, "hash": "c911a341f647a0ccf3a27df11e3cf063877668a6f0d2d516966d2584e464711d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "118680ef-4928-4cc0-ab0c-dd869b4d5046", "node_type": "1", "metadata": {}, "hash": "e7f86f3195a680caa1465c0a42536efdf2daf58a10e0e5cb85b42dabfbf673fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "while wide transformations might limit parallelism due to dependency on multiple partitions\n\nSpark Architecture and Transformations\n\nIn Apache Spark, understanding the distinction between narrow and wide transformations is crucial. \nNarrow transformations excel in local processing within partitions, optimizing performance, while \nwide transformations, although necessary for certain operations, introduce overhead due to data \nshuffling and global reorganization across partitions.\nLet\u2019s look at the significance of Spark RDDs:\n\u2022\t Distributed data processing: RDDs enable distributed processing of large-scale data across a \ncluster of machines, promoting parallelism and scalability\n\u2022\t Fault tolerance and reliability: Their immutability and lineage-based recovery ensure fault \ntolerance and reliability in distributed environments\n\u2022\t Flexibility in operations: RDDs support a wide array of transformations and actions, allowing \ndiverse data manipulations and processing operations\nEvolution and alternatives\nWhile RDDs remain fundamental, Spark\u2019s DataFrame and Dataset APIs offer optimized, higher-level \nabstractions suitable for structured data processing and optimization.", "mimetype": "text/plain", "start_char_idx": 76994, "end_char_idx": 78175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "118680ef-4928-4cc0-ab0c-dd869b4d5046": {"__data__": {"id_": "118680ef-4928-4cc0-ab0c-dd869b4d5046", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3f88296-eaf4-4f23-8b61-b2581e660d97", "node_type": "1", "metadata": {}, "hash": "9bbc1c4b8e8c3137ebcc54bb158918e6e99bb5982413efe9965ae02cb6f2d1e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08599053-8a3c-42f2-bc72-866dc55b67fb", "node_type": "1", "metadata": {}, "hash": "35e99cfcdb83a9693cbcf21d020acd45bb1da168d4f79c3f2d98c2329b50ba97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark RDDs serve as the bedrock of distributed data processing within the Apache Spark framework, \nproviding immutability, fault tolerance, and the foundational structure for performing parallel \noperations on distributed datasets. Although RDDs are fundamental, Spark\u2019s DataFrame and Dataset \nAPIs offer advancements in performance and structured data processing, catering to various use cases \nand preferences within the Spark ecosystem.\nSummary\nIn this chapter, we learned about Spark\u2019s architecture and its inner workings. This exploration of Spark\u2019s \ndistributed computing landscape covered different Spark components, such as the Spark driver and \nSparkSession. We also talked about the different types of cluster managers available in Spark. \nThen, we touched on different types of partitioning regarding Spark and its deployment modes.\nNext, we discussed Spark executors, jobs, stages, and tasks and highlighted the differences between \nthem before learning about RDDs and their transformation types, learning more about narrow and \nwide transformations.", "mimetype": "text/plain", "start_char_idx": 78176, "end_char_idx": 79238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08599053-8a3c-42f2-bc72-866dc55b67fb": {"__data__": {"id_": "08599053-8a3c-42f2-bc72-866dc55b67fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "118680ef-4928-4cc0-ab0c-dd869b4d5046", "node_type": "1", "metadata": {}, "hash": "e7f86f3195a680caa1465c0a42536efdf2daf58a10e0e5cb85b42dabfbf673fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cc734f1-2a46-4402-997f-ee1631ca6210", "node_type": "1", "metadata": {}, "hash": "d9eac80f4d3dfe4d057c5df914f9af51e09fed742707ec3cffba1590fadf0f32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also talked about the different types of cluster managers available in Spark. \nThen, we touched on different types of partitioning regarding Spark and its deployment modes.\nNext, we discussed Spark executors, jobs, stages, and tasks and highlighted the differences between \nthem before learning about RDDs and their transformation types, learning more about narrow and \nwide transformations.\nThese concepts form the foundation for harnessing Spark\u2019s immense capabilities in distributed data \nprocessing and analytics.\nIn the next chapter, we will discuss Spark DataFrames and their corresponding operations.\n\nPart 3: Spark Operations\nIn this part, we will cover Spark DataFrames and their operations, emphasizing their role in structured \ndata processing and analytics. This will include DataFrame creation, manipulation, and various \noperations such as filtering, aggregations, joins, and groupings, demonstrated through illustrative \nexamples. Then, we will discuss advanced operations and optimization techniques, including broadcast \nvariables, accumulators, and custom partitioning.", "mimetype": "text/plain", "start_char_idx": 78844, "end_char_idx": 79934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cc734f1-2a46-4402-997f-ee1631ca6210": {"__data__": {"id_": "1cc734f1-2a46-4402-997f-ee1631ca6210", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08599053-8a3c-42f2-bc72-866dc55b67fb", "node_type": "1", "metadata": {}, "hash": "35e99cfcdb83a9693cbcf21d020acd45bb1da168d4f79c3f2d98c2329b50ba97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22280c5a-dd62-4469-8ff3-40ca7f2738a9", "node_type": "1", "metadata": {}, "hash": "f06e48217f0f324e6492d352fe1c6a9ffd643e5ca44e3840c41a8377259ed840", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This will include DataFrame creation, manipulation, and various \noperations such as filtering, aggregations, joins, and groupings, demonstrated through illustrative \nexamples. Then, we will discuss advanced operations and optimization techniques, including broadcast \nvariables, accumulators, and custom partitioning. This part also talks about performance optimization \nstrategies, highlighting the significance of adaptive query execution and offering practical tips for \nenhancing Spark job performance. Furthermore, we will explore SQL queries in Spark, focusing on its \nSQL-like querying capabilities and interoperability with the DataFrame API. Examples will illustrate \ncomplex data manipulations and analytics through SQL queries in Spark.\nThis part has the following chapters:\n\u2022\t Chapter 4, Spark DataFrames and their Operations\n\u2022\t Chapter 5, Advanced Operations and Optimizations in Spark\n\u2022\t Chapter 6, SQL Queries in Spark", "mimetype": "text/plain", "start_char_idx": 79617, "end_char_idx": 80550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22280c5a-dd62-4469-8ff3-40ca7f2738a9": {"__data__": {"id_": "22280c5a-dd62-4469-8ff3-40ca7f2738a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc734f1-2a46-4402-997f-ee1631ca6210", "node_type": "1", "metadata": {}, "hash": "d9eac80f4d3dfe4d057c5df914f9af51e09fed742707ec3cffba1590fadf0f32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8af5916-e05b-43a3-aef7-c50f5abfe8dd", "node_type": "1", "metadata": {}, "hash": "0d5c8f4204a2cd6e4755cf2f8dfebb38d00785782ae350c835ed3486fa504550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we will explore SQL queries in Spark, focusing on its \nSQL-like querying capabilities and interoperability with the DataFrame API. Examples will illustrate \ncomplex data manipulations and analytics through SQL queries in Spark.\nThis part has the following chapters:\n\u2022\t Chapter 4, Spark DataFrames and their Operations\n\u2022\t Chapter 5, Advanced Operations and Optimizations in Spark\n\u2022\t Chapter 6, SQL Queries in Spark\n\n\nSpark DataFrames \nand their Operations\nIn this chapter, we will learn about a few different APIs in Spark and talk about their features. We will \nalso get started with Spark\u2019s DataFrame operations and look at different data viewing and manipulation \ntechniques such as filtering, adding, renaming, and dropping columns available in Spark.", "mimetype": "text/plain", "start_char_idx": 80124, "end_char_idx": 80891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8af5916-e05b-43a3-aef7-c50f5abfe8dd": {"__data__": {"id_": "d8af5916-e05b-43a3-aef7-c50f5abfe8dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22280c5a-dd62-4469-8ff3-40ca7f2738a9", "node_type": "1", "metadata": {}, "hash": "f06e48217f0f324e6492d352fe1c6a9ffd643e5ca44e3840c41a8377259ed840", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "597869a5-0f27-457e-9e04-0b9792ee348e", "node_type": "1", "metadata": {}, "hash": "d960796e2d9db9fb7f972a1bb7921461786d54e910db6b74cf04c012a55d296e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark DataFrames \nand their Operations\nIn this chapter, we will learn about a few different APIs in Spark and talk about their features. We will \nalso get started with Spark\u2019s DataFrame operations and look at different data viewing and manipulation \ntechniques such as filtering, adding, renaming, and dropping columns available in Spark.\nWe will cover these concepts under the following topics:\n\u2022\t The Spark DataFrame API\n\u2022\t Creating DataFrames\n\u2022\t Viewing DataFrames\n\u2022\t Manipulating DataFrames\n\u2022\t Aggregating DataFrames\nBy the end of this chapter, you will know how to work with PySpark DataFrames. You\u2019ll also discover \nvarious data manipulation techniques and see how you can view data after manipulating it.\nGetting Started in PySpark\nIn the previous chapters, we discussed that Spark primarily uses four languages, which are Scala, Python, \nR, and SQL.", "mimetype": "text/plain", "start_char_idx": 80553, "end_char_idx": 81410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "597869a5-0f27-457e-9e04-0b9792ee348e": {"__data__": {"id_": "597869a5-0f27-457e-9e04-0b9792ee348e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8af5916-e05b-43a3-aef7-c50f5abfe8dd", "node_type": "1", "metadata": {}, "hash": "0d5c8f4204a2cd6e4755cf2f8dfebb38d00785782ae350c835ed3486fa504550", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5305be54-edac-476d-b3bb-37ce1fbf92a6", "node_type": "1", "metadata": {}, "hash": "71578de904899a59703bbcdb345180cdfb21eefbb3dde3d3d37ffdc6896f70f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You\u2019ll also discover \nvarious data manipulation techniques and see how you can view data after manipulating it.\nGetting Started in PySpark\nIn the previous chapters, we discussed that Spark primarily uses four languages, which are Scala, Python, \nR, and SQL. When any of these languages are used, the underlying execution engine is the same. This \nprovides the necessary unification we talked about in Chapter 2. This means that developers can use \nany language of their choice and can also switch between different APIs in applications.\nFor the context of this book, we\u2019re going to focus on Python as the primary language. Spark used with \nPython is called PySpark.\nLet\u2019s get started with the installation of Spark.\n\nSpark DataFrames and their Operations\n\nInstalling Spark\nTo get started with Spark, you would have to first install it on your computer. There are a few ways to \ninstall Spark. We will focus on just one in this section.", "mimetype": "text/plain", "start_char_idx": 81153, "end_char_idx": 82088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5305be54-edac-476d-b3bb-37ce1fbf92a6": {"__data__": {"id_": "5305be54-edac-476d-b3bb-37ce1fbf92a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "597869a5-0f27-457e-9e04-0b9792ee348e", "node_type": "1", "metadata": {}, "hash": "d960796e2d9db9fb7f972a1bb7921461786d54e910db6b74cf04c012a55d296e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3b9173a-e00b-4e15-8ebe-a9722495df0b", "node_type": "1", "metadata": {}, "hash": "715a23d1d8a278d8b0e2cb83660bb4169cd9d8840f9ec6186bda1d0d1ced964b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the context of this book, we\u2019re going to focus on Python as the primary language. Spark used with \nPython is called PySpark.\nLet\u2019s get started with the installation of Spark.\n\nSpark DataFrames and their Operations\n\nInstalling Spark\nTo get started with Spark, you would have to first install it on your computer. There are a few ways to \ninstall Spark. We will focus on just one in this section.\nPySpark provides pip installation from PyPI. You can install it as follows:\npip install pyspark\nOnce Spark is installed, you will need to create a Spark session.\nCreating a Spark session\nOnce you have installed Spark on your system, you can get started with creating a Spark session. \nA Spark session is the entry point of any Spark application.", "mimetype": "text/plain", "start_char_idx": 81690, "end_char_idx": 82434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3b9173a-e00b-4e15-8ebe-a9722495df0b": {"__data__": {"id_": "d3b9173a-e00b-4e15-8ebe-a9722495df0b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5305be54-edac-476d-b3bb-37ce1fbf92a6", "node_type": "1", "metadata": {}, "hash": "71578de904899a59703bbcdb345180cdfb21eefbb3dde3d3d37ffdc6896f70f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b967033a-0a65-42ba-bb95-35facc7eb69a", "node_type": "1", "metadata": {}, "hash": "81a378400b12f1bcc82322150373547ed0011bc457581be274381ac5bc499143", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will focus on just one in this section.\nPySpark provides pip installation from PyPI. You can install it as follows:\npip install pyspark\nOnce Spark is installed, you will need to create a Spark session.\nCreating a Spark session\nOnce you have installed Spark on your system, you can get started with creating a Spark session. \nA Spark session is the entry point of any Spark application. To create a Spark session, you will initialize \nit in the following way:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nWhen you are running your code in the Spark shell, the Spark session is automatically created for \nyou so you don\u2019t have to manually execute this code to create a Spark session. This session is usually \ncreated in a variable called spark.\nIt is important to note that we can only create a single spark session at any given time. Duplicating a \nSpark session is not possible in Spark.", "mimetype": "text/plain", "start_char_idx": 82046, "end_char_idx": 82971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b967033a-0a65-42ba-bb95-35facc7eb69a": {"__data__": {"id_": "b967033a-0a65-42ba-bb95-35facc7eb69a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3b9173a-e00b-4e15-8ebe-a9722495df0b", "node_type": "1", "metadata": {}, "hash": "715a23d1d8a278d8b0e2cb83660bb4169cd9d8840f9ec6186bda1d0d1ced964b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14d6ba39-f710-4c04-b382-741d40c6e92f", "node_type": "1", "metadata": {}, "hash": "758397ae78447e69263eced90105e66057f77530f3fe11daf9f14220b4ccb925", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This session is usually \ncreated in a variable called spark.\nIt is important to note that we can only create a single spark session at any given time. Duplicating a \nSpark session is not possible in Spark.\nNow, let\u2019s take a look at different data APIs in Spark DataFrames.\nDataset API\nDataset is a newer interface added to Spark 1.6. It is a distributed collection of data. The Dataset API \nis available in Java and Scala, but not in Python and R. The Dataset API uses Resilient Distributed \nDatasets (RDDs) and hence provides additional features of RDDs, such as fixed typing. It also uses \nSpark SQL\u2019s optimized engine for faster queries.\nSince a lot of the data engineering and data science community is already familiar with Python and \nuses it extensively for data architectures in production, PySpark also provides an equivalent API for \nDataFrames for this purpose.", "mimetype": "text/plain", "start_char_idx": 82766, "end_char_idx": 83638, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14d6ba39-f710-4c04-b382-741d40c6e92f": {"__data__": {"id_": "14d6ba39-f710-4c04-b382-741d40c6e92f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b967033a-0a65-42ba-bb95-35facc7eb69a", "node_type": "1", "metadata": {}, "hash": "81a378400b12f1bcc82322150373547ed0011bc457581be274381ac5bc499143", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee3bc039-1de2-4475-8e4c-87fdb311afd1", "node_type": "1", "metadata": {}, "hash": "fb1b4703ffc2835a050168b13ea476e114d40e5b856c274b1ebd6548358a4238", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It also uses \nSpark SQL\u2019s optimized engine for faster queries.\nSince a lot of the data engineering and data science community is already familiar with Python and \nuses it extensively for data architectures in production, PySpark also provides an equivalent API for \nDataFrames for this purpose. Let\u2019s take a look at it in the next section.\nDataFrame API\nThe motivation of Spark DataFrames comes from Pandas DataFrames in Python. A DataFrame is \nessentially a set of rows and columns. You can think of it like a table where you have table headers as \ncolumn names and below these headers are data arranged accordingly. This table-like format has been \npart of computations for a long time in tools such as relational databases and comma-separated files.\n\nCreating DataFrame operations\n\nSpark\u2019s DataFrame API is built on top of RDDs. The underlying structures to store the data are still \nRDDs but DataFrames create an abstraction on top of the RDDs to hide its complexity.", "mimetype": "text/plain", "start_char_idx": 83344, "end_char_idx": 84315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee3bc039-1de2-4475-8e4c-87fdb311afd1": {"__data__": {"id_": "ee3bc039-1de2-4475-8e4c-87fdb311afd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14d6ba39-f710-4c04-b382-741d40c6e92f", "node_type": "1", "metadata": {}, "hash": "758397ae78447e69263eced90105e66057f77530f3fe11daf9f14220b4ccb925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0", "node_type": "1", "metadata": {}, "hash": "5041756d1d3bed4f04bcb0ade24e811ebf2b694d87c9fd8fea4e1c0954d6e8fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This table-like format has been \npart of computations for a long time in tools such as relational databases and comma-separated files.\n\nCreating DataFrame operations\n\nSpark\u2019s DataFrame API is built on top of RDDs. The underlying structures to store the data are still \nRDDs but DataFrames create an abstraction on top of the RDDs to hide its complexity. Just as RDDs \nare lazily evaluated and are immutable, DataFrames are also evaluated lazily and are immutable. If you \ncan remember from previous chapters, lazy evaluation gives Spark performance gains and optimization \nby running the computations only when needed. This also gives Spark a large number of optimizations \nin its DataFrames by planning how to best compute the operations. The computations start when an \naction is called on a DataFrame. There are a lot of different ways to create Spark DataFrames. We will \nlearn about some of those in this chapter.", "mimetype": "text/plain", "start_char_idx": 83962, "end_char_idx": 84880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0": {"__data__": {"id_": "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee3bc039-1de2-4475-8e4c-87fdb311afd1", "node_type": "1", "metadata": {}, "hash": "fb1b4703ffc2835a050168b13ea476e114d40e5b856c274b1ebd6548358a4238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff9f6d4e-5e6d-44b9-802e-42168c61411d", "node_type": "1", "metadata": {}, "hash": "cf12e02f5546ed4fd0e91b12e5573e968823bd0ac9edfda02354b60b5f428c81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This also gives Spark a large number of optimizations \nin its DataFrames by planning how to best compute the operations. The computations start when an \naction is called on a DataFrame. There are a lot of different ways to create Spark DataFrames. We will \nlearn about some of those in this chapter.\nLet\u2019s take a look at what a DataFrame is in Spark.\nCreating DataFrame operations\nAs we have already discussed, DataFrames are the main building blocks of Spark data. They consist \nof rows and column data structures.\nDataFrames in PySpark are created using the pyspark.sql.SparkSession.createDataFrame \nfunction. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames, RDDs, and pyspark.\nsql.Rows to create DataFrames.\nSpark DataFrames also has an argument named schema that specifies the schema of the DataFrame.", "mimetype": "text/plain", "start_char_idx": 84581, "end_char_idx": 85413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff9f6d4e-5e6d-44b9-802e-42168c61411d": {"__data__": {"id_": "ff9f6d4e-5e6d-44b9-802e-42168c61411d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0", "node_type": "1", "metadata": {}, "hash": "5041756d1d3bed4f04bcb0ade24e811ebf2b694d87c9fd8fea4e1c0954d6e8fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3", "node_type": "1", "metadata": {}, "hash": "21bd90bf8ca53875d6871534b057c566cbb86c8364e1679eb8f279891e23f2f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They consist \nof rows and column data structures.\nDataFrames in PySpark are created using the pyspark.sql.SparkSession.createDataFrame \nfunction. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames, RDDs, and pyspark.\nsql.Rows to create DataFrames.\nSpark DataFrames also has an argument named schema that specifies the schema of the DataFrame. \nYou can either choose to specify the schema explicitly or let Spark infer the schema from the DataFrame \nitself. If you don\u2019t specify this argument in the code, Spark will infer the schema on its own.\nThere are different ways to create DataFrames in Spark. Some of them are explained in the \nfollowing sections.\nUsing a list of rows\nThe first way to create DataFrames we see is by using rows of data. You can think of rows of data as \nlists. They would share common header values for each of the values in the list.", "mimetype": "text/plain", "start_char_idx": 85047, "end_char_idx": 85930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3": {"__data__": {"id_": "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff9f6d4e-5e6d-44b9-802e-42168c61411d", "node_type": "1", "metadata": {}, "hash": "cf12e02f5546ed4fd0e91b12e5573e968823bd0ac9edfda02354b60b5f428c81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4b4b28e-143c-4f39-b291-0cbbe163dc3f", "node_type": "1", "metadata": {}, "hash": "e7afd65f559caf4de8000aa618a45175011d0e894c3a37f003dd10fb092672e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are different ways to create DataFrames in Spark. Some of them are explained in the \nfollowing sections.\nUsing a list of rows\nThe first way to create DataFrames we see is by using rows of data. You can think of rows of data as \nlists. They would share common header values for each of the values in the list.\nHere\u2019s the code to use when creating a new DataFrame using rows of data:\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\ndata_df = spark.createDataFrame([\nRow(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, \n1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\nRow(col_1=200, col_2=300., col_3='string_test_2',", "mimetype": "text/plain", "start_char_idx": 85616, "end_char_idx": 86282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4b4b28e-143c-4f39-b291-0cbbe163dc3f": {"__data__": {"id_": "f4b4b28e-143c-4f39-b291-0cbbe163dc3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3", "node_type": "1", "metadata": {}, "hash": "21bd90bf8ca53875d6871534b057c566cbb86c8364e1679eb8f279891e23f2f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fadc2540-05d9-4332-9ef0-d8a2edfd32e3", "node_type": "1", "metadata": {}, "hash": "c328cfe6ec97488049679f49b43f78588599cbcaa59797baf379a6ff55e0bdcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "createDataFrame([\nRow(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, \n1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\nRow(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, \n2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\nRow(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, \n3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n])\n\nSpark DataFrames and their Operations\n\nAs a result,", "mimetype": "text/plain", "start_char_idx": 86104, "end_char_idx": 86509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fadc2540-05d9-4332-9ef0-d8a2edfd32e3": {"__data__": {"id_": "fadc2540-05d9-4332-9ef0-d8a2edfd32e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4b4b28e-143c-4f39-b291-0cbbe163dc3f", "node_type": "1", "metadata": {}, "hash": "e7afd65f559caf4de8000aa618a45175011d0e894c3a37f003dd10fb092672e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad5ec67d-3e88-4584-9be9-099f20811dde", "node_type": "1", "metadata": {}, "hash": "c69b805df9f71d96d33d4b48ba37aed235825a139964f431595027fd33bf31de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "col_5=datetime(2023, 1, 2, 12, 0)),\nRow(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, \n3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n])\n\nSpark DataFrames and their Operations\n\nAs a result, you will see a DataFrame with our specified columns and their data types:\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, \ncol_5: timestamp]\nNow, we\u2019ll see how we can specify the schema for a Spark DataFrame explicitly.\nUsing a list of rows with schema\nThe schema of a DataFrame defines what would be the different data types present in each of the rows \nand columns of a DataFrame.", "mimetype": "text/plain", "start_char_idx": 86308, "end_char_idx": 86912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad5ec67d-3e88-4584-9be9-099f20811dde": {"__data__": {"id_": "ad5ec67d-3e88-4584-9be9-099f20811dde", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fadc2540-05d9-4332-9ef0-d8a2edfd32e3", "node_type": "1", "metadata": {}, "hash": "c328cfe6ec97488049679f49b43f78588599cbcaa59797baf379a6ff55e0bdcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2909a32-6aba-48fc-aae9-12e5e4a339e7", "node_type": "1", "metadata": {}, "hash": "f21d8489f33f98b162038bef0fbe9f89500762f7bbbb9a214bfcbefa7e036da3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "col_2: double, col_3: string, col_4: date, \ncol_5: timestamp]\nNow, we\u2019ll see how we can specify the schema for a Spark DataFrame explicitly.\nUsing a list of rows with schema\nThe schema of a DataFrame defines what would be the different data types present in each of the rows \nand columns of a DataFrame. Explicitly defining schema helps in cases where we want to enforce \ncertain data types to our datasets.\nNow, we will explicitly tell Spark which schema to use for the DataFrame that we\u2019re creating. Notice \nthat the majority of the code remains the same\u2014we\u2019re simply adding another argument named \nschema in the code for creating the DataFrame to explicitly tell which columns would have what \nkind of datatypes:\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\ndata_df = spark.createDataFrame([\nRow(col_1=100,", "mimetype": "text/plain", "start_char_idx": 86609, "end_char_idx": 87457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2909a32-6aba-48fc-aae9-12e5e4a339e7": {"__data__": {"id_": "d2909a32-6aba-48fc-aae9-12e5e4a339e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad5ec67d-3e88-4584-9be9-099f20811dde", "node_type": "1", "metadata": {}, "hash": "c69b805df9f71d96d33d4b48ba37aed235825a139964f431595027fd33bf31de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "419ff4c2-100f-461d-b09c-49d92f3e86d2", "node_type": "1", "metadata": {}, "hash": "147bb75b83ad0ad6574595381af437cb07edf12a5cf365325344f81020c75681", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notice \nthat the majority of the code remains the same\u2014we\u2019re simply adding another argument named \nschema in the code for creating the DataFrame to explicitly tell which columns would have what \nkind of datatypes:\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\ndata_df = spark.createDataFrame([\nRow(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, \n1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\nRow(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, \n2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\nRow(col_1=400, col_2=500.,", "mimetype": "text/plain", "start_char_idx": 87111, "end_char_idx": 87691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "419ff4c2-100f-461d-b09c-49d92f3e86d2": {"__data__": {"id_": "419ff4c2-100f-461d-b09c-49d92f3e86d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2909a32-6aba-48fc-aae9-12e5e4a339e7", "node_type": "1", "metadata": {}, "hash": "f21d8489f33f98b162038bef0fbe9f89500762f7bbbb9a214bfcbefa7e036da3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a284450-61e3-41c9-a5f5-7d7db3173362", "node_type": "1", "metadata": {}, "hash": "da20819100c7402db1cb241cefada62ab89a970d9d5454c3ca5870765baf9400", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, 1, 12, 0)),\nRow(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, \n2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\nRow(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, \n3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 \ntimestamp')\nAs a result, you will see a DataFrame with our specified columns and their data types:\ndata_df\nDataFrame[col_1: bigint, col_2: double, col_3: string,", "mimetype": "text/plain", "start_char_idx": 87539, "end_char_idx": 88007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a284450-61e3-41c9-a5f5-7d7db3173362": {"__data__": {"id_": "8a284450-61e3-41c9-a5f5-7d7db3173362", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "419ff4c2-100f-461d-b09c-49d92f3e86d2", "node_type": "1", "metadata": {}, "hash": "147bb75b83ad0ad6574595381af437cb07edf12a5cf365325344f81020c75681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24619ca6-683b-40ed-bb32-292688ae6e01", "node_type": "1", "metadata": {}, "hash": "0a6ea1cd32f9acb33a31ddfd32cf917320b7d30da781e5d17b19182fd63ce52b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "col_5=datetime(2023, 1, 3, 12, 0))\n], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 \ntimestamp')\nAs a result, you will see a DataFrame with our specified columns and their data types:\ndata_df\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, \ncol_5: timestamp]\nNow, let\u2019s take a look at how we can create DataFrames using Pandas DataFrames.\nUsing Pandas DataFrames\nDataFrames can also be created using Pandas DataFrames. To achieve this, you would need to create a \nDataFrame in Pandas first. Once that is created, you would then convert that DataFrame to a PySpark \nDataFrame. The following code demonstrates this process:\n\nCreating DataFrame operations\n\nfrom datetime import datetime,", "mimetype": "text/plain", "start_char_idx": 87740, "end_char_idx": 88465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24619ca6-683b-40ed-bb32-292688ae6e01": {"__data__": {"id_": "24619ca6-683b-40ed-bb32-292688ae6e01", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a284450-61e3-41c9-a5f5-7d7db3173362", "node_type": "1", "metadata": {}, "hash": "da20819100c7402db1cb241cefada62ab89a970d9d5454c3ca5870765baf9400", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1decaea8-06b5-4566-be70-a1e74bf48e42", "node_type": "1", "metadata": {}, "hash": "1d24bfbd684677b89f0b2481963c2e3f62b0e05f4275602e26f345a6215f9f9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using Pandas DataFrames\nDataFrames can also be created using Pandas DataFrames. To achieve this, you would need to create a \nDataFrame in Pandas first. Once that is created, you would then convert that DataFrame to a PySpark \nDataFrame. The following code demonstrates this process:\n\nCreating DataFrame operations\n\nfrom datetime import datetime, date\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nrdd = spark.sparkContext.parallelize([\n(100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, \n1, 12, 0)),\n(200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, \n2, 12, 0)),\n(300, 400.,", "mimetype": "text/plain", "start_char_idx": 88120, "end_char_idx": 88757, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1decaea8-06b5-4566-be70-a1e74bf48e42": {"__data__": {"id_": "1decaea8-06b5-4566-be70-a1e74bf48e42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24619ca6-683b-40ed-bb32-292688ae6e01", "node_type": "1", "metadata": {}, "hash": "0a6ea1cd32f9acb33a31ddfd32cf917320b7d30da781e5d17b19182fd63ce52b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1dba6fb-db28-409f-b136-4c523f2861f9", "node_type": "1", "metadata": {}, "hash": "52c6a96cf5a01dfa770294ce6046c7b03ccac9bacbaa11ce0ded781708238cc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, \n1, 12, 0)),\n(200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, \n2, 12, 0)),\n(300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, \n3, 12, 0))\n])\ndata_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', \n'col_3', 'col_4', 'col_5'])\nAs a result, you will see a DataFrame with our specified columns and their data types:\nDataFrame[col_1: bigint, col_2: double, col_3: string,", "mimetype": "text/plain", "start_char_idx": 88596, "end_char_idx": 89059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1dba6fb-db28-409f-b136-4c523f2861f9": {"__data__": {"id_": "a1dba6fb-db28-409f-b136-4c523f2861f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1decaea8-06b5-4566-be70-a1e74bf48e42", "node_type": "1", "metadata": {}, "hash": "1d24bfbd684677b89f0b2481963c2e3f62b0e05f4275602e26f345a6215f9f9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c48c526e-d899-4330-8526-d2092bb7224a", "node_type": "1", "metadata": {}, "hash": "4a9e5d9f71786cf9c248ee1624c324b83b9924ab5b9c370ec6de42a47829ebbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, \n3, 12, 0))\n])\ndata_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', \n'col_3', 'col_4', 'col_5'])\nAs a result, you will see a DataFrame with our specified columns and their data types:\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, \ncol_5: timestamp]\nNow, let\u2019s take a look at how we can create DataFrames using tuples.\nUsing tuples\nAnother way to create DataFrames is through tuples. This means that we can create a tuple as a row \nand add each tuple as a separate row in the DataFrame. Each tuple would contain the data for each \nof the columns of the DataFrame. The following code demonstrates this:\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.", "mimetype": "text/plain", "start_char_idx": 88808, "end_char_idx": 89513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c48c526e-d899-4330-8526-d2092bb7224a": {"__data__": {"id_": "c48c526e-d899-4330-8526-d2092bb7224a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1dba6fb-db28-409f-b136-4c523f2861f9", "node_type": "1", "metadata": {}, "hash": "52c6a96cf5a01dfa770294ce6046c7b03ccac9bacbaa11ce0ded781708238cc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e374941-a670-48ec-80df-6a23e3195988", "node_type": "1", "metadata": {}, "hash": "4cb2ee067e0e7f6daf047acd9e80bd61ca1c4e2224f92a38d8b85e47f3037d72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using tuples\nAnother way to create DataFrames is through tuples. This means that we can create a tuple as a row \nand add each tuple as a separate row in the DataFrame. Each tuple would contain the data for each \nof the columns of the DataFrame. The following code demonstrates this:\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\nrdd = spark.sparkContext.parallelize([\n(100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, \n1, 12, 0)),\n(200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, \n2, 12, 0)),\n(300, 400., 'string_test_3', date(2023, 3,", "mimetype": "text/plain", "start_char_idx": 89161, "end_char_idx": 89765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e374941-a670-48ec-80df-6a23e3195988": {"__data__": {"id_": "8e374941-a670-48ec-80df-6a23e3195988", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c48c526e-d899-4330-8526-d2092bb7224a", "node_type": "1", "metadata": {}, "hash": "4a9e5d9f71786cf9c248ee1624c324b83b9924ab5b9c370ec6de42a47829ebbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e503861-ceec-4d66-b756-c7e683b42921", "node_type": "1", "metadata": {}, "hash": "60a7c94415223af01586712d02ba2222caa8b238457701d7c949650e3ed9e782", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, 1), datetime(2023, 1, \n1, 12, 0)),\n(200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, \n2, 12, 0)),\n(300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, \n3, 12, 0))\n])\ndata_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', \n'col_3', 'col_4', 'col_5'])\n\nSpark DataFrames and their Operations\n\nAs a result, you will see a DataFrame with our specified columns and their data types:\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date,", "mimetype": "text/plain", "start_char_idx": 89607, "end_char_idx": 90089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e503861-ceec-4d66-b756-c7e683b42921": {"__data__": {"id_": "3e503861-ceec-4d66-b756-c7e683b42921", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e374941-a670-48ec-80df-6a23e3195988", "node_type": "1", "metadata": {}, "hash": "4cb2ee067e0e7f6daf047acd9e80bd61ca1c4e2224f92a38d8b85e47f3037d72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "360f4755-ca31-4e83-8d8e-4f95e708e11e", "node_type": "1", "metadata": {}, "hash": "444af46ae5d3c41f50cf61bc3ff8e65168ab7f358b371dca99342a3b0d0817e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "createDataFrame(rdd, schema=['col_1', 'col_2', \n'col_3', 'col_4', 'col_5'])\n\nSpark DataFrames and their Operations\n\nAs a result, you will see a DataFrame with our specified columns and their data types:\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, \ncol_5: timestamp]\nNow, let\u2019s take a look at different ways we can view the DataFrames in Spark and see the results of the \nDataFrames that we just created.\nHow to view the DataFrames\nThere are different statements in Spark to view data. The DataFrames that we created in the previous \nsection through different methods all yield the same result as the DataFrame. Let\u2019s look at a few \ndifferent ways to view DataFrames.\nViewing DataFrames\nThe first way to show a DataFrame is through the DataFrame.show() statement.", "mimetype": "text/plain", "start_char_idx": 89819, "end_char_idx": 90605, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "360f4755-ca31-4e83-8d8e-4f95e708e11e": {"__data__": {"id_": "360f4755-ca31-4e83-8d8e-4f95e708e11e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e503861-ceec-4d66-b756-c7e683b42921", "node_type": "1", "metadata": {}, "hash": "60a7c94415223af01586712d02ba2222caa8b238457701d7c949650e3ed9e782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7e7eb72-b500-4003-baeb-0161f13c7965", "node_type": "1", "metadata": {}, "hash": "9c5b7b23b4a337809d605eaafd9228381d6b51b02789630be830303c5db3cba7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How to view the DataFrames\nThere are different statements in Spark to view data. The DataFrames that we created in the previous \nsection through different methods all yield the same result as the DataFrame. Let\u2019s look at a few \ndifferent ways to view DataFrames.\nViewing DataFrames\nThe first way to show a DataFrame is through the DataFrame.show() statement. Here\u2019s an example:\ndata_df.show()\nAs a result, you will see a DataFrame with our specified columns and the data inside this DataFrame:\n+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|col_3|col_4|col_5|\n+-----+-----+-------------+----------+-------------------+\n|100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n|300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|\n+-----+-----+-------------+----------+-------------------+\nWe can also select the total rows that can be viewed in a single statement.", "mimetype": "text/plain", "start_char_idx": 90247, "end_char_idx": 91196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7e7eb72-b500-4003-baeb-0161f13c7965": {"__data__": {"id_": "f7e7eb72-b500-4003-baeb-0161f13c7965", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "360f4755-ca31-4e83-8d8e-4f95e708e11e", "node_type": "1", "metadata": {}, "hash": "444af46ae5d3c41f50cf61bc3ff8e65168ab7f358b371dca99342a3b0d0817e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c18b7eba-b3ff-468b-88a7-3dd7d5959a98", "node_type": "1", "metadata": {}, "hash": "bfdcc4ab30b1e0bbc003c298245ce0ddeaccb27244e9f9f83e88dc27fdaaeb6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s see how we can do \nthat in the next topic.\nViewing top n rows\nWe can also be selective in the number of rows that can be viewed in a single statement. We can \ncontrol that using a parameter in DataFrame.show(). Here\u2019s an example of looking at only the \ntop two rows of the DataFrame.\nIf you specify n to be a specific number, then only those sets of rows would be shown.", "mimetype": "text/plain", "start_char_idx": 91197, "end_char_idx": 91573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c18b7eba-b3ff-468b-88a7-3dd7d5959a98": {"__data__": {"id_": "c18b7eba-b3ff-468b-88a7-3dd7d5959a98", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7e7eb72-b500-4003-baeb-0161f13c7965", "node_type": "1", "metadata": {}, "hash": "9c5b7b23b4a337809d605eaafd9228381d6b51b02789630be830303c5db3cba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "938f022d-1d5b-4d15-9096-ab57320bed3f", "node_type": "1", "metadata": {}, "hash": "706f55061fe167c14185a1311bf34049b3d139d3603c342ce6c6de1ebbe66a2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Viewing top n rows\nWe can also be selective in the number of rows that can be viewed in a single statement. We can \ncontrol that using a parameter in DataFrame.show(). Here\u2019s an example of looking at only the \ntop two rows of the DataFrame.\nIf you specify n to be a specific number, then only those sets of rows would be shown. Here\u2019s an example:\ndata_df.show(2)\n\nHow to view the DataFrames\n\nAs a result, you will see a DataFrame with its top two rows:\n+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|col_3|col_4|col_5|\n+------+------+-----------+----------+-------------------+\n|100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n------+-----+-------------+----------+-------------------+\nonly showing top 2 rows.", "mimetype": "text/plain", "start_char_idx": 91246, "end_char_idx": 92046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "938f022d-1d5b-4d15-9096-ab57320bed3f": {"__data__": {"id_": "938f022d-1d5b-4d15-9096-ab57320bed3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c18b7eba-b3ff-468b-88a7-3dd7d5959a98", "node_type": "1", "metadata": {}, "hash": "bfdcc4ab30b1e0bbc003c298245ce0ddeaccb27244e9f9f83e88dc27fdaaeb6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9810f677-9bff-476d-8b03-c7717759f185", "node_type": "1", "metadata": {}, "hash": "468f004d981603398280e95ed4a3fc2e91bdfac613015038cb7c4148f82a20bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Viewing DataFrame schema\nWe can also choose to see the schema of the DataFrame using the printSchema() function:\ndata_df.printSchema()\nAs a result, you will see the schema of a DataFrame with our specified columns and their data types:\nroot\n |-- col_1: long (nullable = true)\n |-- col_2: double (nullable = true)\n |-- col_3: string (nullable = true)\n |-- col_4: date (nullable = true)\n |-- col_5: timestamp (nullable = true)\nViewing data vertically\nWhen the data becomes too long to fit into the screen, it\u2019s sometimes useful to see the data in a \nvertical format instead of a horizontal table view. Here\u2019s an example of how you can view the data in \na vertical format:\ndata_df.show(1, vertical=True)\nAs a result,", "mimetype": "text/plain", "start_char_idx": 92047, "end_char_idx": 92760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9810f677-9bff-476d-8b03-c7717759f185": {"__data__": {"id_": "9810f677-9bff-476d-8b03-c7717759f185", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "938f022d-1d5b-4d15-9096-ab57320bed3f", "node_type": "1", "metadata": {}, "hash": "706f55061fe167c14185a1311bf34049b3d139d3603c342ce6c6de1ebbe66a2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "690e4acf-bdef-438a-8a32-2852a380eec5", "node_type": "1", "metadata": {}, "hash": "2d3f0d8da4db353701a56b27741331e393920935bbffe404169c68811f7f5fee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here\u2019s an example of how you can view the data in \na vertical format:\ndata_df.show(1, vertical=True)\nAs a result, you will see a DataFrame with our specified columns and their data but in a vertical format:\n-RECORD 0------------------\n col_1| 100\n col_2| 200.0\n col_3| string_test_1\n col_4| 2023-01-01\n col_5| 2023-01-01 12:00:00\nonly showing top 1 row\n\nSpark DataFrames and their Operations\n\nViewing columns of data\nWhen we just need to view the columns that exist in a DataFrame, we would use the following:\ndata_df.columns\nAs a result, you will see a list of the columns in the DataFrame:\n['col_1', 'col_2', 'col_3', 'col_4',", "mimetype": "text/plain", "start_char_idx": 92647, "end_char_idx": 93275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "690e4acf-bdef-438a-8a32-2852a380eec5": {"__data__": {"id_": "690e4acf-bdef-438a-8a32-2852a380eec5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9810f677-9bff-476d-8b03-c7717759f185", "node_type": "1", "metadata": {}, "hash": "468f004d981603398280e95ed4a3fc2e91bdfac613015038cb7c4148f82a20bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4efb1a4-0128-4c52-993b-7f0e3c0fced0", "node_type": "1", "metadata": {}, "hash": "4931a803b557adb1593c156602bdb974639ec205ecfa211de7991229df63c43c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we would use the following:\ndata_df.columns\nAs a result, you will see a list of the columns in the DataFrame:\n['col_1', 'col_2', 'col_3', 'col_4', 'col_5']\nViewing summary statistics\nNow, let\u2019s take a look at how we can view the summary statistics of a DataFrame:\nShow the summary of the DataFrame\ndata_df.select('col_1', 'col_2', 'col_3').describe().show()\nAs a result, you will see a DataFrame with its summary statistics for each column defined:\n+-------+-------+-------+-------------+\n|summary|col_1|col_2|col_3|\n+-------+-------+-------+-------------+\n|count|3|3|3|\n|mean| 200.0 | 300.0 |null|\n| stddev| 100.0 | 100.", "mimetype": "text/plain", "start_char_idx": 93129, "end_char_idx": 93750, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4efb1a4-0128-4c52-993b-7f0e3c0fced0": {"__data__": {"id_": "d4efb1a4-0128-4c52-993b-7f0e3c0fced0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "690e4acf-bdef-438a-8a32-2852a380eec5", "node_type": "1", "metadata": {}, "hash": "2d3f0d8da4db353701a56b27741331e393920935bbffe404169c68811f7f5fee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be79215a-2e71-486f-b310-529a9ef91973", "node_type": "1", "metadata": {}, "hash": "9aa01ef7be265b63906003169ce62ca32ace350fd5b7375da211714d19e00689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "describe().show()\nAs a result, you will see a DataFrame with its summary statistics for each column defined:\n+-------+-------+-------+-------------+\n|summary|col_1|col_2|col_3|\n+-------+-------+-------+-------------+\n|count|3|3|3|\n|mean| 200.0 | 300.0 |null|\n| stddev| 100.0 | 100.0 |null|\n|min| 100| 200.0 |string_test_1|\n|max| 300| 400.0 |string_test_3|\n+-------+-------+-------+-------------+\nNow, let\u2019s take a look at the collect statement.\nCollecting the data\nA collect statement is used when we want to get all the data that is being processed in different clusters \nback to the driver.", "mimetype": "text/plain", "start_char_idx": 93469, "end_char_idx": 94061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be79215a-2e71-486f-b310-529a9ef91973": {"__data__": {"id_": "be79215a-2e71-486f-b310-529a9ef91973", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4efb1a4-0128-4c52-993b-7f0e3c0fced0", "node_type": "1", "metadata": {}, "hash": "4931a803b557adb1593c156602bdb974639ec205ecfa211de7991229df63c43c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "023eff69-4e7e-41c2-b396-50cb5a708842", "node_type": "1", "metadata": {}, "hash": "cbfc1fd5bfec1aa00a5656b9f1a406a22d84fcfd5d92b7171c251dd4113d1bdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 |null|\n|min| 100| 200.0 |string_test_1|\n|max| 300| 400.0 |string_test_3|\n+-------+-------+-------+-------------+\nNow, let\u2019s take a look at the collect statement.\nCollecting the data\nA collect statement is used when we want to get all the data that is being processed in different clusters \nback to the driver. When using a collect statement, we need to make sure that the driver has enough \nmemory to hold the processed data. If the driver doesn\u2019t have enough memory to hold the data, we \nwill get out-of-memory errors.\nThis is how you show the collect statement:\ndata_df.collect()\nThis statement will then show result as follows:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.", "mimetype": "text/plain", "start_char_idx": 93750, "end_char_idx": 94450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "023eff69-4e7e-41c2-b396-50cb5a708842": {"__data__": {"id_": "023eff69-4e7e-41c2-b396-50cb5a708842", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be79215a-2e71-486f-b310-529a9ef91973", "node_type": "1", "metadata": {}, "hash": "9aa01ef7be265b63906003169ce62ca32ace350fd5b7375da211714d19e00689", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ec356b5-477a-4745-ba6e-88e5ae8c4627", "node_type": "1", "metadata": {}, "hash": "72b233672011825e0ca3a612f3a144c3153bd47a7218ade97774f9142a8dc92d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the driver doesn\u2019t have enough memory to hold the data, we \nwill get out-of-memory errors.\nThis is how you show the collect statement:\ndata_df.collect()\nThis statement will then show result as follows:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.\ndate(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),\n\nCollecting the data\n\n Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.", "mimetype": "text/plain", "start_char_idx": 94178, "end_char_idx": 94603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ec356b5-477a-4745-ba6e-88e5ae8c4627": {"__data__": {"id_": "3ec356b5-477a-4745-ba6e-88e5ae8c4627", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "023eff69-4e7e-41c2-b396-50cb5a708842", "node_type": "1", "metadata": {}, "hash": "cbfc1fd5bfec1aa00a5656b9f1a406a22d84fcfd5d92b7171c251dd4113d1bdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc7742f7-e13f-434b-96ba-66b8bb2ad86b", "node_type": "1", "metadata": {}, "hash": "1416c80a1652539e7fbabc38ad43affdb3a1f1a3b932fb0e9ec141cb8e882d83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),\n\nCollecting the data\n\n Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.\ndate(2023, 2, 1), col_5=datetime.datetime(2023, 1, 2, 12, 0)),\n Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.\ndate(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\nThere are a few ways to avoid out-of-memory errors. We will explore some of the options that avoid \nout-of-memory errors such as take, tail, and head statements.", "mimetype": "text/plain", "start_char_idx": 94451, "end_char_idx": 94959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc7742f7-e13f-434b-96ba-66b8bb2ad86b": {"__data__": {"id_": "fc7742f7-e13f-434b-96ba-66b8bb2ad86b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ec356b5-477a-4745-ba6e-88e5ae8c4627", "node_type": "1", "metadata": {}, "hash": "72b233672011825e0ca3a612f3a144c3153bd47a7218ade97774f9142a8dc92d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55f1ddd3-a23b-47bb-8d45-85ef1ee68645", "node_type": "1", "metadata": {}, "hash": "9aa8b0fb13ee3dcba5c5d87261f866f235a02dc3896f5d6ae3f1687597e84146", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\nThere are a few ways to avoid out-of-memory errors. We will explore some of the options that avoid \nout-of-memory errors such as take, tail, and head statements. These statements return only a subset \nof the data and not all of the data in a DataFrame, therefore, they are very useful to inspect the data \nwithout having to lead all the data in driver memory.\nNow, let\u2019s take a look at the take statement.\nUsing take\nA take statement takes an argument for a number of elements to return from the top of a DataFrame.", "mimetype": "text/plain", "start_char_idx": 94735, "end_char_idx": 95313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55f1ddd3-a23b-47bb-8d45-85ef1ee68645": {"__data__": {"id_": "55f1ddd3-a23b-47bb-8d45-85ef1ee68645", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc7742f7-e13f-434b-96ba-66b8bb2ad86b", "node_type": "1", "metadata": {}, "hash": "1416c80a1652539e7fbabc38ad43affdb3a1f1a3b932fb0e9ec141cb8e882d83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e8b3264-ab20-490b-b14c-c89aa4be3022", "node_type": "1", "metadata": {}, "hash": "67a2f800c992ef750e85c25e10ebe76f109f7f15116eb1cc9f02c56324aa2ade", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These statements return only a subset \nof the data and not all of the data in a DataFrame, therefore, they are very useful to inspect the data \nwithout having to lead all the data in driver memory.\nNow, let\u2019s take a look at the take statement.\nUsing take\nA take statement takes an argument for a number of elements to return from the top of a DataFrame. \nWe will see how it is used in the following code example:\ndata_df.take(1)\nAs a result, you will see a DataFrame with its top row:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.", "mimetype": "text/plain", "start_char_idx": 94960, "end_char_idx": 95512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e8b3264-ab20-490b-b14c-c89aa4be3022": {"__data__": {"id_": "7e8b3264-ab20-490b-b14c-c89aa4be3022", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55f1ddd3-a23b-47bb-8d45-85ef1ee68645", "node_type": "1", "metadata": {}, "hash": "9aa8b0fb13ee3dcba5c5d87261f866f235a02dc3896f5d6ae3f1687597e84146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c139b91f-c56b-4745-9d0a-bdd63c9f44c6", "node_type": "1", "metadata": {}, "hash": "80f4436a0946287a712d0c43af7bce783e26b020c1d5b8a4e232d7ee7f8a5253", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using take\nA take statement takes an argument for a number of elements to return from the top of a DataFrame. \nWe will see how it is used in the following code example:\ndata_df.take(1)\nAs a result, you will see a DataFrame with its top row:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.\ndate(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]\nIn this example, we\u2019re only returning the first element of the DataFrame by giving 1 as an argument \nvalue to the take() function. Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at the tail statement.\nUsing tail\nThe tail statement takes an argument for a number of elements to return from the bottom of a \nDataFrame.", "mimetype": "text/plain", "start_char_idx": 95204, "end_char_idx": 95919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c139b91f-c56b-4745-9d0a-bdd63c9f44c6": {"__data__": {"id_": "c139b91f-c56b-4745-9d0a-bdd63c9f44c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e8b3264-ab20-490b-b14c-c89aa4be3022", "node_type": "1", "metadata": {}, "hash": "67a2f800c992ef750e85c25e10ebe76f109f7f15116eb1cc9f02c56324aa2ade", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1233690-dd7a-45b4-9f14-6f108b61db4e", "node_type": "1", "metadata": {}, "hash": "4a0e904c16e327ff218f432fd788243c6a88d0dbada20ce6b3e81026601a9d89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at the tail statement.\nUsing tail\nThe tail statement takes an argument for a number of elements to return from the bottom of a \nDataFrame. We will see how it is used in the following code example:\ndata_df.tail(1)\nAs a result, you will see a DataFrame with its last row of data:\n[Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.\ndate(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\nIn this example, we\u2019re only returning the last element of the DataFrame by giving 1 as an argument \nvalue to the tail() function. Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at the head statement.", "mimetype": "text/plain", "start_char_idx": 95707, "end_char_idx": 96416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1233690-dd7a-45b4-9f14-6f108b61db4e": {"__data__": {"id_": "a1233690-dd7a-45b4-9f14-6f108b61db4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c139b91f-c56b-4745-9d0a-bdd63c9f44c6", "node_type": "1", "metadata": {}, "hash": "80f4436a0946287a712d0c43af7bce783e26b020c1d5b8a4e232d7ee7f8a5253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67263779-31fd-46ae-862e-7c83bcae98bd", "node_type": "1", "metadata": {}, "hash": "f6fcf239abc30bce475a0e09ae6235c3ec58319fed8a4a69b18effb77a1656a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\nIn this example, we\u2019re only returning the last element of the DataFrame by giving 1 as an argument \nvalue to the tail() function. Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at the head statement.\n\nSpark DataFrames and their Operations\n\nUsing head\nThe head statement takes an argument for a number of elements to return from the top of a DataFrame. \nWe will see how it is used in the following code example:\ndata_df.head(1)\nAs a result, you will see a DataFrame with its top row of data:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.", "mimetype": "text/plain", "start_char_idx": 96127, "end_char_idx": 96775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67263779-31fd-46ae-862e-7c83bcae98bd": {"__data__": {"id_": "67263779-31fd-46ae-862e-7c83bcae98bd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1233690-dd7a-45b4-9f14-6f108b61db4e", "node_type": "1", "metadata": {}, "hash": "4a0e904c16e327ff218f432fd788243c6a88d0dbada20ce6b3e81026601a9d89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e427c614-9315-4228-bea1-d8f0459d6009", "node_type": "1", "metadata": {}, "hash": "35a4a5e9f3dc039013bced5c5293d77cc2c28388fc9a80603b302f1511560550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will see how it is used in the following code example:\ndata_df.head(1)\nAs a result, you will see a DataFrame with its top row of data:\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.\ndate(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]\nIn this example, we\u2019re only returning the first element of the DataFrame by giving 1 as an argument \nvalue to the head() function. Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at how we can count the number of rows in a DataFrame.", "mimetype": "text/plain", "start_char_idx": 96570, "end_char_idx": 97098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e427c614-9315-4228-bea1-d8f0459d6009": {"__data__": {"id_": "e427c614-9315-4228-bea1-d8f0459d6009", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67263779-31fd-46ae-862e-7c83bcae98bd", "node_type": "1", "metadata": {}, "hash": "f6fcf239abc30bce475a0e09ae6235c3ec58319fed8a4a69b18effb77a1656a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01d02fa3-cfb1-40e2-88cc-801891efe245", "node_type": "1", "metadata": {}, "hash": "d3b718f4deb5a67cae825b05cb00e71fb5dd6e3109eb5fb5f7af3fe72fad45fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, only one row is returned in the result.\nNow, let\u2019s take a look at how we can count the number of rows in a DataFrame.\nCounting the number of rows of data\nWhen we just need to count the number of rows in a DataFrame, we would use the following:\ndata_df.count()\nAs a result, you will see the total count of rows in a DataFrame:\n\nIn PySpark, several methods are available for retrieving data from a DataFrame or RDD, each with \nits own characteristics and use cases. Here\u2019s a summary of the major differences between take, collect, \nshow, head, and tail that we used earlier in this section for data retrieval.\ntake(n)\nThis function returns an array containing the first n elements from the DataFrame or RDD\n\u2022\t It is useful for quickly inspecting a small subset of the data\n\u2022\t It performs a lazy evaluation,", "mimetype": "text/plain", "start_char_idx": 96970, "end_char_idx": 97785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01d02fa3-cfb1-40e2-88cc-801891efe245": {"__data__": {"id_": "01d02fa3-cfb1-40e2-88cc-801891efe245", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e427c614-9315-4228-bea1-d8f0459d6009", "node_type": "1", "metadata": {}, "hash": "35a4a5e9f3dc039013bced5c5293d77cc2c28388fc9a80603b302f1511560550", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3031ccef-e341-488b-a3f2-80fc1d2718b4", "node_type": "1", "metadata": {}, "hash": "c511f11122f7c8f7a6f082b78f6ff24d94faf64147a001d35f24f189c3cdf7fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here\u2019s a summary of the major differences between take, collect, \nshow, head, and tail that we used earlier in this section for data retrieval.\ntake(n)\nThis function returns an array containing the first n elements from the DataFrame or RDD\n\u2022\t It is useful for quickly inspecting a small subset of the data\n\u2022\t It performs a lazy evaluation, meaning it only computes the required number of elements\ncollect()\nThis function retrieves all elements from the DataFrame or RDD and returns them as a list\n\u2022\t It should be used with caution as it brings all data to the driver node,", "mimetype": "text/plain", "start_char_idx": 97445, "end_char_idx": 98018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3031ccef-e341-488b-a3f2-80fc1d2718b4": {"__data__": {"id_": "3031ccef-e341-488b-a3f2-80fc1d2718b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01d02fa3-cfb1-40e2-88cc-801891efe245", "node_type": "1", "metadata": {}, "hash": "d3b718f4deb5a67cae825b05cb00e71fb5dd6e3109eb5fb5f7af3fe72fad45fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bff44c06-3069-4bf7-aa12-9c1662f37155", "node_type": "1", "metadata": {}, "hash": "e46cadd0c27f6384a405b97ddffdb63b03b1142c924b8279433c83a279a0a4d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "take(n)\nThis function returns an array containing the first n elements from the DataFrame or RDD\n\u2022\t It is useful for quickly inspecting a small subset of the data\n\u2022\t It performs a lazy evaluation, meaning it only computes the required number of elements\ncollect()\nThis function retrieves all elements from the DataFrame or RDD and returns them as a list\n\u2022\t It should be used with caution as it brings all data to the driver node, which can lead to out-of-\nmemory errors for large datasets\n\u2022\t It is suitable for small datasets or when working with aggregated results that fit into memory\n\nConverting a PySpark DataFrame to a Pandas DataFrame\n\nshow(n)\nThis function displays the first n rows of the DataFrame in a tabular format\n\u2022\t It is primarily used for visual inspection of data during exploratory data analysis (EDA) \nor debugging\n\u2022\t It provides a user-friendly display of data with column headers and formatting\nhead(n)\nThis function returns the first n rows of the DataFrame as a list of Row objects\n\u2022\t It is similar to take(n) but it returns Row objects instead of simple values\n\u2022\t It is often used when you need access to specific column values while working with structured data\ntail(n)\nThis function returns the last n rows of the DataFrame\n\u2022\t It is useful for examining the end of the dataset,", "mimetype": "text/plain", "start_char_idx": 97589, "end_char_idx": 98892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bff44c06-3069-4bf7-aa12-9c1662f37155": {"__data__": {"id_": "bff44c06-3069-4bf7-aa12-9c1662f37155", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3031ccef-e341-488b-a3f2-80fc1d2718b4", "node_type": "1", "metadata": {}, "hash": "c511f11122f7c8f7a6f082b78f6ff24d94faf64147a001d35f24f189c3cdf7fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e", "node_type": "1", "metadata": {}, "hash": "ffd49872ff24f755c46e5d0c46d7e66155d17adca084295f06d3ab195920c9fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "especially in cases where data is sorted in \ndescending order\n\u2022\t It performs a more expensive operation compared to head(n) as it may involve scanning \nthe entire dataset\nIn summary, take and collect are used to retrieve data elements, with take being more suitable \nfor small subsets and collect for retrieving all data (with caution). show is used for visual inspection, \nhead retrieves the first rows as Row objects, and tail retrieves the last rows of the dataset. Each \nmethod serves different purposes and should be chosen based on the specific requirements of the \ndata analysis task.\nWhen working with data in PySpark, sometimes, you will need to use some Python functions on the \nDataFrames. To achieve that, you will have to convert PySpark DataFrames to Pandas DataFrames. \nNow, let\u2019s take a look at how we can convert a Pyspark DataFrame to a Pandas DataFrame.", "mimetype": "text/plain", "start_char_idx": 98893, "end_char_idx": 99765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e": {"__data__": {"id_": "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff44c06-3069-4bf7-aa12-9c1662f37155", "node_type": "1", "metadata": {}, "hash": "e46cadd0c27f6384a405b97ddffdb63b03b1142c924b8279433c83a279a0a4d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96f76069-1dd9-4587-9155-c0280df6b1d6", "node_type": "1", "metadata": {}, "hash": "8a439f8cf73586d58765a0d292c6c5c854da57b8518b707c6cb094414d20b092", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each \nmethod serves different purposes and should be chosen based on the specific requirements of the \ndata analysis task.\nWhen working with data in PySpark, sometimes, you will need to use some Python functions on the \nDataFrames. To achieve that, you will have to convert PySpark DataFrames to Pandas DataFrames. \nNow, let\u2019s take a look at how we can convert a Pyspark DataFrame to a Pandas DataFrame.\nConverting a PySpark DataFrame to a Pandas DataFrame\nAt various times in your workflow, you will want to switch from a Pyspark DataFrame to a Pandas \nDataFrame. There are options to convert a PySpark DataFrame to a Pandas DataFrame. This option \nis toPandas().\nOne thing to note here is that Python inherently is not distributed. Therefore, when a PySpark \nDataFrame is converted to Pandas, the driver would need to collect all the data in its memory.", "mimetype": "text/plain", "start_char_idx": 99362, "end_char_idx": 100217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96f76069-1dd9-4587-9155-c0280df6b1d6": {"__data__": {"id_": "96f76069-1dd9-4587-9155-c0280df6b1d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e", "node_type": "1", "metadata": {}, "hash": "ffd49872ff24f755c46e5d0c46d7e66155d17adca084295f06d3ab195920c9fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a098279-cd1d-45be-a61b-9e549be819fe", "node_type": "1", "metadata": {}, "hash": "fe36d052ec42c29c8d2367ef0088447ffcc65e69ed9d25d29e1f2688bfc2fb13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are options to convert a PySpark DataFrame to a Pandas DataFrame. This option \nis toPandas().\nOne thing to note here is that Python inherently is not distributed. Therefore, when a PySpark \nDataFrame is converted to Pandas, the driver would need to collect all the data in its memory. We \nneed to make sure that the driver\u2019s memory is able to collect the data in itself. If the data is not able \nto fit in the driver\u2019s memory, it will cause an out-of-memory error.\n\nSpark DataFrames and their Operations\n\nHere\u2019s an example to see how we can convert a PySpark DataFrame to a Pandas DataFrame:\ndata_df.toPandas()\nAs a result, you will see a DataFrame with our specified columns and their data types:\ncol_1\ncol_2\ncol_3\ncol_4\ncol_5", "mimetype": "text/plain", "start_char_idx": 99927, "end_char_idx": 100660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a098279-cd1d-45be-a61b-9e549be819fe": {"__data__": {"id_": "6a098279-cd1d-45be-a61b-9e549be819fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96f76069-1dd9-4587-9155-c0280df6b1d6", "node_type": "1", "metadata": {}, "hash": "8a439f8cf73586d58765a0d292c6c5c854da57b8518b707c6cb094414d20b092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7654d8bb-f7d1-4b01-afc8-117ea54494d6", "node_type": "1", "metadata": {}, "hash": "cce2af2f14d4d02b1260078e956e924c4ca81662248642cb493e6ca9539cdc53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark DataFrames and their Operations\n\nHere\u2019s an example to see how we can convert a PySpark DataFrame to a Pandas DataFrame:\ndata_df.toPandas()\nAs a result, you will see a DataFrame with our specified columns and their data types:\ncol_1\ncol_2\ncol_3\ncol_4\ncol_5\n\n\n200.0\nString_test_1\n2023-01-01\n2023-01-01 12:00:00\n\n\n300.0\nString_test_2\n2023-02-01\n2023-01-02 12:00:00\n\n\n400.0\nString_test_3\n2023-03-01\n2023-01-03 12:00:00\nTable 4.1: DataFrame with the columns and data types specified by us\nIn the next section, we will learn about different data manipulation techniques.", "mimetype": "text/plain", "start_char_idx": 100399, "end_char_idx": 100969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7654d8bb-f7d1-4b01-afc8-117ea54494d6": {"__data__": {"id_": "7654d8bb-f7d1-4b01-afc8-117ea54494d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a098279-cd1d-45be-a61b-9e549be819fe", "node_type": "1", "metadata": {}, "hash": "fe36d052ec42c29c8d2367ef0088447ffcc65e69ed9d25d29e1f2688bfc2fb13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b40fb0d2-cc97-4642-901a-68026202faa4", "node_type": "1", "metadata": {}, "hash": "f9315039102f62765562de5c6d2ae9d41645480a1cddf090be8820c0227de734", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "400.0\nString_test_3\n2023-03-01\n2023-01-03 12:00:00\nTable 4.1: DataFrame with the columns and data types specified by us\nIn the next section, we will learn about different data manipulation techniques. You will need to filter, \nslice, and dice the data based on different criteria for different purposes. Therefore, data manipulation \nis essential in working with data.\nHow to manipulate data on rows and columns\nIn this section, we will learn how to do different data manipulation operations on Spark DataFrames \nrows and columns.\nWe will start by looking at how we can select columns in a Spark DataFrame.\nSelecting columns\nWe can use column functions for data manipulation at the column level in a Spark DataFrame.", "mimetype": "text/plain", "start_char_idx": 100769, "end_char_idx": 101485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b40fb0d2-cc97-4642-901a-68026202faa4": {"__data__": {"id_": "b40fb0d2-cc97-4642-901a-68026202faa4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7654d8bb-f7d1-4b01-afc8-117ea54494d6", "node_type": "1", "metadata": {}, "hash": "cce2af2f14d4d02b1260078e956e924c4ca81662248642cb493e6ca9539cdc53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c79739bb-d186-4311-8c11-c17ef8c0e999", "node_type": "1", "metadata": {}, "hash": "37701029563d39b19dc3338a400ed4e72aca0dbcd13efeda9ecf95dbcce74592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, data manipulation \nis essential in working with data.\nHow to manipulate data on rows and columns\nIn this section, we will learn how to do different data manipulation operations on Spark DataFrames \nrows and columns.\nWe will start by looking at how we can select columns in a Spark DataFrame.\nSelecting columns\nWe can use column functions for data manipulation at the column level in a Spark DataFrame. \nTo select a column in a DataFrame, we would use the select() function like so:\nfrom pyspark.sql import Column\ndata_df.select(data_df.col_3).show()\nAs a result, you will see only one column of the DataFrame with its data:\n+-------------+\n|col_3|\n+-------------+\n|string_test_1|\n|string_test_2|\n|string_test_3|\n+-------------+\n\nHow to manipulate data on rows and columns\n\nThe important thing to note here is that the resulting DataFrame with one column is a new \nDataFrame.", "mimetype": "text/plain", "start_char_idx": 101073, "end_char_idx": 101958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c79739bb-d186-4311-8c11-c17ef8c0e999": {"__data__": {"id_": "c79739bb-d186-4311-8c11-c17ef8c0e999", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b40fb0d2-cc97-4642-901a-68026202faa4", "node_type": "1", "metadata": {}, "hash": "f9315039102f62765562de5c6d2ae9d41645480a1cddf090be8820c0227de734", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "039645ce-26b7-40d6-aac6-d032fc917186", "node_type": "1", "metadata": {}, "hash": "cf5bc2d45aad2337b1d83d3a16ea1ab1d1616a935e26e8fea22012d6c184c3a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recalling what we discussed in Chapter 3, RDDs are immutable. The underlying \ndata structure for DataFrames is RDDs, therefore, DataFrames are also immutable. This means \nevery time you make a change to a DataFrame, a new DataFrame would be created out of it. \nYou would either have to assign the resultant DataFrame to a new DataFrame or overwrite the \noriginal DataFrame.\nThere are some other ways to achieve the same result in PySpark as well. Some of those are \ndemonstrated here:\ndata_df.select('col_3').show()\ndata_df.select(data_df['col_3']).show()\nOnce we select the required columns, there will be instances where you will need to add new columns \nto a DataFrame. We will now take a look at how we can create columns in a Spark DataFrame.\nCreating columns\nWe can use a withColumn() function to create a new column in a DataFrame.", "mimetype": "text/plain", "start_char_idx": 101959, "end_char_idx": 102797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "039645ce-26b7-40d6-aac6-d032fc917186": {"__data__": {"id_": "039645ce-26b7-40d6-aac6-d032fc917186", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c79739bb-d186-4311-8c11-c17ef8c0e999", "node_type": "1", "metadata": {}, "hash": "37701029563d39b19dc3338a400ed4e72aca0dbcd13efeda9ecf95dbcce74592", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "beb4bfe1-ae2b-45db-89bc-1092d6a866aa", "node_type": "1", "metadata": {}, "hash": "aaaa9fc97bd361ac9ddadd2736ae32f12f7ae79888f825d4419d04937b12e056", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will now take a look at how we can create columns in a Spark DataFrame.\nCreating columns\nWe can use a withColumn() function to create a new column in a DataFrame. To create a new \ncolumn, we would need to pass the column name and column values to fill the column with. In the \nfollowing example, we\u2019re creating a new column named col_6 and putting a constant literal A in \nthis column:\nfrom pyspark.sql import functions as F\ndata_df = data_df.withColumn(\"col_6\", F.lit(\"A\"))\ndata_df.show()\nAs a result, you will see a DataFrame with an additional column named col_6 filled with multiple \nA instances:\nThe lit() function is used to fill constant values in a column.\nYou can also delete columns that are no longer needed in a DataFrame. We will now take a look at \nhow we can drop columns in a Spark DataFrame.", "mimetype": "text/plain", "start_char_idx": 102632, "end_char_idx": 103443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "beb4bfe1-ae2b-45db-89bc-1092d6a866aa": {"__data__": {"id_": "beb4bfe1-ae2b-45db-89bc-1092d6a866aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "039645ce-26b7-40d6-aac6-d032fc917186", "node_type": "1", "metadata": {}, "hash": "cf5bc2d45aad2337b1d83d3a16ea1ab1d1616a935e26e8fea22012d6c184c3a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ffb57a7-3515-4979-a2ac-aa466597d7a2", "node_type": "1", "metadata": {}, "hash": "c0161ea189614508191d2f379eb2038521003c8f4eebb4870e122cc76c0ef9e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can also delete columns that are no longer needed in a DataFrame. We will now take a look at \nhow we can drop columns in a Spark DataFrame.\n\nSpark DataFrames and their Operations\n\nDropping columns\nIf we need to drop a column from a Spark DataFrame, we would use the drop() function. We need \nto provide the name of the column to be dropped from the DataFrame.", "mimetype": "text/plain", "start_char_idx": 103300, "end_char_idx": 103663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ffb57a7-3515-4979-a2ac-aa466597d7a2": {"__data__": {"id_": "7ffb57a7-3515-4979-a2ac-aa466597d7a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beb4bfe1-ae2b-45db-89bc-1092d6a866aa", "node_type": "1", "metadata": {}, "hash": "aaaa9fc97bd361ac9ddadd2736ae32f12f7ae79888f825d4419d04937b12e056", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fe9b0a2-5081-4e1a-a90f-568e78017e3e", "node_type": "1", "metadata": {}, "hash": "38929ce94ce34d8660068c809c291e0740f187c20c20da55b770e8a1f8449cf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can also delete columns that are no longer needed in a DataFrame. We will now take a look at \nhow we can drop columns in a Spark DataFrame.\n\nSpark DataFrames and their Operations\n\nDropping columns\nIf we need to drop a column from a Spark DataFrame, we would use the drop() function. We need \nto provide the name of the column to be dropped from the DataFrame. Here\u2019s an example of how to \nuse this function:\ndata_df = data_df.drop(\"col_5\")\ndata_df.show()\nAs a result, you will see that col_5 is dropped from the DataFrame:\n+-----+-----+-------------+----------+------+\n|col_1|col_2|col_3|col_4|col_6|\n+-----+-----+-------------+----------+------+\n|100|200.0|string_test_1|2023-01-01|A|\n|200|300.0|string_test_2|2023-02-01|A|\n|300|400.0|string_test_3|2023-03-01|A|\n+-----+-----+-------------+----------+------+\nWe have successfully dropped col_5 from this DataFrame.", "mimetype": "text/plain", "start_char_idx": 103300, "end_char_idx": 104169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fe9b0a2-5081-4e1a-a90f-568e78017e3e": {"__data__": {"id_": "2fe9b0a2-5081-4e1a-a90f-568e78017e3e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ffb57a7-3515-4979-a2ac-aa466597d7a2", "node_type": "1", "metadata": {}, "hash": "c0161ea189614508191d2f379eb2038521003c8f4eebb4870e122cc76c0ef9e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4", "node_type": "1", "metadata": {}, "hash": "ed03ab4f12a0a18280a06a46f782b4ff9c4962bcc93707b522ec49c2a04109f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can also drop multiple columns in the same drop statement as well:\ndata_df = data_df.drop(\"col_4\", \"col_5\")\nAlso note that if we drop a column that does not exist in the DataFrame, it will not result in any errors. \nThe resulting DataFrame would remain as it is.\nJust like dropping columns, you can also update columns in Spark DataFrames. Now, we will take a \nlook at how we can update columns in a Spark DataFrame.\nUpdating columns\nUpdating columns can also be done with the help of the withColumn() function in Spark. We \nneed to provide the name of the column to be updated along with the updated value. Notice that we \ncan also use this function to calculate some new values for the columns.", "mimetype": "text/plain", "start_char_idx": 104170, "end_char_idx": 104870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4": {"__data__": {"id_": "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fe9b0a2-5081-4e1a-a90f-568e78017e3e", "node_type": "1", "metadata": {}, "hash": "38929ce94ce34d8660068c809c291e0740f187c20c20da55b770e8a1f8449cf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c151e3f2-0172-4485-868a-8a31aa20a095", "node_type": "1", "metadata": {}, "hash": "4d5d9bf006346233fda0ee48f80c71d99bd935ee6509767813732de86278ed8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, we will take a \nlook at how we can update columns in a Spark DataFrame.\nUpdating columns\nUpdating columns can also be done with the help of the withColumn() function in Spark. We \nneed to provide the name of the column to be updated along with the updated value. Notice that we \ncan also use this function to calculate some new values for the columns. Here\u2019s an example:\ndata_df.withColumn(\"col_2\", F.col(\"col_2\") / 100).show()\nThis will give us the following updated frame:\n+-----+-----+-------------+----------+------+\n|col_1|col_2|col_3|col_4| col_6|\n+-----+-----+-------------+----------+------+\n|100|200.0|string_test_1|2023-01-01|A|\n|200|300.0|string_test_2|2023-02-01|A|\n|300|400.0|string_test_3|2023-03-01|A|\n+-----+-----+-------------+----------+------+\n\nHow to manipulate data on rows and columns\n\nOne thing to note here is the use of the col function when updating the column.", "mimetype": "text/plain", "start_char_idx": 104514, "end_char_idx": 105406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c151e3f2-0172-4485-868a-8a31aa20a095": {"__data__": {"id_": "c151e3f2-0172-4485-868a-8a31aa20a095", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4", "node_type": "1", "metadata": {}, "hash": "ed03ab4f12a0a18280a06a46f782b4ff9c4962bcc93707b522ec49c2a04109f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aef326da-0073-4176-9d55-1dfe4fb9fec5", "node_type": "1", "metadata": {}, "hash": "f1e5db43d206c79f1628cfaf1bd9cb6768bc2a01e013fd8f18afd96e87ae0d24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This function is \nused for column-wise operators. If we don\u2019t use this function, our code will return an error.\nYou don\u2019t always have to update a column in a DataFrame if you only need to rename a column. Now, \nwe will see how we can rename columns in a Spark DataFrame.\nRenaming columns\nFor changing the name of a column, we would use the withColumnRenamed() function in Spark. \nWe would need to provide the column name that needs to be changed along with the new column \nname.", "mimetype": "text/plain", "start_char_idx": 105407, "end_char_idx": 105885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aef326da-0073-4176-9d55-1dfe4fb9fec5": {"__data__": {"id_": "aef326da-0073-4176-9d55-1dfe4fb9fec5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c151e3f2-0172-4485-868a-8a31aa20a095", "node_type": "1", "metadata": {}, "hash": "4d5d9bf006346233fda0ee48f80c71d99bd935ee6509767813732de86278ed8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d04f3a5-b9e5-4e12-bc87-bfa511016c78", "node_type": "1", "metadata": {}, "hash": "c904704319d70ea7ed164af9ef09fa1d52175f5cc14cb9fea14fdfacb01e4d2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You don\u2019t always have to update a column in a DataFrame if you only need to rename a column. Now, \nwe will see how we can rename columns in a Spark DataFrame.\nRenaming columns\nFor changing the name of a column, we would use the withColumnRenamed() function in Spark. \nWe would need to provide the column name that needs to be changed along with the new column \nname. Here\u2019s the code to illustrate this:\ndata_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\ndata_df.show()\nAs a result, we\u2019ll see the following change:\n+-----+-----+-------------+----------+------+\n|col_1|col_2|string_col|col_4| col_6|\n+-----+-----+-------------+----------+------+\n|100|200.0|string_test_1|2023-01-01|A|\n|200|300.0|string_test_2|2023-02-01|A|\n|300|400.0|string_test_3|2023-03-01|A|\n+-----+-----+-------------+----------+------+\nNotice that col_3 is now called string_col after making the change.", "mimetype": "text/plain", "start_char_idx": 105519, "end_char_idx": 106401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d04f3a5-b9e5-4e12-bc87-bfa511016c78": {"__data__": {"id_": "1d04f3a5-b9e5-4e12-bc87-bfa511016c78", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aef326da-0073-4176-9d55-1dfe4fb9fec5", "node_type": "1", "metadata": {}, "hash": "f1e5db43d206c79f1628cfaf1bd9cb6768bc2a01e013fd8f18afd96e87ae0d24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7", "node_type": "1", "metadata": {}, "hash": "bf1da7b465cd45a0f1cd46b454eac286104616364f9152f6c297f1b925d4968e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, let\u2019s shift our focus to some data manipulation techniques in Spark DataFrames. You can have \nsearch-like functionality in Spark DataFrames for finding different values in a column. Now, let\u2019s take \na look at how we can find unique values in a column of a Spark DataFrame.\nFinding unique values in a column\nFinding unique values is a very useful function that would give us distinct values in a column. For \nthis purpose, we can use the distinct() function of the Spark DataFrame like so:\ndata_df.select(\"col_6\").distinct().show()\nHere is the result:\n+------+\n|col_6 |\n+------+\n|A|\n+------+\n\nSpark DataFrames and their Operations\n\nWe applied a distinct function on col_6 to get all the unique values in this column. In our case, \nthe column just had one distinct value, A, so that was shown.", "mimetype": "text/plain", "start_char_idx": 106402, "end_char_idx": 107198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7": {"__data__": {"id_": "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d04f3a5-b9e5-4e12-bc87-bfa511016c78", "node_type": "1", "metadata": {}, "hash": "c904704319d70ea7ed164af9ef09fa1d52175f5cc14cb9fea14fdfacb01e4d2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c69731a-be9a-4009-b3aa-8f4626ba5adb", "node_type": "1", "metadata": {}, "hash": "b36d8055da028491b7db7b91609533338250b6d20eaeeea1655acefe1c8525a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our case, \nthe column just had one distinct value, A, so that was shown.\nWe can also use it to find the count of distinct values in a given column. Here\u2019s an example of how \nto use this function:\ndata_df.select(F.countDistinct(\"col_6\").alias(\"Total_Unique\")).show()\nHere is the result:\n+------+\n|col_6 |\n+------+\n|1|\n+------+\nIn this example, we can see the total count of distinct values in col_6. Currently, it is the only type \nof distinct value present in this column, therefore, it returned 1.\nOne other useful function in Spark data manipulation is changing the case of a column. Now, let\u2019s \ntake a look at how we can change the case of a column in a Spark DataFrame.\nChanging the case of a column\nThere is also a function that exists in Spark to change the case of a column.", "mimetype": "text/plain", "start_char_idx": 107123, "end_char_idx": 107907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c69731a-be9a-4009-b3aa-8f4626ba5adb": {"__data__": {"id_": "9c69731a-be9a-4009-b3aa-8f4626ba5adb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7", "node_type": "1", "metadata": {}, "hash": "bf1da7b465cd45a0f1cd46b454eac286104616364f9152f6c297f1b925d4968e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3483b7b-6c2b-48f3-8756-a3f79c09770c", "node_type": "1", "metadata": {}, "hash": "9d9079036c19a43b00f0b75eafd56147761fde067a74b3db25991a8062bd1c8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, it is the only type \nof distinct value present in this column, therefore, it returned 1.\nOne other useful function in Spark data manipulation is changing the case of a column. Now, let\u2019s \ntake a look at how we can change the case of a column in a Spark DataFrame.\nChanging the case of a column\nThere is also a function that exists in Spark to change the case of a column. We don\u2019t need to specify \neach value of the column separately to make use of the function. Once applied, the whole column\u2019s \nvalues would change case. One such example is as follows:\nfrom pyspark.sql.functions import upper\ndata_df.withColumn('upper_string_col', upper(data_df.string_col)).\nshow()\nHere is the result:\nIn this example, we change the case of string_col to all caps.", "mimetype": "text/plain", "start_char_idx": 107525, "end_char_idx": 108287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3483b7b-6c2b-48f3-8756-a3f79c09770c": {"__data__": {"id_": "e3483b7b-6c2b-48f3-8756-a3f79c09770c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c69731a-be9a-4009-b3aa-8f4626ba5adb", "node_type": "1", "metadata": {}, "hash": "b36d8055da028491b7db7b91609533338250b6d20eaeeea1655acefe1c8525a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c70def48-f634-49de-88ae-da400c8f32fd", "node_type": "1", "metadata": {}, "hash": "415bcfaf34887383f682c190a32ea4d01dcafbf03fa18399f51f3cc591f1fbc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We don\u2019t need to specify \neach value of the column separately to make use of the function. Once applied, the whole column\u2019s \nvalues would change case. One such example is as follows:\nfrom pyspark.sql.functions import upper\ndata_df.withColumn('upper_string_col', upper(data_df.string_col)).\nshow()\nHere is the result:\nIn this example, we change the case of string_col to all caps. We need to assign this to a new \ncolumn, so, we create a column called upper_string_col to store these upper-case values. Also, \nnote that this column is not added to the original data_df because we did not save the results back \nin data_df.\n\nHow to manipulate data on rows and columns\n\nA lot of times in data manipulation, we would need functions to filter DataFrames. We will now take \na look at how we can filter data in a Spark DataFrame.", "mimetype": "text/plain", "start_char_idx": 107908, "end_char_idx": 108730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c70def48-f634-49de-88ae-da400c8f32fd": {"__data__": {"id_": "c70def48-f634-49de-88ae-da400c8f32fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3483b7b-6c2b-48f3-8756-a3f79c09770c", "node_type": "1", "metadata": {}, "hash": "9d9079036c19a43b00f0b75eafd56147761fde067a74b3db25991a8062bd1c8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75540c36-8d19-44ae-80e4-7e2d00207ad0", "node_type": "1", "metadata": {}, "hash": "3c349835e3a3ea28f9bef8e42c057a0bb6711526011da002f1a1eeb7a9970b37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, \nnote that this column is not added to the original data_df because we did not save the results back \nin data_df.\n\nHow to manipulate data on rows and columns\n\nA lot of times in data manipulation, we would need functions to filter DataFrames. We will now take \na look at how we can filter data in a Spark DataFrame.\nFiltering a DataFrame\nFiltering a DataFrame means that we can get a subset of rows or columns from a DataFrame. There \nare different ways of filtering a Spark DataFrame.", "mimetype": "text/plain", "start_char_idx": 108410, "end_char_idx": 108900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75540c36-8d19-44ae-80e4-7e2d00207ad0": {"__data__": {"id_": "75540c36-8d19-44ae-80e4-7e2d00207ad0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c70def48-f634-49de-88ae-da400c8f32fd", "node_type": "1", "metadata": {}, "hash": "415bcfaf34887383f682c190a32ea4d01dcafbf03fa18399f51f3cc591f1fbc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43afd3f1-c638-47bf-868e-81fbb74bfa43", "node_type": "1", "metadata": {}, "hash": "c8cfdbda8a591595c1152b8ed1067518c6432459a52a46b587e02c8d2e43cd87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How to manipulate data on rows and columns\n\nA lot of times in data manipulation, we would need functions to filter DataFrames. We will now take \na look at how we can filter data in a Spark DataFrame.\nFiltering a DataFrame\nFiltering a DataFrame means that we can get a subset of rows or columns from a DataFrame. There \nare different ways of filtering a Spark DataFrame. We will take a look at one example here:\ndata_df.filter(data_df.col_1 == 100).show()\nHere is the result:\n+-----+-----+-------------+----------+------+\n|col_1|col_2|string_col|col_4|col_6 |\n+-----+-----+-------------+----------+------+\n|100|200.0|string_test_1|2023-01-01|A|\n+-----+-----+-------------+----------+------+\nIn this example, we filter data_df to only include rows where the column value of col_1 is equal to \n100.", "mimetype": "text/plain", "start_char_idx": 108531, "end_char_idx": 109326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43afd3f1-c638-47bf-868e-81fbb74bfa43": {"__data__": {"id_": "43afd3f1-c638-47bf-868e-81fbb74bfa43", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75540c36-8d19-44ae-80e4-7e2d00207ad0", "node_type": "1", "metadata": {}, "hash": "3c349835e3a3ea28f9bef8e42c057a0bb6711526011da002f1a1eeb7a9970b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9", "node_type": "1", "metadata": {}, "hash": "94c8e9014fc02e20a90c57b888b6f4bc0166df2dec5a7808cdea2a5830bde1eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This criterion is met by only one row, therefore, a single row is returned in the resulting DataFrame.\nYou can use this function to slice and dice your data in a number of different ways based on \nthe requirements.\nSince we are talking about data filtering, we can also filter data based on logical operators as well. We \nwill now take a look at how we can use logical operators in DataFrames to filter data.\nLogical operators in a DataFrame\nAnother important set of operators that we can combine with filter operations in a DataFrame are the \nlogical operators. These consist of AND and OR operators, amongst others. These are used to filter \nDataFrames based on complex conditions.", "mimetype": "text/plain", "start_char_idx": 109327, "end_char_idx": 110010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9": {"__data__": {"id_": "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43afd3f1-c638-47bf-868e-81fbb74bfa43", "node_type": "1", "metadata": {}, "hash": "c8cfdbda8a591595c1152b8ed1067518c6432459a52a46b587e02c8d2e43cd87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f566fa3-378e-499c-9fc9-e413776e17ba", "node_type": "1", "metadata": {}, "hash": "6d2bb92cba31c1a3efae9db24ed0b0034df6009cd05f5302c473b0a8e66068f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \nwill now take a look at how we can use logical operators in DataFrames to filter data.\nLogical operators in a DataFrame\nAnother important set of operators that we can combine with filter operations in a DataFrame are the \nlogical operators. These consist of AND and OR operators, amongst others. These are used to filter \nDataFrames based on complex conditions. Let\u2019s take a look at how we use the AND operator here:\ndata_df.filter((data_df.col_1 == 100)\n& (data_df.col_6 == 'A')).show()\nHere is the result:\n+-----+------+-------------+----------+------+\n|col_1| col_2|string_col|col_4|col_6 |\n+-----+------+-------------+----------+------+\n|100| 200.0|string_test_1|2023-01-01|A|\n+-----+------+-------------+----------+------+\nIn this example, we\u2019re trying to get the rows where the value of col_1 is equal to 100 and the value \nof col_6 is A. Currently, only one row fulfills this condition, therefore, one row is returned as a result.", "mimetype": "text/plain", "start_char_idx": 109645, "end_char_idx": 110586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f566fa3-378e-499c-9fc9-e413776e17ba": {"__data__": {"id_": "7f566fa3-378e-499c-9fc9-e413776e17ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9", "node_type": "1", "metadata": {}, "hash": "94c8e9014fc02e20a90c57b888b6f4bc0166df2dec5a7808cdea2a5830bde1eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "314f3be1-65e6-401a-9e89-4ce9a0dbab08", "node_type": "1", "metadata": {}, "hash": "a07c5dc0157d3c02d5eaa2c3681df00877f5ae0b5a1bc33dc9b0348a915e24af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark DataFrames and their Operations\n\nNow, let\u2019s see how we can use the OR operator to combine conditions:\ndata_df.filter((data_df.col_1 == 100)\n| (data_df.col_2 == 300.00)).show()\nThis statement will give the following result:\n+-----+------+-------------+----------+------+\n|col_1| col_2|string_col|col_4| col_6|\n+-----+------+-------------+----------+------+\n|100| 200.0|string_test_1|2023-01-01|A|\n|200| 300.0|string_test_2|2023-02-01|A|\n+-----+------+-------------+----------+------+\nIn this example, we\u2019re trying to get the rows where the value of col_1 is equal to 100 or the value \nof col_2 is equal to 300.0.", "mimetype": "text/plain", "start_char_idx": 110588, "end_char_idx": 111205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "314f3be1-65e6-401a-9e89-4ce9a0dbab08": {"__data__": {"id_": "314f3be1-65e6-401a-9e89-4ce9a0dbab08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f566fa3-378e-499c-9fc9-e413776e17ba", "node_type": "1", "metadata": {}, "hash": "6d2bb92cba31c1a3efae9db24ed0b0034df6009cd05f5302c473b0a8e66068f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af07c2b6-0049-4443-8273-5d7f2dd1e868", "node_type": "1", "metadata": {}, "hash": "d3fdfbabaee47f9f936fd4bbf61ebd471d9c6ab42a22dd8baac11264e142a8cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, two rows fulfill this condition, therefore, they are returned \nas a result.\nIn data filtering, there is another important function to find values in a list. Now, we will see how you \nuse the isin() function in PySpark.\nUsing isin()\nThe isin() function is used to find values in a DataFrame column that exist in a list. To do this, \nwe would create a list with some values in it. Once we have the list, then we would use the isin() \nfunction to see whether some of the values that are in the list exist in the DataFrame.", "mimetype": "text/plain", "start_char_idx": 111206, "end_char_idx": 111736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af07c2b6-0049-4443-8273-5d7f2dd1e868": {"__data__": {"id_": "af07c2b6-0049-4443-8273-5d7f2dd1e868", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "314f3be1-65e6-401a-9e89-4ce9a0dbab08", "node_type": "1", "metadata": {}, "hash": "a07c5dc0157d3c02d5eaa2c3681df00877f5ae0b5a1bc33dc9b0348a915e24af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d07bed13-a8d7-47bf-a416-a27f19e2eee3", "node_type": "1", "metadata": {}, "hash": "2ac667ec3a04578c7608bc4fa42150cdc5a83ae9b6ea65d5a46c472f84a7e5e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using isin()\nThe isin() function is used to find values in a DataFrame column that exist in a list. To do this, \nwe would create a list with some values in it. Once we have the list, then we would use the isin() \nfunction to see whether some of the values that are in the list exist in the DataFrame. Here\u2019s an example \nto demonstrate this:\nlist = [100, 200]\ndata_df.filter(data_df.col_1.isin(list)).show()\nHere is the result:\n+-----+-----+-------------+----------+------+\n|col_1|col_2|string_col|col_4|col_6 |\n+-----+-----+-------------+----------+------+\n|100|200.0|string_test_1|2023-01-01|A|\n|200|300.0|string_test_2|2023-02-01|A|\n+-----+-----+-------------+----------+------+\nIn this example, we see that the values 100 and 200 are present in the data_df DataFrame in two \nof its rows, therefore, both rows are returned.", "mimetype": "text/plain", "start_char_idx": 111436, "end_char_idx": 112261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d07bed13-a8d7-47bf-a416-a27f19e2eee3": {"__data__": {"id_": "d07bed13-a8d7-47bf-a416-a27f19e2eee3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af07c2b6-0049-4443-8273-5d7f2dd1e868", "node_type": "1", "metadata": {}, "hash": "d3fdfbabaee47f9f936fd4bbf61ebd471d9c6ab42a22dd8baac11264e142a8cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b", "node_type": "1", "metadata": {}, "hash": "758c6d59cf194f427e801ae355be0cac78d6c04f1a0898e3d4309fb01ae96437", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can also convert data types for different columns in Spark DataFrames. Now, let\u2019s look at how we \ncan convert different data types in Spark DataFrames.\n\nHow to manipulate data on rows and columns\n\nDatatype conversions\nIn this section, we\u2019ll see different ways of converting data types in Spark DataFrame columns.\nWe will start by using the cast function in Spark. The following code illustrates this:\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import \nStringType,BooleanType,DateType,IntegerType\ndata_df_2 = data_df.withColumn(\"col_4\",col(\"col_4\").\ncast(StringType())) \\\n.withColumn(\"col_1\",col(\"col_1\").cast(IntegerType()))\ndata_df_2.printSchema()\ndata_df.", "mimetype": "text/plain", "start_char_idx": 112262, "end_char_idx": 112940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b": {"__data__": {"id_": "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d07bed13-a8d7-47bf-a416-a27f19e2eee3", "node_type": "1", "metadata": {}, "hash": "2ac667ec3a04578c7608bc4fa42150cdc5a83ae9b6ea65d5a46c472f84a7e5e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e314e09e-92ba-41d7-ab2b-979237ffee8e", "node_type": "1", "metadata": {}, "hash": "a03ca14659361a19e5f1bb635fc57448748834cd35a52d692586467b7754108e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates this:\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import \nStringType,BooleanType,DateType,IntegerType\ndata_df_2 = data_df.withColumn(\"col_4\",col(\"col_4\").\ncast(StringType())) \\\n.withColumn(\"col_1\",col(\"col_1\").cast(IntegerType()))\ndata_df_2.printSchema()\ndata_df.show()\nHere is the result:\nroot\n |-- col_1: integer (nullable = true)\n |-- col_2: double (nullable = true)\n |-- string_col: string (nullable = true)\n |-- col_4: string (nullable = true)\n |-- col_6: string (nullable = false)\n+-----+-----+-------------+----------+-----+\n|col_1|col_2|string_col|col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|100|200.", "mimetype": "text/plain", "start_char_idx": 112629, "end_char_idx": 113300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e314e09e-92ba-41d7-ab2b-979237ffee8e": {"__data__": {"id_": "e314e09e-92ba-41d7-ab2b-979237ffee8e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b", "node_type": "1", "metadata": {}, "hash": "758c6d59cf194f427e801ae355be0cac78d6c04f1a0898e3d4309fb01ae96437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9526e2d-83b4-4945-a15e-bf9a5c214db0", "node_type": "1", "metadata": {}, "hash": "9134ac27249443e67dacbf03b4e616ce671d5fc1af62f47564409c2e50027862", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0|string_test_1|2023-01-01|A|\n|200|300.0|string_test_2|2023-02-01|A|\n|300|400.0|string_test_3|2023-03-01|A|\n+-----+-----+-------------+----------+-----+\nIn the preceding code, we see that we\u2019re changing the data types of two columns, namely col_4 and \ncol_1. First, we change col_4 to string type. This column was previously a date column. Then, we \nchange col_1 to integer type from long.", "mimetype": "text/plain", "start_char_idx": 113300, "end_char_idx": 113689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9526e2d-83b4-4945-a15e-bf9a5c214db0": {"__data__": {"id_": "b9526e2d-83b4-4945-a15e-bf9a5c214db0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e314e09e-92ba-41d7-ab2b-979237ffee8e", "node_type": "1", "metadata": {}, "hash": "a03ca14659361a19e5f1bb635fc57448748834cd35a52d692586467b7754108e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0acda8b-015b-4e2f-9303-638f791d170a", "node_type": "1", "metadata": {}, "hash": "01675b53cc10172feff1ea4bd2e216950190e1fa59232b7bed034da370953f5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0|string_test_3|2023-03-01|A|\n+-----+-----+-------------+----------+-----+\nIn the preceding code, we see that we\u2019re changing the data types of two columns, namely col_4 and \ncol_1. First, we change col_4 to string type. This column was previously a date column. Then, we \nchange col_1 to integer type from long.\nHere is the schema of data_df for reference:\nroot\n |-- col_1: long (nullable = true)\n |-- col_2: double (nullable = true)\n |-- string_col: string (nullable = true)\n |-- col_4: date (nullable = true)\n |-- col_5: timestamp (nullable = true)\n |-- col_6: string (nullable = false)\nWe see that col_1 and col_4 were different data types.", "mimetype": "text/plain", "start_char_idx": 113378, "end_char_idx": 114021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0acda8b-015b-4e2f-9303-638f791d170a": {"__data__": {"id_": "c0acda8b-015b-4e2f-9303-638f791d170a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9526e2d-83b4-4945-a15e-bf9a5c214db0", "node_type": "1", "metadata": {}, "hash": "9134ac27249443e67dacbf03b4e616ce671d5fc1af62f47564409c2e50027862", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8468dbe4-714e-4770-951c-e498ad245e3f", "node_type": "1", "metadata": {}, "hash": "3700a2b9b2b0b2a39a0e1e43a2eb8acd5e0c239fee06fc1cd34d0df61d94ddd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark DataFrames and their Operations\n\nThe next example of changing the data types of the columns is by using the selectExpr() function. \nThe following code illustrates this:\ndata_df_3 = data_df_2.selectExpr(\"cast(col_4 as date) col_4\",\n\"cast(col_1 as long) col_1\")\ndata_df_3.printSchema()\ndata_df_3.show(truncate=False)\nHere is the result:\nroot\n |-- col_4: date (nullable = true)\n |-- col_1: long (nullable = true)\n+----------+-----+\n|col_4|col_1|\n+----------+-----+\n|2023-01-01|100|\n|2023-02-01|200|\n|2023-03-01|300|\n+----------+-----+\nIn the preceding code, we see that we\u2019re changing the data types of two columns, namely col_4 and \ncol_1.", "mimetype": "text/plain", "start_char_idx": 114023, "end_char_idx": 114666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8468dbe4-714e-4770-951c-e498ad245e3f": {"__data__": {"id_": "8468dbe4-714e-4770-951c-e498ad245e3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0acda8b-015b-4e2f-9303-638f791d170a", "node_type": "1", "metadata": {}, "hash": "01675b53cc10172feff1ea4bd2e216950190e1fa59232b7bed034da370953f5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5822eeba-5a06-4278-8df1-1ca75b61a3ec", "node_type": "1", "metadata": {}, "hash": "b4b8348f2c3a3fc195ad44209f11ca3da4c2a548250e078b8d7ef9a4965e79d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we change col_4 back to the date type. Then, we change col_1 to the long type.\nThe next example of changing the data types of the columns is by using SQL.", "mimetype": "text/plain", "start_char_idx": 114667, "end_char_idx": 114828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5822eeba-5a06-4278-8df1-1ca75b61a3ec": {"__data__": {"id_": "5822eeba-5a06-4278-8df1-1ca75b61a3ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8468dbe4-714e-4770-951c-e498ad245e3f", "node_type": "1", "metadata": {}, "hash": "3700a2b9b2b0b2a39a0e1e43a2eb8acd5e0c239fee06fc1cd34d0df61d94ddd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06cba9e4-c6ff-48a5-b3e3-a0508d82c529", "node_type": "1", "metadata": {}, "hash": "6a1b29fd0625e5f3636bc2bf9814c4537eefd2f2ac7ad86add130823bb0fb28f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we change col_4 back to the date type. Then, we change col_1 to the long type.\nThe next example of changing the data types of the columns is by using SQL. The following code \nillustrates this:\ndata_df_3.createOrReplaceTempView(\"CastExample\")\ndata_df_4 = spark.sql(\"SELECT DOUBLE(col_1), DATE(col_4) from \nCastExample\")\ndata_df_4.printSchema()\ndata_df_4.show(truncate=False)\nHere is the result:\nroot\n |-- col_1: double (nullable = true)\n |-- col_4: date (nullable = true)\n+-----+----------+\n|col_1|col_4|\n+-----+----------+\n|100.0|2023-01-01|\n|200.0|2023-02-01|\n|300.0|2023-03-01|\n+-----+----------+\n\nHow to manipulate data on rows and columns\n\nIn the preceding code, we see that we\u2019re changing the data types of two columns, namely col_4 and \ncol_1.", "mimetype": "text/plain", "start_char_idx": 114667, "end_char_idx": 115423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06cba9e4-c6ff-48a5-b3e3-a0508d82c529": {"__data__": {"id_": "06cba9e4-c6ff-48a5-b3e3-a0508d82c529", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5822eeba-5a06-4278-8df1-1ca75b61a3ec", "node_type": "1", "metadata": {}, "hash": "b4b8348f2c3a3fc195ad44209f11ca3da4c2a548250e078b8d7ef9a4965e79d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "092ccb19-fb27-49da-9d9d-4b3397a7315d", "node_type": "1", "metadata": {}, "hash": "ba0a5a1c6b1f1de40566b35ba6785251ba7bc8dc56d70e8eb15a64c178e469b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we use createOrReplaceTempView() to create a table named CastExample. \nThen, we use this table to change col_4 back to the date type. Then, we change col_1 to the \ndouble type.\nIn the data analysis world, working with null values is very valuable. Now, let\u2019s take a look at how we \ncan drop null values from a DataFrame.\nDropping null values from a DataFrame\nSometimes, in the data, there exist null values that can make clean data messy. Dropping nulls is an \nessential exercise that a lot of data analysts and data engineers need to do. Pyspark provides us with \nrelevant functions to do this.\nLet\u2019s create another DataFrame called salary_data to show some of the next operations:\nsalary_data = [(\"John\", \"Field-eng\", 3500),\n(\"Michael\", \"Field-eng\", 4500),\n(\"Robert\", None, 4000),", "mimetype": "text/plain", "start_char_idx": 115424, "end_char_idx": 116213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "092ccb19-fb27-49da-9d9d-4b3397a7315d": {"__data__": {"id_": "092ccb19-fb27-49da-9d9d-4b3397a7315d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06cba9e4-c6ff-48a5-b3e3-a0508d82c529", "node_type": "1", "metadata": {}, "hash": "6a1b29fd0625e5f3636bc2bf9814c4537eefd2f2ac7ad86add130823bb0fb28f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4443e3fa-29f9-4278-a011-af8e3042177c", "node_type": "1", "metadata": {}, "hash": "35be6ce33780fb345acec11c452a976f0ce9b5bc5f1a9e00941e090dfdab7146", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dropping nulls is an \nessential exercise that a lot of data analysts and data engineers need to do. Pyspark provides us with \nrelevant functions to do this.\nLet\u2019s create another DataFrame called salary_data to show some of the next operations:\nsalary_data = [(\"John\", \"Field-eng\", 3500),\n(\"Michael\", \"Field-eng\", 4500),\n(\"Robert\", None, 4000),\n(\"Maria\", \"Finance\", 3500),\n(\"John\", \"Sales\", 3000),\n(\"Kelly\", \"Finance\", 3500),\n(\"Kate\", \"Finance\", 3000),\n(\"Martin\", None, 3500),\n(\"Kiran\", \"Sales\", 2200),\n(\"Michael\", \"Field-eng\", 4500)\n]\ncolumns= [\"Employee\", \"Department\", \"Salary\"]\nsalary_data = spark.createDataFrame(data = salary_data, schema = \ncolumns)\nsalary_data.", "mimetype": "text/plain", "start_char_idx": 115870, "end_char_idx": 116538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4443e3fa-29f9-4278-a011-af8e3042177c": {"__data__": {"id_": "4443e3fa-29f9-4278-a011-af8e3042177c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "092ccb19-fb27-49da-9d9d-4b3397a7315d", "node_type": "1", "metadata": {}, "hash": "ba0a5a1c6b1f1de40566b35ba6785251ba7bc8dc56d70e8eb15a64c178e469b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2053d1a9-a0eb-4913-adca-5f10204c5c21", "node_type": "1", "metadata": {}, "hash": "80812319139d88da0102033d415b3aabbbf3feacbc9adee94cad3fc773f6a0bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(\"Kelly\", \"Finance\", 3500),\n(\"Kate\", \"Finance\", 3000),\n(\"Martin\", None, 3500),\n(\"Kiran\", \"Sales\", 2200),\n(\"Michael\", \"Field-eng\", 4500)\n]\ncolumns= [\"Employee\", \"Department\", \"Salary\"]\nsalary_data = spark.createDataFrame(data = salary_data, schema = \ncolumns)\nsalary_data.printSchema()\nsalary_data.", "mimetype": "text/plain", "start_char_idx": 116267, "end_char_idx": 116564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2053d1a9-a0eb-4913-adca-5f10204c5c21": {"__data__": {"id_": "2053d1a9-a0eb-4913-adca-5f10204c5c21", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4443e3fa-29f9-4278-a011-af8e3042177c", "node_type": "1", "metadata": {}, "hash": "35be6ce33780fb345acec11c452a976f0ce9b5bc5f1a9e00941e090dfdab7146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5cdd13b-20d9-4291-989b-e407b1183b83", "node_type": "1", "metadata": {}, "hash": "98fb4c731b3cb2b1a525e1a010df1bd6e0894a131ac835210d34de9f9c77ad0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Finance\", 3500),\n(\"Kate\", \"Finance\", 3000),\n(\"Martin\", None, 3500),\n(\"Kiran\", \"Sales\", 2200),\n(\"Michael\", \"Field-eng\", 4500)\n]\ncolumns= [\"Employee\", \"Department\", \"Salary\"]\nsalary_data = spark.createDataFrame(data = salary_data, schema = \ncolumns)\nsalary_data.printSchema()\nsalary_data.show()\nHere is the result:\nroot\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\nSpark DataFrames and their Operations\n\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|John| Field-eng| 3500 |\n|Michael| Field-eng| 4500 |\n|Robert|null| 4000 |\n|Maria|Finance| 3500 |\n|John|Sales| 3000 |\n|Kelly|Finance| 3500 |\n|Kate|Finance| 3000 |\n|Martin|null| 3500 |\n|Kiran|Sales| 2200 |\n|Michael|Field-eng| 4500 |\n+--------+----------+------+\nNow,", "mimetype": "text/plain", "start_char_idx": 116277, "end_char_idx": 117105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5cdd13b-20d9-4291-989b-e407b1183b83": {"__data__": {"id_": "e5cdd13b-20d9-4291-989b-e407b1183b83", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2053d1a9-a0eb-4913-adca-5f10204c5c21", "node_type": "1", "metadata": {}, "hash": "80812319139d88da0102033d415b3aabbbf3feacbc9adee94cad3fc773f6a0bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3b01a48-b84e-40b7-9025-de2b14b2fd09", "node_type": "1", "metadata": {}, "hash": "0248b1de5c3ac0f97c567faab7a86b322eda065e2a20665deba9d2ff53da90b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "let\u2019s take a look at the dropna() function; this will help us drop null values from our DataFrame:\nsalary_data.dropna().show()\nHere is the result:\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| John| Field-eng| 3500 |\n| Michael| Field-eng| 4500 |\n| Maria|Finance| 3500 |\n| John|Sales| 3000 |\n| Kelly|Finance| 3500 |\n| Kate|Finance| 3000 |\n| Kiran|Sales| 2200 |\n| Michael| Field-eng| 4500 |\n+--------+----------+------+\nWe see in the resulting DataFrame that rows with Robert and Martin are deleted from the new \nDataFrame when we use the dropna() function.\nDeduplicating data is another useful technique that is often required in data analysis tasks.", "mimetype": "text/plain", "start_char_idx": 117106, "end_char_idx": 117798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3b01a48-b84e-40b7-9025-de2b14b2fd09": {"__data__": {"id_": "a3b01a48-b84e-40b7-9025-de2b14b2fd09", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5cdd13b-20d9-4291-989b-e407b1183b83", "node_type": "1", "metadata": {}, "hash": "98fb4c731b3cb2b1a525e1a010df1bd6e0894a131ac835210d34de9f9c77ad0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3967a15-6aeb-4631-9d46-d6477070a86f", "node_type": "1", "metadata": {}, "hash": "4547b87b799a19fec81dcb2bf49793b94b1bd247fc423555b0d680c6d76a4c25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deduplicating data is another useful technique that is often required in data analysis tasks. Now, let\u2019s \ntake a look at how we can drop duplicate values from a DataFrame.\n\nHow to manipulate data on rows and columns\n\nDropping duplicates from a DataFrame\nSometimes, in the data, there are redundant values present that would make clean data messy. Dropping \nthese values might be needed in a lot of use cases. PySpark provides us with the dropDuplicates() \nfunction to do this.", "mimetype": "text/plain", "start_char_idx": 117705, "end_char_idx": 118181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3967a15-6aeb-4631-9d46-d6477070a86f": {"__data__": {"id_": "c3967a15-6aeb-4631-9d46-d6477070a86f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3b01a48-b84e-40b7-9025-de2b14b2fd09", "node_type": "1", "metadata": {}, "hash": "0248b1de5c3ac0f97c567faab7a86b322eda065e2a20665deba9d2ff53da90b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a736e4f9-15a0-40de-967a-6e35134ae524", "node_type": "1", "metadata": {}, "hash": "5e3f8039ef8759fbe59e1eeeaa2047d9c441137afe94a4312a63d1c5196b8614", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, let\u2019s \ntake a look at how we can drop duplicate values from a DataFrame.\n\nHow to manipulate data on rows and columns\n\nDropping duplicates from a DataFrame\nSometimes, in the data, there are redundant values present that would make clean data messy. Dropping \nthese values might be needed in a lot of use cases. PySpark provides us with the dropDuplicates() \nfunction to do this. Here\u2019s the code to illustrate this:\nnew_salary_data = salary_data.dropDuplicates().show()\nHere is the result:\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|John|Field-eng|3500|\n|Michael|Field-eng|4500|\n|Maria|Finance|3500|\n|John|Sales|3000|\n|Kelly|Finance|3500|\n|Kate|Finance|3000|\n|Kiran|Sales|2200|\n+--------+----------+------+\nWe see in this example that employees named Michael are only shown once in the resulting DataFrame \nafter we apply the dropDuplicates() function to the original DataFrame.", "mimetype": "text/plain", "start_char_idx": 117799, "end_char_idx": 118725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a736e4f9-15a0-40de-967a-6e35134ae524": {"__data__": {"id_": "a736e4f9-15a0-40de-967a-6e35134ae524", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3967a15-6aeb-4631-9d46-d6477070a86f", "node_type": "1", "metadata": {}, "hash": "4547b87b799a19fec81dcb2bf49793b94b1bd247fc423555b0d680c6d76a4c25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d716e990-de77-4d34-a4a5-fd9f39686f53", "node_type": "1", "metadata": {}, "hash": "9dbf25ead4360f4099544fe32aae08f93d4270290790c00377efc1cdea5f4714", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This name and its \ncorresponding values exist in the original DataFrame twice.\nNow that we have learned about different data filtering techniques, we will now see how we can \naggregate data in Pyspark DataFrames.\nUsing aggregates in a DataFrame\nSome of the methods available in Spark for aggregating data are as follows:\n\u2022\t agg\n\u2022\t avg\n\u2022\t count\n\u2022\t max\n\u2022\t mean\n\u2022\t min\n\u2022\t pivot\n\u2022\t sum\n\nSpark DataFrames and their Operations\n\nWe will see some of them in action in the following code examples.\nAverage (avg)\nIn the following example, we see how to use aggregate functions in Spark.", "mimetype": "text/plain", "start_char_idx": 118726, "end_char_idx": 119302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d716e990-de77-4d34-a4a5-fd9f39686f53": {"__data__": {"id_": "d716e990-de77-4d34-a4a5-fd9f39686f53", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a736e4f9-15a0-40de-967a-6e35134ae524", "node_type": "1", "metadata": {}, "hash": "5e3f8039ef8759fbe59e1eeeaa2047d9c441137afe94a4312a63d1c5196b8614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d45b420-02a7-4774-b242-06fa4268df16", "node_type": "1", "metadata": {}, "hash": "6ca3f81b28125e7843876e3b482863e00ce1b8b616cd3f6e049def51a77b6d6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Average (avg)\nIn the following example, we see how to use aggregate functions in Spark. We will start by calculating \nthe average of all the values in a column:\nfrom pyspark.sql.functions import countDistinct, avg\nsalary_data.select(avg('Salary')).show()\nHere is the result:\n+-----------+\n|avg(Salary)|\n+-----------+\n|3520.0|\n+-----------+\nThis example calculates the average of the salary column of the salary_data DataFrame. We have \npassed the Salary column to the avg function and it has calculated the average of that column for us.\nNow, let\u2019s take a look at how to count different elements in a PySpark DataFrame.", "mimetype": "text/plain", "start_char_idx": 119215, "end_char_idx": 119834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d45b420-02a7-4774-b242-06fa4268df16": {"__data__": {"id_": "4d45b420-02a7-4774-b242-06fa4268df16", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d716e990-de77-4d34-a4a5-fd9f39686f53", "node_type": "1", "metadata": {}, "hash": "9dbf25ead4360f4099544fe32aae08f93d4270290790c00377efc1cdea5f4714", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae4e376a-2014-4c43-a327-8b33bc9c2196", "node_type": "1", "metadata": {}, "hash": "a16ffc19ed98391fe3c627a14ea54cb8222fc22cd09dbcf25cf1001aa4f7a925", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have \npassed the Salary column to the avg function and it has calculated the average of that column for us.\nNow, let\u2019s take a look at how to count different elements in a PySpark DataFrame.\nCount\nIn the following code example, we can see how you use aggregate functions in Spark:\nsalary_data.agg({'Salary':'count'}).show()\nHere is the result:\n+-------------+\n|count(Salary)|\n+-------------+\n|10|\n+-------------+\nThis example calculates the total count of the values in the Salary column of the salary_data \nDataFrame. We have passed the Salary column to the agg function with count as its other \nparameter, and it has calculated the count of that column for us.\nNow, let\u2019s take a look at how to count distinct elements in a PySpark DataFrame.", "mimetype": "text/plain", "start_char_idx": 119642, "end_char_idx": 120387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae4e376a-2014-4c43-a327-8b33bc9c2196": {"__data__": {"id_": "ae4e376a-2014-4c43-a327-8b33bc9c2196", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d45b420-02a7-4774-b242-06fa4268df16", "node_type": "1", "metadata": {}, "hash": "6ca3f81b28125e7843876e3b482863e00ce1b8b616cd3f6e049def51a77b6d6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a506b590-0a4d-410c-b8c4-489e5beab966", "node_type": "1", "metadata": {}, "hash": "fbad04eb9c05ea90e0c56a484dda202306ea476274b80b4783f97c3731a880d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the agg function with count as its other \nparameter, and it has calculated the count of that column for us.\nNow, let\u2019s take a look at how to count distinct elements in a PySpark DataFrame.\n\nHow to manipulate data on rows and columns\n\nCount distinct values\nIn the following example, we will look at how to count distinct elements in a PySpark DataFrame:\nsalary_data.select(countDistinct(\"Salary\").alias(\"Distinct Salary\")).\nshow()\nHere is the result:\n+---------------+\n|Distinct Salary|\n+---------------+\n|5|\n+---------------+\nThis example calculates total distinct values in the salary column of the salary_data DataFrame. \nWe have passed the Salary column to the countDistinct function and it has calculated the \ncount of that column for us.\nNow, let\u2019s take a look at how to find maximum values in a PySpark DataFrame.", "mimetype": "text/plain", "start_char_idx": 120163, "end_char_idx": 121018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a506b590-0a4d-410c-b8c4-489e5beab966": {"__data__": {"id_": "a506b590-0a4d-410c-b8c4-489e5beab966", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae4e376a-2014-4c43-a327-8b33bc9c2196", "node_type": "1", "metadata": {}, "hash": "a16ffc19ed98391fe3c627a14ea54cb8222fc22cd09dbcf25cf1001aa4f7a925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5613560-6433-4e09-8a9a-13a7d194d8db", "node_type": "1", "metadata": {}, "hash": "3f98743ba37e3e4bf3f809aa4cf7420e3a0d643d89096d0eb79f2b846294fe73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "show()\nHere is the result:\n+---------------+\n|Distinct Salary|\n+---------------+\n|5|\n+---------------+\nThis example calculates total distinct values in the salary column of the salary_data DataFrame. \nWe have passed the Salary column to the countDistinct function and it has calculated the \ncount of that column for us.\nNow, let\u2019s take a look at how to find maximum values in a PySpark DataFrame.\nFinding maximums (max)\nIn the following code example, we will take a look at how to find maximum values in a column of a \nPySpark DataFrame:\nsalary_data.agg({'Salary':'max'}).show()\nHere is the result:\n+-----------+\n|max(Salary)|\n+-----------+\n|4500|\n+-----------+\nThis example calculates the maximum value out of all the values in the Salary column of the \nsalary_data DataFrame.", "mimetype": "text/plain", "start_char_idx": 120622, "end_char_idx": 121399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5613560-6433-4e09-8a9a-13a7d194d8db": {"__data__": {"id_": "b5613560-6433-4e09-8a9a-13a7d194d8db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a506b590-0a4d-410c-b8c4-489e5beab966", "node_type": "1", "metadata": {}, "hash": "fbad04eb9c05ea90e0c56a484dda202306ea476274b80b4783f97c3731a880d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7609f27f-9005-45e6-bf37-cd28313e0f1e", "node_type": "1", "metadata": {}, "hash": "b087c98fcf3d167ffddaf5ca5ff144dafaeb7c5580f8ad1afda83111551df89e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finding maximums (max)\nIn the following code example, we will take a look at how to find maximum values in a column of a \nPySpark DataFrame:\nsalary_data.agg({'Salary':'max'}).show()\nHere is the result:\n+-----------+\n|max(Salary)|\n+-----------+\n|4500|\n+-----------+\nThis example calculates the maximum value out of all the values in the Salary column of the \nsalary_data DataFrame. We have passed the Salary column to the agg function with max as \nits other parameter, and it has calculated the maximum of that column for us.\nNow, let\u2019s take a look at how to get the sum of all elements in a PySpark DataFrame.", "mimetype": "text/plain", "start_char_idx": 121019, "end_char_idx": 121628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7609f27f-9005-45e6-bf37-cd28313e0f1e": {"__data__": {"id_": "7609f27f-9005-45e6-bf37-cd28313e0f1e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5613560-6433-4e09-8a9a-13a7d194d8db", "node_type": "1", "metadata": {}, "hash": "3f98743ba37e3e4bf3f809aa4cf7420e3a0d643d89096d0eb79f2b846294fe73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf4ce08f-d95a-4d0b-b0f9-514709d96e28", "node_type": "1", "metadata": {}, "hash": "5c952316851776cff61d476df52a309ffbf90fd38de80ad553ca76340b72718e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the agg function with max as \nits other parameter, and it has calculated the maximum of that column for us.\nNow, let\u2019s take a look at how to get the sum of all elements in a PySpark DataFrame.\n\nSpark DataFrames and their Operations\n\nSum\nIn the following code example, we will look at how to sum all values in a PySpark DataFrames:\nsalary_data.agg({'Salary':'sum'}).show()\nHere is the result:\n+-----------+\n|sum(Salary)|\n+-----------+\n|35200|\n+-----------+\nThis example calculates the sum of all the values in the Salary column of the salary_data \nDataFrame. We have passed the Salary column to the agg function with sum as its other parameter, \nand it has calculated the sum of that column for us.\nNow, let\u2019s take a look at how to sort data in a PySpark DataFrame.", "mimetype": "text/plain", "start_char_idx": 121400, "end_char_idx": 122200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf4ce08f-d95a-4d0b-b0f9-514709d96e28": {"__data__": {"id_": "cf4ce08f-d95a-4d0b-b0f9-514709d96e28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7609f27f-9005-45e6-bf37-cd28313e0f1e", "node_type": "1", "metadata": {}, "hash": "b087c98fcf3d167ffddaf5ca5ff144dafaeb7c5580f8ad1afda83111551df89e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46b090a9-cfc8-4bf9-8c54-3085dd3539c3", "node_type": "1", "metadata": {}, "hash": "15cf924517700a3082b80534906277c1d5e551c55de88e568cae5f733a37471a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the agg function with sum as its other parameter, \nand it has calculated the sum of that column for us.\nNow, let\u2019s take a look at how to sort data in a PySpark DataFrame.\nSort data with OrderBy\nIn the following code example, we will look at how can we sort data in ascending order in a \nPySpark DataFrame:\nsalary_data.orderBy(\"Salary\").show()\nHere is the result:\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| Kiran| Sales| 2200 |\n| Kate| Finance| 3000 |\n| John| Sales| 3000 |\n| John| Field-eng| 3500 |\n| Martin | null| 3500 |\n| Kelly| Finance| 3500 |\n| Maria| Finance| 3500 |\n| Robert | null| 4000 |\n| Michael| Field-eng| 4500 |\n| Michael| Field-eng| 4500 |\n+--------+----------+------+\n\nSummary\n\nThis example sorts the full DataFrame based on the values in the Salary column of the salary_\ndata DataFrame.", "mimetype": "text/plain", "start_char_idx": 121994, "end_char_idx": 122879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46b090a9-cfc8-4bf9-8c54-3085dd3539c3": {"__data__": {"id_": "46b090a9-cfc8-4bf9-8c54-3085dd3539c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf4ce08f-d95a-4d0b-b0f9-514709d96e28", "node_type": "1", "metadata": {}, "hash": "5c952316851776cff61d476df52a309ffbf90fd38de80ad553ca76340b72718e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a23b8f52-cca3-4f99-a381-fef16cf0754b", "node_type": "1", "metadata": {}, "hash": "677c6e6af170b16be004f1875efecad5e77e33986e149dc2d1e52cdfc219d491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the orderBy function and it has sorted \nthe DataFrame based on this column.\nWe can also sort the data in descending format by adding another function, desc(), to the original \norderBy function.", "mimetype": "text/plain", "start_char_idx": 122880, "end_char_idx": 123109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a23b8f52-cca3-4f99-a381-fef16cf0754b": {"__data__": {"id_": "a23b8f52-cca3-4f99-a381-fef16cf0754b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46b090a9-cfc8-4bf9-8c54-3085dd3539c3", "node_type": "1", "metadata": {}, "hash": "15cf924517700a3082b80534906277c1d5e551c55de88e568cae5f733a37471a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03342b7f-810a-47d0-9213-0267839e7c4e", "node_type": "1", "metadata": {}, "hash": "17521b5faa2ac2be8a534c424e32dac2be196497b0d18fdf298313ea54d25f05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the orderBy function and it has sorted \nthe DataFrame based on this column.\nWe can also sort the data in descending format by adding another function, desc(), to the original \norderBy function. The following example illustrates this:\nsalary_data.orderBy(salary_data[\"Salary\"].desc()).show()\nHere is the result:\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| Michael| Field-eng| 4500 |\n| Michael| Field-eng| 4500 |\n| Robert | null| 4000 |\n| Martin | null| 3500 |\n| Kelly| Finance| 3500 |\n| Maria| Finance| 3500 |\n| John| Field-eng| 3500 |\n| John| Sales| 3000 |\n| Kate| Finance| 3000 |\n| Kiran| Sales| 2200 |\n+--------+----------+------+\nThis example is sorting the full DataFrame in descending order based on the values in the Salary \ncolumn of the salary_data DataFrame.", "mimetype": "text/plain", "start_char_idx": 122880, "end_char_idx": 123728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03342b7f-810a-47d0-9213-0267839e7c4e": {"__data__": {"id_": "03342b7f-810a-47d0-9213-0267839e7c4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a23b8f52-cca3-4f99-a381-fef16cf0754b", "node_type": "1", "metadata": {}, "hash": "677c6e6af170b16be004f1875efecad5e77e33986e149dc2d1e52cdfc219d491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d78fe297-fc46-4458-b14d-1456143799a2", "node_type": "1", "metadata": {}, "hash": "38154368014a341f55bf27415d8195b0f34d9ebeb284a53dc26595e21e884314", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have passed the Salary column to the orderBy \nfunction with desc() as an additional function call and it has sorted the DataFrame in descending \norder based on this column.\nSummary\nOver the course of this chapter, we have learned how to manipulate data in Spark DataFrames.\nWe talked about the Spark DataFrame API and what different data types are in Spark. We also learned \nhow to create DataFrames in Spark and how we can view these DataFrames once they\u2019ve been created. \nFinally, we learned about different data manipulation and data aggregation functions.\nIn the next chapter, we will cover some advanced operations in Spark with respect to data manipulation.\n\nSpark DataFrames and their Operations\n\nAdvanced Operations and \nOptimizations in Spark\nIn this chapter, we will delve into the advanced capabilities of Apache Spark, equipping you with the \nknowledge and techniques necessary to optimize your data processing workflows.", "mimetype": "text/plain", "start_char_idx": 123729, "end_char_idx": 124665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d78fe297-fc46-4458-b14d-1456143799a2": {"__data__": {"id_": "d78fe297-fc46-4458-b14d-1456143799a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03342b7f-810a-47d0-9213-0267839e7c4e", "node_type": "1", "metadata": {}, "hash": "17521b5faa2ac2be8a534c424e32dac2be196497b0d18fdf298313ea54d25f05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d072a57f-da17-4ec5-8da5-66a9091b9d73", "node_type": "1", "metadata": {}, "hash": "b2dd119c96ac63de3d5c3f0c0ed09b2cbb7f784c4c798acbebf97f820475a8e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we learned about different data manipulation and data aggregation functions.\nIn the next chapter, we will cover some advanced operations in Spark with respect to data manipulation.\n\nSpark DataFrames and their Operations\n\nAdvanced Operations and \nOptimizations in Spark\nIn this chapter, we will delve into the advanced capabilities of Apache Spark, equipping you with the \nknowledge and techniques necessary to optimize your data processing workflows. From the inner \nworkings of the Catalyst optimizer to the intricacies of different types of joins, we will explore advanced \nSpark operations that empower you to harness the full potential of this powerful framework.\nThe chapter will cover the following topics:\n\u2022\t Different options to group data in Spark DataFrames.", "mimetype": "text/plain", "start_char_idx": 124206, "end_char_idx": 124983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d072a57f-da17-4ec5-8da5-66a9091b9d73": {"__data__": {"id_": "d072a57f-da17-4ec5-8da5-66a9091b9d73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d78fe297-fc46-4458-b14d-1456143799a2", "node_type": "1", "metadata": {}, "hash": "38154368014a341f55bf27415d8195b0f34d9ebeb284a53dc26595e21e884314", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52d8f2aa-cc48-49ff-8feb-e959db97b6cb", "node_type": "1", "metadata": {}, "hash": "aba1f56908025b79244f1a9a2f759f0c39dfa67157889844052644f2d04cd1b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the inner \nworkings of the Catalyst optimizer to the intricacies of different types of joins, we will explore advanced \nSpark operations that empower you to harness the full potential of this powerful framework.\nThe chapter will cover the following topics:\n\u2022\t Different options to group data in Spark DataFrames.\n\u2022\t Various types of joins in Spark, including inner join, left join, right join, outer join, cross join, \nbroadcast join, and shuffle join, each with its unique use cases and implications\n\u2022\t Shuffle and broadcast joins, with a focus on broadcast hash joins and shuffle sort-merge joins, \nalong with their applications and optimization strategies\n\u2022\t Reading and writing data to disk in Spark using different data formats, such as CSV, Parquet, \nand others.\n\u2022\t Using Spark SQL for different operations\n\u2022\t The Catalyst optimizer,", "mimetype": "text/plain", "start_char_idx": 124666, "end_char_idx": 125510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52d8f2aa-cc48-49ff-8feb-e959db97b6cb": {"__data__": {"id_": "52d8f2aa-cc48-49ff-8feb-e959db97b6cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d072a57f-da17-4ec5-8da5-66a9091b9d73", "node_type": "1", "metadata": {}, "hash": "b2dd119c96ac63de3d5c3f0c0ed09b2cbb7f784c4c798acbebf97f820475a8e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fde529aa-0156-4c5b-994a-b64edb137a6e", "node_type": "1", "metadata": {}, "hash": "d7f1effa995a1141f813a9c3ec98718302d1d38f027704ee1ce0f6fa5bfdb8c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Using Spark SQL for different operations\n\u2022\t The Catalyst optimizer, a pivotal component in Spark\u2019s query execution engine that employs \nrule-based and cost-based optimizations to enhance query performance\n\u2022\t The distinction between narrow and wide transformations in Spark and when to use each type \nto achieve optimal parallelism and resource efficiency\n\u2022\t Data persistence and caching techniques to reduce recomputation and expedite data processing, \nwith best practices for efficient memory management\n\u2022\t Data partitioning through repartition and coalesce, and how to use these operations to balance \nworkloads and optimize data distribution\n\nAdvanced Operations and Optimizations in Spark\n\n\u2022\t User-defined functions (UDFs) and custom functions, which allow you to implement specialized \ndata processing logic,", "mimetype": "text/plain", "start_char_idx": 125440, "end_char_idx": 126256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fde529aa-0156-4c5b-994a-b64edb137a6e": {"__data__": {"id_": "fde529aa-0156-4c5b-994a-b64edb137a6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52d8f2aa-cc48-49ff-8feb-e959db97b6cb", "node_type": "1", "metadata": {}, "hash": "aba1f56908025b79244f1a9a2f759f0c39dfa67157889844052644f2d04cd1b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7d87110-7365-4065-90e1-354cc47b22c0", "node_type": "1", "metadata": {}, "hash": "356d73e17ff720875047d727d9bb3e2a50f62d33137e5ddb0e79edaaa789e36f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with best practices for efficient memory management\n\u2022\t Data partitioning through repartition and coalesce, and how to use these operations to balance \nworkloads and optimize data distribution\n\nAdvanced Operations and Optimizations in Spark\n\n\u2022\t User-defined functions (UDFs) and custom functions, which allow you to implement specialized \ndata processing logic, as well as when and how to leverage them effectively\n\u2022\t Performing advanced optimizations in Spark using the Catalyst optimizer and Adaptive Query \nExecution (AQE)\n\u2022\t Data-based optimization techniques and their benefits\nEach section will provide in-depth insights, practical examples, and best practices, ensuring you are \nwell-equipped to handle complex data processing challenges in Apache Spark. By the end of this \nchapter, you will possess the knowledge and skills needed to harness the advanced capabilities of \nSpark and unlock its full potential for your data-driven endeavors.", "mimetype": "text/plain", "start_char_idx": 125896, "end_char_idx": 126843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7d87110-7365-4065-90e1-354cc47b22c0": {"__data__": {"id_": "b7d87110-7365-4065-90e1-354cc47b22c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fde529aa-0156-4c5b-994a-b64edb137a6e", "node_type": "1", "metadata": {}, "hash": "d7f1effa995a1141f813a9c3ec98718302d1d38f027704ee1ce0f6fa5bfdb8c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5", "node_type": "1", "metadata": {}, "hash": "d32f959a5bca7ebfdadb353220f14f6e47b68481d63c7ceb28493c3e9c86b83d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "practical examples, and best practices, ensuring you are \nwell-equipped to handle complex data processing challenges in Apache Spark. By the end of this \nchapter, you will possess the knowledge and skills needed to harness the advanced capabilities of \nSpark and unlock its full potential for your data-driven endeavors.\nGrouping data in Spark and different Spark joins\nWe will start with one of the most important data manipulation techniques: grouping and joining \ndata. When we are doing data exploration, grouping data based on different criteria becomes essential \nto data analysis. We will look at how we can group different data using groupBy.\nUsing groupBy in a DataFrame\nWe can group data in a DataFrame based on different criteria \u2013 for example, we can group data \nbased on different columns in a DataFrame. We can also apply different aggregations, such as sum or \naverage, to this grouped data to get a holistic view of data slices.", "mimetype": "text/plain", "start_char_idx": 126523, "end_char_idx": 127467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5": {"__data__": {"id_": "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7d87110-7365-4065-90e1-354cc47b22c0", "node_type": "1", "metadata": {}, "hash": "356d73e17ff720875047d727d9bb3e2a50f62d33137e5ddb0e79edaaa789e36f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df", "node_type": "1", "metadata": {}, "hash": "eb1fc7c056223468415c0a677805a1a4edd76c5e11a23ef2408957f1b93b5d4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will look at how we can group different data using groupBy.\nUsing groupBy in a DataFrame\nWe can group data in a DataFrame based on different criteria \u2013 for example, we can group data \nbased on different columns in a DataFrame. We can also apply different aggregations, such as sum or \naverage, to this grouped data to get a holistic view of data slices.\nFor this purpose, in Spark, we have the groupBy operation. The groupBy operation is similar to \ngroupBy in SQL in that we can do group-wise operations on these grouped datasets. Moreover, \nwe can specify multiple groupBy criteria in a single groupBy statement. The following example \nshows how to use groupBy in PySpark. We will use the DataFrame salary data we created in the \nprevious chapter.", "mimetype": "text/plain", "start_char_idx": 127111, "end_char_idx": 127863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df": {"__data__": {"id_": "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5", "node_type": "1", "metadata": {}, "hash": "d32f959a5bca7ebfdadb353220f14f6e47b68481d63c7ceb28493c3e9c86b83d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe", "node_type": "1", "metadata": {}, "hash": "a68d998be9e4d8e225ca13d5f35b6998b62ca86d335bcb7afe44f28fb34921ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this purpose, in Spark, we have the groupBy operation. The groupBy operation is similar to \ngroupBy in SQL in that we can do group-wise operations on these grouped datasets. Moreover, \nwe can specify multiple groupBy criteria in a single groupBy statement. The following example \nshows how to use groupBy in PySpark. We will use the DataFrame salary data we created in the \nprevious chapter.\nIn the following groupBy statement, we are grouping the salary data based on the Department column:\nsalary_data.groupby('Department')\nAs a result, this operation returns a grouped data object that has been grouped by the Department column:\n<pyspark.sql.group.GroupedData at 0x7fc8495a3c10>\nThis can be assigned to a separate DataFrame and more operations can be done on this data. All the \naggregate operations can also be used for different groups of a DataFrame.", "mimetype": "text/plain", "start_char_idx": 127468, "end_char_idx": 128328, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe": {"__data__": {"id_": "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df", "node_type": "1", "metadata": {}, "hash": "eb1fc7c056223468415c0a677805a1a4edd76c5e11a23ef2408957f1b93b5d4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bd26748-b23b-495b-aae8-438cd255d06c", "node_type": "1", "metadata": {}, "hash": "dc599b9130bef3b02020f9b3b2f51a4e49d36610910cc646d5942c86a4ca4826", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the \naggregate operations can also be used for different groups of a DataFrame.\nWe will use the following statement to get the average salary across different departments in our \nsalary_data DataFrame:\nsalary_data.groupby(\u2018Department\u2019).avg().show()\n\nGrouping data in Spark and different Spark joins\n\nHere\u2019s the result:\n+----------+------------------+\n|Department| avg(Salary)|\n+----------+------------------+\n| null|3750.0|\n| Sales|2600.0|\n| Field-eng| 4166.666666666667|\n| Finance|3333.3333333333335|\n+----------+------------------+ \nIn this example, we can see that each department\u2019s average salary is calculated based on the salary \ncolumn of the salary_data DataFrame. All four departments, including null (since we had null \nvalues in our DataFrame), are included in the resulting DataFrame.", "mimetype": "text/plain", "start_char_idx": 128245, "end_char_idx": 129045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bd26748-b23b-495b-aae8-438cd255d06c": {"__data__": {"id_": "6bd26748-b23b-495b-aae8-438cd255d06c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe", "node_type": "1", "metadata": {}, "hash": "a68d998be9e4d8e225ca13d5f35b6998b62ca86d335bcb7afe44f28fb34921ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fe145b0-c0a5-4220-b961-c08a3da9ca00", "node_type": "1", "metadata": {}, "hash": "04a05553ce395e695addc85135fd6319f3bcd52c1c54dac9c83eb16d2c938870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All four departments, including null (since we had null \nvalues in our DataFrame), are included in the resulting DataFrame.\nNow, let\u2019s take a look at how we can apply complex groupBy operations to data in PySpark DataFrames.\nA complex groupBy statement\ngroupBy can be used in complex data operations, such as multiple aggregations within a single \ngroupBy statement.\nIn the following code snippet, we are going to use groupBy by taking a sum of the salary column \nfor each department. Then, we will round off the sum(Salary) column that we just created to two \ndigits after a decimal. After, we will rename the sum(Salary) column back to Salary.", "mimetype": "text/plain", "start_char_idx": 128922, "end_char_idx": 129567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fe145b0-c0a5-4220-b961-c08a3da9ca00": {"__data__": {"id_": "5fe145b0-c0a5-4220-b961-c08a3da9ca00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bd26748-b23b-495b-aae8-438cd255d06c", "node_type": "1", "metadata": {}, "hash": "dc599b9130bef3b02020f9b3b2f51a4e49d36610910cc646d5942c86a4ca4826", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31d9931b-bf83-4fed-b610-b3de97494e34", "node_type": "1", "metadata": {}, "hash": "baf6b8631005c0159ee11b27a9842901bd83648a982c4e8724fd4eb976a5779a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the following code snippet, we are going to use groupBy by taking a sum of the salary column \nfor each department. Then, we will round off the sum(Salary) column that we just created to two \ndigits after a decimal. After, we will rename the sum(Salary) column back to Salary. All of these \noperations are being done in a single groupBy statement:\nfrom pyspark.sql.functions import col, round\nsalary_data.groupBy('Department')\\\n.sum('Salary')\\\n.withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\\\n.withColumnRenamed('sum(Salary)', 'Salary')\\\n.orderBy('Department')\\\n.show()\nAs a result, we will see the following DataFrame showing the aggregated sum of the Salary column \nbased on each department:\n+----------+------------------+\n|Department|sum(Salary)|\n+----------+------------------+\n| null|7500|\n| Field-eng|12500|\n| Finance|10000|\n| Sales|5200|\n+----------+------------------+\n\nAdvanced Operations and Optimizations in Spark\n\nIn this example, we can see that each department\u2019s total salary is calculated in a new column named \nsum(Salary), after which we round this total up to two decimal places.", "mimetype": "text/plain", "start_char_idx": 129289, "end_char_idx": 130397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31d9931b-bf83-4fed-b610-b3de97494e34": {"__data__": {"id_": "31d9931b-bf83-4fed-b610-b3de97494e34", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fe145b0-c0a5-4220-b961-c08a3da9ca00", "node_type": "1", "metadata": {}, "hash": "04a05553ce395e695addc85135fd6319f3bcd52c1c54dac9c83eb16d2c938870", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bb40e13-bd0f-4554-9d77-37c1049a86cf", "node_type": "1", "metadata": {}, "hash": "1b89a15ce844408f94b70e1cb60fcec0062b79a8baaf527f740dfb6fafea8726", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next statement, we \nrename the sum(Salary) column back to Salary and then sort this resulting DataFrame based \non Department. In the resulting DataFrame, we can see that each department\u2019s sum of salaries is \ncalculated in the new column.\nNow that we know how to group data using different aggregations, let\u2019s take a look at how we can join \ntwo DataFrames together in Spark.\nJoining DataFrames in Spark\nJoin operations are fundamental in data processing tasks and are a core component of Apache Spark. \nSpark provides several types of joins to combine data from different DataFrames or datasets. In this \nsection, we will explore different Spark join operations and when to use each type.\nJoin operations are used to combine data from two or more DataFrames based on a common column. \nThese operations are essential for tasks such as merging datasets, aggregating information, and \nperforming relational operations.", "mimetype": "text/plain", "start_char_idx": 130398, "end_char_idx": 131320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bb40e13-bd0f-4554-9d77-37c1049a86cf": {"__data__": {"id_": "1bb40e13-bd0f-4554-9d77-37c1049a86cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31d9931b-bf83-4fed-b610-b3de97494e34", "node_type": "1", "metadata": {}, "hash": "baf6b8631005c0159ee11b27a9842901bd83648a982c4e8724fd4eb976a5779a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15bb5fa7-cd60-4856-80e8-c097e50c0891", "node_type": "1", "metadata": {}, "hash": "17723fd35e93df91b06cdf2a51ebef34c9d570fe0e720f7aa197461770fea1e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark provides several types of joins to combine data from different DataFrames or datasets. In this \nsection, we will explore different Spark join operations and when to use each type.\nJoin operations are used to combine data from two or more DataFrames based on a common column. \nThese operations are essential for tasks such as merging datasets, aggregating information, and \nperforming relational operations.\nIn Spark, the primary syntax for performing joins is using the .join() method, which takes the \nfollowing parameters:\n\u2022\t other: The other DataFrame to join with\n\u2022\t on: The column(s) on which to join the DataFrames\n\u2022\t how: The type of join to perform (inner, outer, left, or right)\n\u2022\t suffixes: Suffixes to add to columns with the same name in both DataFrames\nThese parameters are used in the main syntax of the join operation, as follows:\nDataframe1.join(Dataframe2, on, how)\nHere, Dataframe1 would be on the left-hand side of the join and Dataframe2 would be on the \nright-hand side of the join.", "mimetype": "text/plain", "start_char_idx": 130908, "end_char_idx": 131917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15bb5fa7-cd60-4856-80e8-c097e50c0891": {"__data__": {"id_": "15bb5fa7-cd60-4856-80e8-c097e50c0891", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bb40e13-bd0f-4554-9d77-37c1049a86cf", "node_type": "1", "metadata": {}, "hash": "1b89a15ce844408f94b70e1cb60fcec0062b79a8baaf527f740dfb6fafea8726", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7946f54-1cd8-40fe-aeed-5a5996f21735", "node_type": "1", "metadata": {}, "hash": "ba23630d9356be1eee45e52924c83a490d8e967fcfd7cf2debfe918b4a90b4db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DataFrames or datasets can be joined based on common columns within a DataFrame, and the result \nof a join query is a new DataFrame.\nWe will demonstrate the join operation on two new DataFrames. First, let\u2019s create these DataFrames. \nThe first DataFrame is called salary_data_with_id:\nsalary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n(2, \"Robert\", \"Sales\", 4000), \\\n(3, \"Maria\", \"Finance\", 3500), \\\n\nJoining DataFrames in Spark\n\n(4, \"Michael\", \"Sales\", 3000), \\\n(5, \"Kelly\", \"Finance\", 3500), \\\n(6, \"Kate\", \"Finance\", 3000), \\\n(7, \"Martin\", \"Finance\", 3500), \\\n(8, \"Kiran\", \"Sales\", 2200), \\\n]\ncolumns= [\"ID\", \"Employee\", \"Department\",", "mimetype": "text/plain", "start_char_idx": 131918, "end_char_idx": 132561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7946f54-1cd8-40fe-aeed-5a5996f21735": {"__data__": {"id_": "f7946f54-1cd8-40fe-aeed-5a5996f21735", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15bb5fa7-cd60-4856-80e8-c097e50c0891", "node_type": "1", "metadata": {}, "hash": "17723fd35e93df91b06cdf2a51ebef34c9d570fe0e720f7aa197461770fea1e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92bf1c5c-34a2-4149-b604-b4edc88ad719", "node_type": "1", "metadata": {}, "hash": "01af9af2aa5f21dd92ee17983901f6c1e4cec99e896fb534eb5f6abc2ca0a488", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Michael\", \"Sales\", 3000), \\\n(5, \"Kelly\", \"Finance\", 3500), \\\n(6, \"Kate\", \"Finance\", 3000), \\\n(7, \"Martin\", \"Finance\", 3500), \\\n(8, \"Kiran\", \"Sales\", 2200), \\\n]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_\nid, schema = columns)\nsalary_data_with_id.show()\nThe resulting DataFrame, named salary_data_with_id,", "mimetype": "text/plain", "start_char_idx": 132359, "end_char_idx": 132741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92bf1c5c-34a2-4149-b604-b4edc88ad719": {"__data__": {"id_": "92bf1c5c-34a2-4149-b604-b4edc88ad719", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7946f54-1cd8-40fe-aeed-5a5996f21735", "node_type": "1", "metadata": {}, "hash": "ba23630d9356be1eee45e52924c83a490d8e967fcfd7cf2debfe918b4a90b4db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72ba7687-abff-4483-8f87-33301c608868", "node_type": "1", "metadata": {}, "hash": "317d359b226d59f6e826e6b4ddad75cd09f195e4181b9d1bb8bbaf4f71efaeba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Finance\", 3000), \\\n(7, \"Martin\", \"Finance\", 3500), \\\n(8, \"Kiran\", \"Sales\", 2200), \\\n]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_\nid, schema = columns)\nsalary_data_with_id.show()\nThe resulting DataFrame, named salary_data_with_id, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John| Field-eng|3500|\n| 2 | Robert | Sales|4000|\n| 3 | Maria| Finance|3500|\n| 4 | Michael| Sales|3000|\n| 5 | Kelly| Finance|3500|\n| 6 | Kate| Finance|3000|\n| 7 | Martin | Finance|3500|\n| 8 | Kiran| Sales|2200|\n+---+--------+----------+------+\nNow,", "mimetype": "text/plain", "start_char_idx": 132433, "end_char_idx": 133111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72ba7687-abff-4483-8f87-33301c608868": {"__data__": {"id_": "72ba7687-abff-4483-8f87-33301c608868", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92bf1c5c-34a2-4149-b604-b4edc88ad719", "node_type": "1", "metadata": {}, "hash": "01af9af2aa5f21dd92ee17983901f6c1e4cec99e896fb534eb5f6abc2ca0a488", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31120862-6715-4546-81e3-d43f5ebbf3db", "node_type": "1", "metadata": {}, "hash": "f4e1c6c39c0ca844a6c0a75bba946c6a3661c6eb8502b9c721d87fd043050592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we\u2019ll create another DataFrame named employee_data:\nemployee_data = [(1, \"NY\", \"M\"), \\\n(2, \"NC\", \"M\"), \\\n(3, \"NY\", \"F\"), \\\n(4, \"TX\", \"M\"), \\\n(5, \"NY\", \"F\"), \\\n(6, \"AZ\", \"F\") \\\n]\ncolumns= [\"ID\", \"State\", \"Gender\"]\nemployee_data = spark.createDataFrame(data = employee_data, schema = \ncolumns)\nemployee_data.show()\n\nAdvanced Operations and Optimizations in Spark\n\nThe resulting DataFrame, named employee_data, looks like this:\n+---+-----+------+\n| ID|State|Gender|\n+---+-----+------+\n|1|NY|M|\n|2|NC|M|\n|3|NY|F|\n|4|TX|M|\n|5|NY|F|\n|6|AZ|F|\n+---+-----+------+\nNow,", "mimetype": "text/plain", "start_char_idx": 133112, "end_char_idx": 133671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31120862-6715-4546-81e3-d43f5ebbf3db": {"__data__": {"id_": "31120862-6715-4546-81e3-d43f5ebbf3db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72ba7687-abff-4483-8f87-33301c608868", "node_type": "1", "metadata": {}, "hash": "317d359b226d59f6e826e6b4ddad75cd09f195e4181b9d1bb8bbaf4f71efaeba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8", "node_type": "1", "metadata": {}, "hash": "9864325c72532cb5a848de541eb340bf4613c1ac93e3ae4be01c1d83c5d97bce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "named employee_data, looks like this:\n+---+-----+------+\n| ID|State|Gender|\n+---+-----+------+\n|1|NY|M|\n|2|NC|M|\n|3|NY|F|\n|4|TX|M|\n|5|NY|F|\n|6|AZ|F|\n+---+-----+------+\nNow, let\u2019s suppose we want to join these two DataFrames together based on the ID column.\nAs we mentioned earlier, Spark offers different types of join operations. We will explore some of them \nin this chapter. Let\u2019s start with inner joins.\nInner joins\nAn inner join is used when we want to join two DataFrames based on values that are common in \nboth DataFrames. Any value that doesn\u2019t exist in any one of the DataFrames would not be part of the \nresulting DataFrame. By default, the join type is an inner join in Spark.", "mimetype": "text/plain", "start_char_idx": 133499, "end_char_idx": 134187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8": {"__data__": {"id_": "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31120862-6715-4546-81e3-d43f5ebbf3db", "node_type": "1", "metadata": {}, "hash": "f4e1c6c39c0ca844a6c0a75bba946c6a3661c6eb8502b9c721d87fd043050592", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06", "node_type": "1", "metadata": {}, "hash": "d40833f2fd1bf96ee9699d1243a435831c1dd4e1d27091a4945e460f0418785b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will explore some of them \nin this chapter. Let\u2019s start with inner joins.\nInner joins\nAn inner join is used when we want to join two DataFrames based on values that are common in \nboth DataFrames. Any value that doesn\u2019t exist in any one of the DataFrames would not be part of the \nresulting DataFrame. By default, the join type is an inner join in Spark.\nUse case\nInner joins are useful for merging data when you are interested in common elements in both DataFrames \n\u2013 for example, joining sales data with customer data to see which customers made a purchase.\nThe following code illustrates how we can use an inner join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"inner\").show()\nThe resulting DataFrame now contains all the columns of both DataFrames \u2013 salary_data_with_\nid and employee_data \u2013 joined together in a single DataFrame.", "mimetype": "text/plain", "start_char_idx": 133830, "end_char_idx": 134744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06": {"__data__": {"id_": "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8", "node_type": "1", "metadata": {}, "hash": "9864325c72532cb5a848de541eb340bf4613c1ac93e3ae4be01c1d83c5d97bce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76fbad2e-6400-438b-a560-be46e856503e", "node_type": "1", "metadata": {}, "hash": "4fdac261dd08f51235ffe2ab9710b1348e0bd079addd30f0564275155e649288", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"inner\").show()\nThe resulting DataFrame now contains all the columns of both DataFrames \u2013 salary_data_with_\nid and employee_data \u2013 joined together in a single DataFrame. It only includes rows that are \ncommon in both DataFrames.", "mimetype": "text/plain", "start_char_idx": 134393, "end_char_idx": 134803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76fbad2e-6400-438b-a560-be46e856503e": {"__data__": {"id_": "76fbad2e-6400-438b-a560-be46e856503e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06", "node_type": "1", "metadata": {}, "hash": "d40833f2fd1bf96ee9699d1243a435831c1dd4e1d27091a4945e460f0418785b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "918faf00-ebb7-492d-b043-0aff7fedfbe5", "node_type": "1", "metadata": {}, "hash": "62c10b304443e247e35ff49a9b8fd0ce7ab9b581cffab7984b6fef852d765f5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates how we can use an inner join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"inner\").show()\nThe resulting DataFrame now contains all the columns of both DataFrames \u2013 salary_data_with_\nid and employee_data \u2013 joined together in a single DataFrame. It only includes rows that are \ncommon in both DataFrames. Here\u2019s what it looks like:\n+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|1|John| Field-eng|3500|1|NY|M|\n|2|Robert|Sales|4000|2|NC|M|\n|3|Maria|Finance|3500|3|NY|F|\n|4| Michael|Sales|3000|4|TX|M|\n|5|Kelly|Finance|3500|5|NY|F|\n|6|Kate|Finance|3000|6|AZ|F|\n+---+--------+----------+------+---+-----+------+\n\nJoining DataFrames in Spark\n\nYou will notice that the how parameter defines the type of join that is being done in this statement.", "mimetype": "text/plain", "start_char_idx": 134393, "end_char_idx": 135343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "918faf00-ebb7-492d-b043-0aff7fedfbe5": {"__data__": {"id_": "918faf00-ebb7-492d-b043-0aff7fedfbe5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76fbad2e-6400-438b-a560-be46e856503e", "node_type": "1", "metadata": {}, "hash": "4fdac261dd08f51235ffe2ab9710b1348e0bd079addd30f0564275155e649288", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83059430-e440-42ad-aa3e-a9cfc4a3be03", "node_type": "1", "metadata": {}, "hash": "239adda0439b8ddde07f46bcaba88711598042635f12e96305910ffb9cd166de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, it says inner because we wanted the DataFrames to join based on an inner join. We can \nalso see that IDs 7 and 8 are missing. The reason is that the employee_data DataFrame did not \ncontain IDs 7 and 8. Since we\u2019re using an inner join, it only joins data based on common data elements \nin both DataFrames. Any data that is not present in either one of the DataFrames will not be part of \nthe resulting DataFrame.\nNext, we will explore outer joins.\nOuter joins\nAn outer join, also known as a full outer join, returns all the rows from both DataFrames, filling in \nmissing values with null.\nWe should use an outer join when we want to join two DataFrames based on values that exist in both \nDataFrames, regardless of whether they don\u2019t exist in the other DataFrame.", "mimetype": "text/plain", "start_char_idx": 135345, "end_char_idx": 136119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83059430-e440-42ad-aa3e-a9cfc4a3be03": {"__data__": {"id_": "83059430-e440-42ad-aa3e-a9cfc4a3be03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "918faf00-ebb7-492d-b043-0aff7fedfbe5", "node_type": "1", "metadata": {}, "hash": "62c10b304443e247e35ff49a9b8fd0ce7ab9b581cffab7984b6fef852d765f5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e984b3bc-8c6a-4f5f-b11c-26f258848ce7", "node_type": "1", "metadata": {}, "hash": "242adfb4424049a0163f8daee8e6e89f1507cfbce99b26602ba712874444fcb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, we will explore outer joins.\nOuter joins\nAn outer join, also known as a full outer join, returns all the rows from both DataFrames, filling in \nmissing values with null.\nWe should use an outer join when we want to join two DataFrames based on values that exist in both \nDataFrames, regardless of whether they don\u2019t exist in the other DataFrame. Any values that exist in \nany one of the DataFrames would be part of the resulting DataFrame.\nUse case\nOuter joins are suitable for situations where you want to include all records from both DataFrames \nwhile accommodating unmatched values \u2013 for example, when merging employee data with project data \nto see which employees are assigned to which projects, including those not currently assigned to any.", "mimetype": "text/plain", "start_char_idx": 135769, "end_char_idx": 136522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e984b3bc-8c6a-4f5f-b11c-26f258848ce7": {"__data__": {"id_": "e984b3bc-8c6a-4f5f-b11c-26f258848ce7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83059430-e440-42ad-aa3e-a9cfc4a3be03", "node_type": "1", "metadata": {}, "hash": "239adda0439b8ddde07f46bcaba88711598042635f12e96305910ffb9cd166de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4860aa7e-93d2-4057-9273-b61143af6859", "node_type": "1", "metadata": {}, "hash": "2ff9be8a0cc6b2d402437f33a4b96da304388d933aad38ebc11028e3d001ae33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Any values that exist in \nany one of the DataFrames would be part of the resulting DataFrame.\nUse case\nOuter joins are suitable for situations where you want to include all records from both DataFrames \nwhile accommodating unmatched values \u2013 for example, when merging employee data with project data \nto see which employees are assigned to which projects, including those not currently assigned to any.\nThe following code illustrates how we use an outer join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"outer\").show()\nThe resulting DataFrame contains data for all the employees in the salary_data_with_id and \nemployee_data DataFrames.", "mimetype": "text/plain", "start_char_idx": 136120, "end_char_idx": 136834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4860aa7e-93d2-4057-9273-b61143af6859": {"__data__": {"id_": "4860aa7e-93d2-4057-9273-b61143af6859", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e984b3bc-8c6a-4f5f-b11c-26f258848ce7", "node_type": "1", "metadata": {}, "hash": "242adfb4424049a0163f8daee8e6e89f1507cfbce99b26602ba712874444fcb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e27568c8-9294-4ec9-bc11-5208a849a15e", "node_type": "1", "metadata": {}, "hash": "01303c5d8ac0ea2f5cd86664029ccca9489accf9b168660cb504367916d4c378", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates how we use an outer join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"outer\").show()\nThe resulting DataFrame contains data for all the employees in the salary_data_with_id and \nemployee_data DataFrames. Here\u2019s what it looks like:\n+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|1|John| Field-eng|3500|1|NY|M|\n|2|Robert|Sales|4000|2|NC|M|\n|3|Maria|Finance|3500|3|NY|F|\n|4| Michael|Sales|3000|4|TX|M|\n|5|Kelly|Finance|3500|5|NY|F|\n|6|Kate|Finance|3000|6|AZ|F|\n|7|Martin|Finance|3500|null| null|null|\n|8|Kiran|Sales|2200|null| null|null|\n+---+--------+----------+------+----+-----+------+\nYou will notice that the how parameter has changed and says outer.", "mimetype": "text/plain", "start_char_idx": 136523, "end_char_idx": 137388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e27568c8-9294-4ec9-bc11-5208a849a15e": {"__data__": {"id_": "e27568c8-9294-4ec9-bc11-5208a849a15e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4860aa7e-93d2-4057-9273-b61143af6859", "node_type": "1", "metadata": {}, "hash": "2ff9be8a0cc6b2d402437f33a4b96da304388d933aad38ebc11028e3d001ae33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67db6a3f-4b76-4b66-960a-b52084360442", "node_type": "1", "metadata": {}, "hash": "0ad0203d458225294562349d49355d29dfef00e483166496dc8fe548bcdc8e13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the resulting DataFrame, IDs \n7 and 8 are now present. However, also notice that the ID, State, and Gender columns for IDs 7 \n\nAdvanced Operations and Optimizations in Spark\n\nand 8 are null. The reason is that the employee_data DataFrame did not contain IDs 7 and 8. \nAny data not present in either of the DataFrames would be part of the resulting DataFrame, but the \ncorresponding columns would be null for the DataFrame that this was not present, as shown in the \ncase of employee IDs 7 and 8.\nNext, we will explore left joins.\nLeft joins\nA left join returns all the rows from the left DataFrame and the matched rows from the right DataFrame. \nIf there is no match in the right DataFrame, the result will contain null values.", "mimetype": "text/plain", "start_char_idx": 137389, "end_char_idx": 138119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67db6a3f-4b76-4b66-960a-b52084360442": {"__data__": {"id_": "67db6a3f-4b76-4b66-960a-b52084360442", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e27568c8-9294-4ec9-bc11-5208a849a15e", "node_type": "1", "metadata": {}, "hash": "01303c5d8ac0ea2f5cd86664029ccca9489accf9b168660cb504367916d4c378", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f149ad65-76ea-49ad-b1ec-e8be8ec89a23", "node_type": "1", "metadata": {}, "hash": "3fff3676339b39383bc4aa6e1641b6bdb23abc48a3618d04a051903b2b889a26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, we will explore left joins.\nLeft joins\nA left join returns all the rows from the left DataFrame and the matched rows from the right DataFrame. \nIf there is no match in the right DataFrame, the result will contain null values.\nUse case\nLeft joins are handy when you want to keep all records from the left DataFrame and only the matching \nrecords from the right DataFrame \u2013 for instance, when merging customer data with transaction data \nto see which customers have made a purchase.\nThe following code illustrates how we can use a left join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"left\").show()\nThe resulting DataFrame contains all the data from the left DataFrame \u2013 that is, salary_data_\nwith_id.", "mimetype": "text/plain", "start_char_idx": 137888, "end_char_idx": 138672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f149ad65-76ea-49ad-b1ec-e8be8ec89a23": {"__data__": {"id_": "f149ad65-76ea-49ad-b1ec-e8be8ec89a23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67db6a3f-4b76-4b66-960a-b52084360442", "node_type": "1", "metadata": {}, "hash": "0ad0203d458225294562349d49355d29dfef00e483166496dc8fe548bcdc8e13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21d0cef0-d28d-4096-9c2d-c859a873286e", "node_type": "1", "metadata": {}, "hash": "1d2eb5b297e8f3db9903fd2534156c5b0cc3d54817fbe6024ca09fb37f4b6b8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates how we can use a left join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"left\").show()\nThe resulting DataFrame contains all the data from the left DataFrame \u2013 that is, salary_data_\nwith_id. It looks like this:\n+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|1|John| Field-eng|3500|1|NY|M|\n|2|Robert|Sales|4000|2|NC|M|\n|3|Maria|Finance|3500|3|NY|F|\n|4| Michael|Sales|3000|4|TX|M|\n|5|Kelly|Finance|3500|5|NY|F|\n|6|Kate|Finance|3000|6|AZ|F|\n|7|Martin|Finance|3500|null| null|null|\n|8|Kiran|Sales|2200|null| null|null|\n+---+--------+----------+------+----+-----+------+\nNote that the how parameter has changed and says left.", "mimetype": "text/plain", "start_char_idx": 138375, "end_char_idx": 139207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21d0cef0-d28d-4096-9c2d-c859a873286e": {"__data__": {"id_": "21d0cef0-d28d-4096-9c2d-c859a873286e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f149ad65-76ea-49ad-b1ec-e8be8ec89a23", "node_type": "1", "metadata": {}, "hash": "3fff3676339b39383bc4aa6e1641b6bdb23abc48a3618d04a051903b2b889a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65f9b2e0-b3e7-4056-88a4-859663f08582", "node_type": "1", "metadata": {}, "hash": "f40de5e1a308134dad8b84c312268e5bfb7c735ff2c220e20fc5935feb56dd33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, IDs 7 and 8 are present. However, \nalso notice that the ID, State, and Gender columns for IDs 7 and 8 are null. The reason is that \nthe employee_data DataFrame did not contain IDs 7 and 8. Since salary_data_with_id \nis the left DataFrame in the join statement, its values take priority in the join.\nAll records from the left DataFrame are present in the resulting DataFrame, and matching records \nfrom the right DataFrame are included. Non-matching entries in the right DataFrame are filled with \nnull values in the result.\n\nJoining DataFrames in Spark\n\nNext, we will explore right joins.\nRight joins\nA right join is similar to a left join, but it returns all the rows from the right DataFrame and the matched \nrows from the left DataFrame. Non-matching rows from the left DataFrame contain null values.", "mimetype": "text/plain", "start_char_idx": 139208, "end_char_idx": 140016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65f9b2e0-b3e7-4056-88a4-859663f08582": {"__data__": {"id_": "65f9b2e0-b3e7-4056-88a4-859663f08582", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21d0cef0-d28d-4096-9c2d-c859a873286e", "node_type": "1", "metadata": {}, "hash": "1d2eb5b297e8f3db9903fd2534156c5b0cc3d54817fbe6024ca09fb37f4b6b8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b8758c7-9793-4811-8d87-814dd645f6d7", "node_type": "1", "metadata": {}, "hash": "d572a04d4c548240fef8b1ca8c81bbd828941ffa50264236ce1b031662911257", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Non-matching entries in the right DataFrame are filled with \nnull values in the result.\n\nJoining DataFrames in Spark\n\nNext, we will explore right joins.\nRight joins\nA right join is similar to a left join, but it returns all the rows from the right DataFrame and the matched \nrows from the left DataFrame. Non-matching rows from the left DataFrame contain null values.\nUse case\nRight joins are the opposite of left joins and are used when you want to keep all records from the right \nDataFrame while including matching records from the left DataFrame.\nThe following code illustrates how to use a right join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"right\").show()\nThe resulting DataFrame contains all the data from the right-hand DataFrame \u2013 that is, employee_\ndata.", "mimetype": "text/plain", "start_char_idx": 139649, "end_char_idx": 140495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b8758c7-9793-4811-8d87-814dd645f6d7": {"__data__": {"id_": "4b8758c7-9793-4811-8d87-814dd645f6d7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65f9b2e0-b3e7-4056-88a4-859663f08582", "node_type": "1", "metadata": {}, "hash": "f40de5e1a308134dad8b84c312268e5bfb7c735ff2c220e20fc5935feb56dd33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "635520b9-4c10-44b4-b9ef-6e63f415f6df", "node_type": "1", "metadata": {}, "hash": "d719a535f3774431cc46713ed0ab6cb5d5903a4b9a7ba8704620813bf3391327", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code illustrates how to use a right join with the DataFrames we created earlier:\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID \n==employee_data.ID,\"right\").show()\nThe resulting DataFrame contains all the data from the right-hand DataFrame \u2013 that is, employee_\ndata. It looks like this:\n+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|1|John| Field-eng|3500|1|NY|M|\n|2|Robert|Sales|4000|2|NC|M|\n|3|Maria|Finance|3500|3|NY|F|\n|4| Michael|Sales|3000|4|TX|M|\n|5|Kelly|Finance|3500|5|NY|F|\n|6|Kate|Finance|3000|6|AZ|F|\n+---+--------+----------+------+---+-----+------+\nNotice the how parameter has changed and now says right.", "mimetype": "text/plain", "start_char_idx": 140200, "end_char_idx": 140953, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "635520b9-4c10-44b4-b9ef-6e63f415f6df": {"__data__": {"id_": "635520b9-4c10-44b4-b9ef-6e63f415f6df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b8758c7-9793-4811-8d87-814dd645f6d7", "node_type": "1", "metadata": {}, "hash": "d572a04d4c548240fef8b1ca8c81bbd828941ffa50264236ce1b031662911257", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19b239f1-aca6-4429-bb9c-8895798fc507", "node_type": "1", "metadata": {}, "hash": "6704c2e7d7b703ff479b309bcaf2f83c106cbc7b8b5f44e4bca421ac35a67308", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting DataFrame shows that \nIDs 7 and 8 are not present. The reason is that the employee_data DataFrame does not contain \nIDs 7 and 8. Since employee_data is the right DataFrame in the join statement, its values take \npriority in the join.\nAll records from the right DataFrame are present in the resulting DataFrame, and matching records \nfrom the left DataFrame are included. Non-matching entries in the left DataFrame are filled with \nnull values in the result.\nNext, we will explore cross joins.\nCross joins\nA cross join, also known as a Cartesian join, combines each row from the left DataFrame with every \nrow from the right DataFrame. This results in a large, Cartesian product DataFrame.\n\nAdvanced Operations and Optimizations in Spark\n\nUse case\nCross joins should be used with caution due to their potential for generating massive datasets.", "mimetype": "text/plain", "start_char_idx": 140954, "end_char_idx": 141810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19b239f1-aca6-4429-bb9c-8895798fc507": {"__data__": {"id_": "19b239f1-aca6-4429-bb9c-8895798fc507", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "635520b9-4c10-44b4-b9ef-6e63f415f6df", "node_type": "1", "metadata": {}, "hash": "d719a535f3774431cc46713ed0ab6cb5d5903a4b9a7ba8704620813bf3391327", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367", "node_type": "1", "metadata": {}, "hash": "7187a2075f94fcb86b0c16f37835dd278b0741dc3697daa9e03ae88896d61c91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, we will explore cross joins.\nCross joins\nA cross join, also known as a Cartesian join, combines each row from the left DataFrame with every \nrow from the right DataFrame. This results in a large, Cartesian product DataFrame.\n\nAdvanced Operations and Optimizations in Spark\n\nUse case\nCross joins should be used with caution due to their potential for generating massive datasets. They \nare typically used when you want to explore all possible combinations of data, such as when generating \ntest data.\nNext, we will explore the union option to join two DataFrames.\nUnion\nUnion is used to join two DataFrames that have a similar schema. To illustrate this, we will create \nanother DataFrame called salary_data_with_id_2 that contains some more values. The schema \nof this DataFrame is the same as the one for salary_data_with_id:\nsalary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n(2,", "mimetype": "text/plain", "start_char_idx": 141426, "end_char_idx": 142321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367": {"__data__": {"id_": "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19b239f1-aca6-4429-bb9c-8895798fc507", "node_type": "1", "metadata": {}, "hash": "6704c2e7d7b703ff479b309bcaf2f83c106cbc7b8b5f44e4bca421ac35a67308", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f93f9750-eb9d-4469-85f2-ca95c8aa1824", "node_type": "1", "metadata": {}, "hash": "e15d4f7fc61ed496ab3a618444f6eebbe2e5966402228bdf0878eeb431f047da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Union\nUnion is used to join two DataFrames that have a similar schema. To illustrate this, we will create \nanother DataFrame called salary_data_with_id_2 that contains some more values. The schema \nof this DataFrame is the same as the one for salary_data_with_id:\nsalary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n(2, \"Robert\", \"Sales\", 4000), \\\n(3, \"Aliya\", \"Finance\", 3500), \\\n(4, \"Nate\", \"Sales\", 3000), \\\n]\ncolumns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_\nid_2, schema = columns2)\nsalary_data_with_id_2.printSchema()\nsalary_data_with_id_2.show(truncate=False)\nAs a result,", "mimetype": "text/plain", "start_char_idx": 141995, "end_char_idx": 142655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f93f9750-eb9d-4469-85f2-ca95c8aa1824": {"__data__": {"id_": "f93f9750-eb9d-4469-85f2-ca95c8aa1824", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367", "node_type": "1", "metadata": {}, "hash": "7187a2075f94fcb86b0c16f37835dd278b0741dc3697daa9e03ae88896d61c91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90699bf6-8283-49e6-a729-33c82ff4a77e", "node_type": "1", "metadata": {}, "hash": "769fa6bb98931f78867814eb910b5c7202ef5d51d94ae89c7df5aebcb635513a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3500), \\\n(4, \"Nate\", \"Sales\", 3000), \\\n]\ncolumns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_\nid_2, schema = columns2)\nsalary_data_with_id_2.printSchema()\nsalary_data_with_id_2.show(truncate=False)\nAs a result, you will see the schema of the DataFrame first,", "mimetype": "text/plain", "start_char_idx": 142374, "end_char_idx": 142703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90699bf6-8283-49e6-a729-33c82ff4a77e": {"__data__": {"id_": "90699bf6-8283-49e6-a729-33c82ff4a77e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f93f9750-eb9d-4469-85f2-ca95c8aa1824", "node_type": "1", "metadata": {}, "hash": "e15d4f7fc61ed496ab3a618444f6eebbe2e5966402228bdf0878eeb431f047da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "064db35c-1d84-41e9-ad4a-91ba40ad0888", "node_type": "1", "metadata": {}, "hash": "e83b25124d8deb09ef07e31770e33fdd60d519b2ec75545ba39bf66e9c55d29d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Nate\", \"Sales\", 3000), \\\n]\ncolumns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_\nid_2, schema = columns2)\nsalary_data_with_id_2.printSchema()\nsalary_data_with_id_2.show(truncate=False)\nAs a result, you will see the schema of the DataFrame first, after which you will see the actual DataFrame \nand its values:\nroot\n |-- ID: long (nullable = true)\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1|John|Field-eng |3500|\n|2|Robert|Sales|4000|\n|3|Aliya|Finance|3500|\n|4|Nate|Sales|3000|\n+---+--------+----------+------+\n\nReading and writing data\n\nOnce we have this DataFrame,", "mimetype": "text/plain", "start_char_idx": 142387, "end_char_idx": 143199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "064db35c-1d84-41e9-ad4a-91ba40ad0888": {"__data__": {"id_": "064db35c-1d84-41e9-ad4a-91ba40ad0888", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90699bf6-8283-49e6-a729-33c82ff4a77e", "node_type": "1", "metadata": {}, "hash": "769fa6bb98931f78867814eb910b5c7202ef5d51d94ae89c7df5aebcb635513a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34f1216a-326d-4042-b7ae-82fc20263a7c", "node_type": "1", "metadata": {}, "hash": "b7478c6e3a0b7c9e6cdcb0ea9165af75f1427c9ff5c346a598daab1c99c93e9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can use the union() function to join the salary_data_with_\nid and salary_data_with_id_2 DataFrames together. The following example illustrates this:\nunionDF = salary_data_with_id.union(salary_data_with_id_2)\nunionDF.show(truncate=False)\nThe resulting DataFrame, named unionDF,", "mimetype": "text/plain", "start_char_idx": 143200, "end_char_idx": 143479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34f1216a-326d-4042-b7ae-82fc20263a7c": {"__data__": {"id_": "34f1216a-326d-4042-b7ae-82fc20263a7c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "064db35c-1d84-41e9-ad4a-91ba40ad0888", "node_type": "1", "metadata": {}, "hash": "e83b25124d8deb09ef07e31770e33fdd60d519b2ec75545ba39bf66e9c55d29d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89aea79b-b545-4b89-8247-6fdbfe3a63dd", "node_type": "1", "metadata": {}, "hash": "3e48a8f99d51eae850e637cf0c8617cdbbaae7c43e7c4f284db34e577b80447f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can use the union() function to join the salary_data_with_\nid and salary_data_with_id_2 DataFrames together. The following example illustrates this:\nunionDF = salary_data_with_id.union(salary_data_with_id_2)\nunionDF.show(truncate=False)\nThe resulting DataFrame, named unionDF, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1|John|Field-eng |3500|\n|2|Robert|Sales|4000|\n|3|Maria|Finance|3500|\n|4|Michael |Sales|3000|\n|5|Kelly|Finance|3500|\n|6|Kate|Finance|3000|\n|7|Martin|Finance|3500|\n|8|Kiran|Sales|2200|\n|1|John|Field-eng |3500|\n|2|Robert|Sales|4000|\n|3|Aliya|Finance|3500|\n|4|Nate|Sales|3000|\n+---+--------+----------+------+\nAs you can see,", "mimetype": "text/plain", "start_char_idx": 143200, "end_char_idx": 143918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89aea79b-b545-4b89-8247-6fdbfe3a63dd": {"__data__": {"id_": "89aea79b-b545-4b89-8247-6fdbfe3a63dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34f1216a-326d-4042-b7ae-82fc20263a7c", "node_type": "1", "metadata": {}, "hash": "b7478c6e3a0b7c9e6cdcb0ea9165af75f1427c9ff5c346a598daab1c99c93e9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd164969-2607-472d-a2ce-a2a0e078d1e7", "node_type": "1", "metadata": {}, "hash": "3e197af4c8a0d5ff2a4fff38da2db4685ef16fdd658a6328b88b041b2a1fef4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "both DataFrames are joined together and as a result, new rows are added to the resulting \nDataFrame. The last four rows are from salary_data_with_id_2 and were added to the rows \nof salary_data_with_id. This is another way to join two DataFrames together.\nIn this section, we explored different types of Spark joins and their appropriate use cases. Choosing the \nright join type is crucial to ensure efficient data processing in Spark, and understanding the implications \nof each type will help you make informed decisions in your data analysis and processing tasks.\nNow, let\u2019s look at how we can read and write data in Spark.\nReading and writing data\nWhen we work with Spark and do all the operations in Spark for data manipulation, one of the most \nimportant things that we need to do is read and write data to disk.", "mimetype": "text/plain", "start_char_idx": 143919, "end_char_idx": 144737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd164969-2607-472d-a2ce-a2a0e078d1e7": {"__data__": {"id_": "dd164969-2607-472d-a2ce-a2a0e078d1e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89aea79b-b545-4b89-8247-6fdbfe3a63dd", "node_type": "1", "metadata": {}, "hash": "3e48a8f99d51eae850e637cf0c8617cdbbaae7c43e7c4f284db34e577b80447f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f07533c3-5f01-4172-b371-ac2859efe5b5", "node_type": "1", "metadata": {}, "hash": "384e163243b2253ccda977bedb3515b658ed823e0a42da7ffe71b1ff4a877a07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, let\u2019s look at how we can read and write data in Spark.\nReading and writing data\nWhen we work with Spark and do all the operations in Spark for data manipulation, one of the most \nimportant things that we need to do is read and write data to disk. Remember, Spark is an in-memory \nframework, which means that all the operations take place in the memory of the compute or cluster. \nOnce these operations are completed, we\u2019ll want to write that data to disk. Similarly, before we \nmanipulate any data, we\u2019ll likely need to read data from disk as well.\n\nAdvanced Operations and Optimizations in Spark\n\nThere are several data formats that Spark supports for reading and writing different types of data files. \nWe will discuss the following formats in this chapter.", "mimetype": "text/plain", "start_char_idx": 144486, "end_char_idx": 145250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f07533c3-5f01-4172-b371-ac2859efe5b5": {"__data__": {"id_": "f07533c3-5f01-4172-b371-ac2859efe5b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd164969-2607-472d-a2ce-a2a0e078d1e7", "node_type": "1", "metadata": {}, "hash": "3e197af4c8a0d5ff2a4fff38da2db4685ef16fdd658a6328b88b041b2a1fef4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2f7e06c-d897-4881-87fc-4cc7fa506c0e", "node_type": "1", "metadata": {}, "hash": "85d96e10bfad3fb7c1c4fb181564d7143784e024962b89b90325982afce843f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once these operations are completed, we\u2019ll want to write that data to disk. Similarly, before we \nmanipulate any data, we\u2019ll likely need to read data from disk as well.\n\nAdvanced Operations and Optimizations in Spark\n\nThere are several data formats that Spark supports for reading and writing different types of data files. \nWe will discuss the following formats in this chapter.\n\u2022\t Comma Separated Values (CSV)\n\u2022\t Parquet\n\u2022\t Optimized Row Columnar (ORC)\nPlease note that these are not the only formats that Spark supports but this is a very popular subset of \nformats. A lot of other formats are also supported by Spark, such as Avro, text, JDBC, Delta, and others.\nIn the next section, we will discuss the CSV file format and how to read and write CSV format data files.", "mimetype": "text/plain", "start_char_idx": 144871, "end_char_idx": 145643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2f7e06c-d897-4881-87fc-4cc7fa506c0e": {"__data__": {"id_": "f2f7e06c-d897-4881-87fc-4cc7fa506c0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f07533c3-5f01-4172-b371-ac2859efe5b5", "node_type": "1", "metadata": {}, "hash": "384e163243b2253ccda977bedb3515b658ed823e0a42da7ffe71b1ff4a877a07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a770de1-cef2-494e-98f9-06ae5abd048b", "node_type": "1", "metadata": {}, "hash": "f1d194bea658a1adf171ed4cd20b21ab2281f278bc9a3c85ad406526da1384ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A lot of other formats are also supported by Spark, such as Avro, text, JDBC, Delta, and others.\nIn the next section, we will discuss the CSV file format and how to read and write CSV format data files.\nReading and writing CSV files\nIn this section, we will discuss how to read and write data from the CSV file format. In this file format, \ndata is separated by commas. This is a very popular data format because of its ease of use and simplicity.\nLet\u2019s look at how to write CSV files with Spark by running the following code:\nsalary_data_with_id.write.csv('salary_data.csv', mode='overwrite', \nheader=True)\nspark.read.csv('/salary_data.csv', header=True).show()\nThe resulting DataFrame, named salary_data_with_id,", "mimetype": "text/plain", "start_char_idx": 145441, "end_char_idx": 146155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a770de1-cef2-494e-98f9-06ae5abd048b": {"__data__": {"id_": "2a770de1-cef2-494e-98f9-06ae5abd048b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2f7e06c-d897-4881-87fc-4cc7fa506c0e", "node_type": "1", "metadata": {}, "hash": "85d96e10bfad3fb7c1c4fb181564d7143784e024962b89b90325982afce843f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "464985a9-2857-4151-888c-5c6072a051b3", "node_type": "1", "metadata": {}, "hash": "a1d5510127de95b3a55091bc3e286d3412167eece0846a0e45e93b3d697e08e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is a very popular data format because of its ease of use and simplicity.\nLet\u2019s look at how to write CSV files with Spark by running the following code:\nsalary_data_with_id.write.csv('salary_data.csv', mode='overwrite', \nheader=True)\nspark.read.csv('/salary_data.csv', header=True).show()\nThe resulting DataFrame, named salary_data_with_id, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John| Field-eng|3500|\n| 2 | Robert | Sales|4000|\n| 3 | Maria| Finance|3500|\n| 4 | Michael| Sales|3000|\n| 5 | Kelly| Finance|3500|\n| 6 | Kate| Finance|3000|\n| 7 | Martin | Finance|3500|\n| 8 | Kiran| Sales|2200|\n+---+--------+----------+------+\nThere are certain parameters in the dataframe.", "mimetype": "text/plain", "start_char_idx": 145811, "end_char_idx": 146567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "464985a9-2857-4151-888c-5c6072a051b3": {"__data__": {"id_": "464985a9-2857-4151-888c-5c6072a051b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a770de1-cef2-494e-98f9-06ae5abd048b", "node_type": "1", "metadata": {}, "hash": "f1d194bea658a1adf171ed4cd20b21ab2281f278bc9a3c85ad406526da1384ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fd8ad0b-305c-464e-bc28-68b84b940f6b", "node_type": "1", "metadata": {}, "hash": "76c3beb6e63433ade31df1befeb3e1c2ff1814718b0e16fb2be3f5cf688b2dd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "write.csv() function that we can see here. The \nfirst parameter is the dataframe name that we need to write to disk. The second parameter, header, \nspecifies whether the file that we need to write should be written with a header row or not.\nThere are certain parameters in the dataframe.read.csv() function that we should discuss. \nThe first parameter is the path/name value of the file that we need to read. The second parameter, \nheader, specifies whether the file has a header row to be read.\n\nReading and writing data\n\nIn the first statement, we\u2019re writing the salary_data DataFrame to a CSV file named salary_\ndata.csv.\nIn the next statement, we\u2019re reading back the same file that we wrote to see its contents. We can see \nthat the resulting file contains the same data that we wrote.", "mimetype": "text/plain", "start_char_idx": 146567, "end_char_idx": 147356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fd8ad0b-305c-464e-bc28-68b84b940f6b": {"__data__": {"id_": "9fd8ad0b-305c-464e-bc28-68b84b940f6b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "464985a9-2857-4151-888c-5c6072a051b3", "node_type": "1", "metadata": {}, "hash": "a1d5510127de95b3a55091bc3e286d3412167eece0846a0e45e93b3d697e08e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed3543b-380a-47b4-aeae-0f2c88e59108", "node_type": "1", "metadata": {}, "hash": "a0f091a30fd53b9bab4386f1367f5b284287d2027d322c687af1b3016b1490f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second parameter, \nheader, specifies whether the file has a header row to be read.\n\nReading and writing data\n\nIn the first statement, we\u2019re writing the salary_data DataFrame to a CSV file named salary_\ndata.csv.\nIn the next statement, we\u2019re reading back the same file that we wrote to see its contents. We can see \nthat the resulting file contains the same data that we wrote.\nLet\u2019s look at another function that can be used to read CSV files with Spark:\nfrom pyspark.sql.types import *\nfilePath = '/salary_data.csv'\ncolumns= [\"ID\", \"State\", \"Gender\"] \nschema = StructType([\nStructField(\"ID\", IntegerType(),True),\nStructField(\"State\",StringType(),True),\nStructField(\"Gender\",StringType(),True)\n])\nread_data = spark.read.format(\"csv\").option(\"header\",\"true\").", "mimetype": "text/plain", "start_char_idx": 146976, "end_char_idx": 147738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ed3543b-380a-47b4-aeae-0f2c88e59108": {"__data__": {"id_": "3ed3543b-380a-47b4-aeae-0f2c88e59108", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fd8ad0b-305c-464e-bc28-68b84b940f6b", "node_type": "1", "metadata": {}, "hash": "76c3beb6e63433ade31df1befeb3e1c2ff1814718b0e16fb2be3f5cf688b2dd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd63fa0e-04ff-4403-9eee-127f17f44a26", "node_type": "1", "metadata": {}, "hash": "c9463e5674d5ec24493ad7da64d6349ed55c10d8f41e61912817670bf20dec6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "schema(schema).load(filePath)\nread_data.show()\nThe resulting DataFrame, named read_data, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John| Field-eng|3500|\n| 2 | Robert | Sales|4000|\n| 3 | Maria| Finance|3500|\n| 4 | Michael| Sales|3000|\n| 5 | Kelly| Finance|3500|\n| 6 | Kate| Finance|3000|\n| 7 | Martin | Finance|3500|\n| 8 | Kiran| Sales|2200|\n+---+--------+----------+------+\nThere are certain parameters in the spark.read.format() function. First, we specify the format \nof the file that needs to be read. Then, we can perform different function calls for different options.", "mimetype": "text/plain", "start_char_idx": 147739, "end_char_idx": 148392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd63fa0e-04ff-4403-9eee-127f17f44a26": {"__data__": {"id_": "cd63fa0e-04ff-4403-9eee-127f17f44a26", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ed3543b-380a-47b4-aeae-0f2c88e59108", "node_type": "1", "metadata": {}, "hash": "a0f091a30fd53b9bab4386f1367f5b284287d2027d322c687af1b3016b1490f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbb2288d-ef75-42d7-ab0c-96b58e170a53", "node_type": "1", "metadata": {}, "hash": "d2cdf7c1378840812161c6d750acfb3103e7b0820559e1ae884cd06ade37aac2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we specify the format \nof the file that needs to be read. Then, we can perform different function calls for different options. In \nthe next call, we specify that the file has a header, so the DataFrame expects to have a header. Then, we \nspecify that we need to have a schema for this data, which is defined in the schema variable. Finally, \nin the load function, we define the path of the file to be loaded.\nNext, we will learn how to read and write Parquet files with Spark.\n\nAdvanced Operations and Optimizations in Spark\n\nReading and writing Parquet files\nIn this section, we will discuss the Parquet file format. Parquet is a columnar file format that makes data \nreading and writing very efficient. It is also a compact file format that facilitates faster reads and writes.\nLet\u2019s learn how to write Parquet files with Spark by running the following code:\nsalary_data_with_id.", "mimetype": "text/plain", "start_char_idx": 148259, "end_char_idx": 149147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbb2288d-ef75-42d7-ab0c-96b58e170a53": {"__data__": {"id_": "dbb2288d-ef75-42d7-ab0c-96b58e170a53", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd63fa0e-04ff-4403-9eee-127f17f44a26", "node_type": "1", "metadata": {}, "hash": "c9463e5674d5ec24493ad7da64d6349ed55c10d8f41e61912817670bf20dec6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a167a58-de5c-461c-bbef-e11384d4c374", "node_type": "1", "metadata": {}, "hash": "2b262276135c49b753b33f9fe8f32221286c863a08a69a7632df3a89c80da639", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advanced Operations and Optimizations in Spark\n\nReading and writing Parquet files\nIn this section, we will discuss the Parquet file format. Parquet is a columnar file format that makes data \nreading and writing very efficient. It is also a compact file format that facilitates faster reads and writes.\nLet\u2019s learn how to write Parquet files with Spark by running the following code:\nsalary_data_with_id.write.parquet('salary_data.parquet', \nmode='overwrite')\nspark.read.parquet(' /salary_data.parquet').show()\nThe resulting DataFrame, named salary_data_with_id,", "mimetype": "text/plain", "start_char_idx": 148744, "end_char_idx": 149305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a167a58-de5c-461c-bbef-e11384d4c374": {"__data__": {"id_": "6a167a58-de5c-461c-bbef-e11384d4c374", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbb2288d-ef75-42d7-ab0c-96b58e170a53", "node_type": "1", "metadata": {}, "hash": "d2cdf7c1378840812161c6d750acfb3103e7b0820559e1ae884cd06ade37aac2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0460ea95-4d81-43c0-bd74-78234b16f512", "node_type": "1", "metadata": {}, "hash": "c4d4862ec93db9a0670559e15b201c7cab32e12a4e7adccbd618d0a08996162f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is also a compact file format that facilitates faster reads and writes.\nLet\u2019s learn how to write Parquet files with Spark by running the following code:\nsalary_data_with_id.write.parquet('salary_data.parquet', \nmode='overwrite')\nspark.read.parquet(' /salary_data.parquet').show()\nThe resulting DataFrame, named salary_data_with_id, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John| Field-eng|3500|\n| 2 | Robert | Sales|4000|\n| 3 | Maria| Finance|3500|\n| 4 | Michael| Sales|3000|\n| 5 | Kelly| Finance|3500|\n| 6 | Kate| Finance|3000|\n| 7 | Martin | Finance|3500|\n| 8 | Kiran| Sales|2200|\n+---+--------+----------+------+\nThere are certain parameters in the dataframe.", "mimetype": "text/plain", "start_char_idx": 148971, "end_char_idx": 149717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0460ea95-4d81-43c0-bd74-78234b16f512": {"__data__": {"id_": "0460ea95-4d81-43c0-bd74-78234b16f512", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a167a58-de5c-461c-bbef-e11384d4c374", "node_type": "1", "metadata": {}, "hash": "2b262276135c49b753b33f9fe8f32221286c863a08a69a7632df3a89c80da639", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07a5fec4-52fb-41dc-b0a6-bee707405cfc", "node_type": "1", "metadata": {}, "hash": "b1f4e1d6a3b581f84a698a7aad545f9ee3b7a6e290ccd3ec6cf2bf336d2991a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "write() function that we can see here. The first \ncall is to the parquet function to define the file type. Then, as the next parameter, we specify the \npath where this Parquet file needs to be written.\nIn the next statement, we\u2019re reading the same file that we wrote, to see its contents. We can see that \nthe resulting file contains the data we wrote.\nNext, we will look at how we can read and write ORC files with Spark.\nReading and writing ORC files\nIn this section, we will discuss the ORC file format. Like Parquet, ORC is also a columnar and compact \nfile format that makes data reading and writing very efficient.\nLet\u2019s learn how to write ORC files with Spark by running the following code:\nsalary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\nspark.read.orc(' /salary_data.orc').", "mimetype": "text/plain", "start_char_idx": 149717, "end_char_idx": 150518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07a5fec4-52fb-41dc-b0a6-bee707405cfc": {"__data__": {"id_": "07a5fec4-52fb-41dc-b0a6-bee707405cfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0460ea95-4d81-43c0-bd74-78234b16f512", "node_type": "1", "metadata": {}, "hash": "c4d4862ec93db9a0670559e15b201c7cab32e12a4e7adccbd618d0a08996162f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8", "node_type": "1", "metadata": {}, "hash": "3b1198c49aa0968adb651c6122468017d9af5c979176b7622792c77a646156db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like Parquet, ORC is also a columnar and compact \nfile format that makes data reading and writing very efficient.\nLet\u2019s learn how to write ORC files with Spark by running the following code:\nsalary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\nspark.read.orc(' /salary_data.orc').show()\n\nReading and writing data\n\nThe resulting DataFrame, named salary_data_with_id,", "mimetype": "text/plain", "start_char_idx": 150224, "end_char_idx": 150603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8": {"__data__": {"id_": "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07a5fec4-52fb-41dc-b0a6-bee707405cfc", "node_type": "1", "metadata": {}, "hash": "b1f4e1d6a3b581f84a698a7aad545f9ee3b7a6e290ccd3ec6cf2bf336d2991a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da583324-82ae-47e8-a773-196fd3a1afb3", "node_type": "1", "metadata": {}, "hash": "13ff3e463952ad61aaa8896e952f894f6398e9ccba81d6e90618a310ff12dc31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like Parquet, ORC is also a columnar and compact \nfile format that makes data reading and writing very efficient.\nLet\u2019s learn how to write ORC files with Spark by running the following code:\nsalary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\nspark.read.orc(' /salary_data.orc').show()\n\nReading and writing data\n\nThe resulting DataFrame, named salary_data_with_id, looks like this:\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John| Field-eng|3500|\n| 2 | Robert | Sales|4000|\n| 3 | Maria| Finance|3500|\n| 4 | Michael| Sales|3000|\n| 5 | Kelly| Finance|3500|\n| 6 | Kate| Finance|3000|\n| 7 | Martin | Finance|3500|\n| 8 | Kiran| Sales|2200|\n+---+--------+----------+------+\nThere are certain parameters in the dataframe.", "mimetype": "text/plain", "start_char_idx": 150224, "end_char_idx": 151015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da583324-82ae-47e8-a773-196fd3a1afb3": {"__data__": {"id_": "da583324-82ae-47e8-a773-196fd3a1afb3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8", "node_type": "1", "metadata": {}, "hash": "3b1198c49aa0968adb651c6122468017d9af5c979176b7622792c77a646156db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c754b380-afd3-444f-bf07-b93620cb348f", "node_type": "1", "metadata": {}, "hash": "85605a7be99cec743f0c3272792a0586db64575f418d114f0141ac14ebc30a5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "write() function that we can see. The first call \nis to the orc function to define the file type. Then, as the next parameter, we specify the path where \nthis Parquet file needs to be written.\nIn the next statement, we\u2019re reading back the same file that we wrote to see its contents. We can see \nthat the resulting file contains the same data that we wrote.\nNext, we will look at how we can read and write Delta files with Spark.\nReading and writing Delta files\nThe Delta file format is an open format that is more optimized than Parquet and other columnar \nformats. When the data is stored in Delta format, you will notice that the underlying files are in \nParquet. The Delta format adds a transactional log on top of Parquet files to make data reads and \nwrites a lot more efficient.", "mimetype": "text/plain", "start_char_idx": 151015, "end_char_idx": 151800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c754b380-afd3-444f-bf07-b93620cb348f": {"__data__": {"id_": "c754b380-afd3-444f-bf07-b93620cb348f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da583324-82ae-47e8-a773-196fd3a1afb3", "node_type": "1", "metadata": {}, "hash": "13ff3e463952ad61aaa8896e952f894f6398e9ccba81d6e90618a310ff12dc31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66a026d4-de66-4c29-ac34-65539d538063", "node_type": "1", "metadata": {}, "hash": "57001731ca4fa50bc5d4ae90cfe9e357564cac0fee4f4044debfa163a9cc16dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reading and writing Delta files\nThe Delta file format is an open format that is more optimized than Parquet and other columnar \nformats. When the data is stored in Delta format, you will notice that the underlying files are in \nParquet. The Delta format adds a transactional log on top of Parquet files to make data reads and \nwrites a lot more efficient.\nLet\u2019s learn how to read and write Delta files with Spark by running the following code:\nsalary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/\nsalary_data_with_id\", mode='overwrite')\ndf = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\ndf.show()\n\nAdvanced Operations and Optimizations in Spark\n\nThe resulting DataFrame, named salary_data_with_id,", "mimetype": "text/plain", "start_char_idx": 151445, "end_char_idx": 152168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66a026d4-de66-4c29-ac34-65539d538063": {"__data__": {"id_": "66a026d4-de66-4c29-ac34-65539d538063", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c754b380-afd3-444f-bf07-b93620cb348f", "node_type": "1", "metadata": {}, "hash": "85605a7be99cec743f0c3272792a0586db64575f418d114f0141ac14ebc30a5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76a5fa63-76a0-4a2d-baf2-be68f493517e", "node_type": "1", "metadata": {}, "hash": "567232e2dfdefa78e02f7af1aa68eaf385f14ec2b80ab1b22ae9b20ddd06fdb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s learn how to read and write Delta files with Spark by running the following code:\nsalary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/\nsalary_data_with_id\", mode='overwrite')\ndf = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\ndf.show()\n\nAdvanced Operations and Optimizations in Spark\n\nThe resulting DataFrame, named salary_data_with_id, looks like this:\n+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|7|Martin|Finance|3500|\n|4| Michael|Sales|3000|\n|6|Kate|Finance|3000|\n|2|Robert|Sales|4000|\n|1|John| Field-eng|3500|\n|5|Kelly|Finance|3500|\n|3|Maria|Finance|3500|\n|8|Kiran|Sales|2200|\n+---+--------+----------+------+\nIn this example,", "mimetype": "text/plain", "start_char_idx": 151801, "end_char_idx": 152518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76a5fa63-76a0-4a2d-baf2-be68f493517e": {"__data__": {"id_": "76a5fa63-76a0-4a2d-baf2-be68f493517e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66a026d4-de66-4c29-ac34-65539d538063", "node_type": "1", "metadata": {}, "hash": "57001731ca4fa50bc5d4ae90cfe9e357564cac0fee4f4044debfa163a9cc16dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26a69f3e-922a-4b3c-a435-6aadd83fe08f", "node_type": "1", "metadata": {}, "hash": "ef72a7c052a03164b10477fc5230fdcb24940c14ff40c59a11c9b2f34056ac50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we\u2019re writing salary_data_with_id to a Delta file. We added the delta \nparameter to the format function, after which we saved the file to a location.\nIn the next statement, we are reading the same Delta file we wrote into a DataFrame called df. The \ncontents of the file remain the same as the DataFrame we used to write it with.\nNow that we know how to manipulate and join data with advanced operations in Spark, we will look \nat how we can use SQL with Spark DataFrames interchangeably to switch between Python and SQL \nas languages. This gives a lot of power to Spark users since this allows them to use multiple languages, \ndepending on the use case and their knowledge of different languages.\nUsing SQL in Spark\nIn Chapter 2, we talked about Spark Core and how it\u2019s shared across different components of Spark. \nDataFrames and Spark SQL can also be used interchangeably.", "mimetype": "text/plain", "start_char_idx": 152519, "end_char_idx": 153394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26a69f3e-922a-4b3c-a435-6aadd83fe08f": {"__data__": {"id_": "26a69f3e-922a-4b3c-a435-6aadd83fe08f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76a5fa63-76a0-4a2d-baf2-be68f493517e", "node_type": "1", "metadata": {}, "hash": "567232e2dfdefa78e02f7af1aa68eaf385f14ec2b80ab1b22ae9b20ddd06fdb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e1521ad-d30a-43ee-99c9-e977ad27b0d5", "node_type": "1", "metadata": {}, "hash": "29267659ceeb1bc5079f9a0e1759439cff859d42fe6ba05119cde9d238e03604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This gives a lot of power to Spark users since this allows them to use multiple languages, \ndepending on the use case and their knowledge of different languages.\nUsing SQL in Spark\nIn Chapter 2, we talked about Spark Core and how it\u2019s shared across different components of Spark. \nDataFrames and Spark SQL can also be used interchangeably. We can also use data stored in DataFrames \nwith Spark SQL queries.\nThe following code illustrates how we can make use of this feature:\nsalary_data_with_id.createOrReplaceTempView(\"SalaryTable\")\nspark.sql(\"SELECT count(*) from SalaryTable\").show()\n The resulting DataFrame looks like this:\n+--------+\n|count(1)|\n+--------+\n|8|\n+--------+\n.\n\nUDFs in Apache Spark\n\nThe createOrReplaceTempView function is used to convert a DataFrame into a table named \nSalaryTable. Once this conversion is made, we can run regular SQL queries on top of this table.", "mimetype": "text/plain", "start_char_idx": 153055, "end_char_idx": 153940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e1521ad-d30a-43ee-99c9-e977ad27b0d5": {"__data__": {"id_": "4e1521ad-d30a-43ee-99c9-e977ad27b0d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26a69f3e-922a-4b3c-a435-6aadd83fe08f", "node_type": "1", "metadata": {}, "hash": "ef72a7c052a03164b10477fc5230fdcb24940c14ff40c59a11c9b2f34056ac50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d603efb8-292d-4488-852a-4b2911132cf9", "node_type": "1", "metadata": {}, "hash": "1222cbeb9cf76a40a073e5744ff5d6e3f5aba02fa7d07feccc1672480c534270", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "UDFs in Apache Spark\n\nThe createOrReplaceTempView function is used to convert a DataFrame into a table named \nSalaryTable. Once this conversion is made, we can run regular SQL queries on top of this table. \nWe are running a count * query to count the total number of elements in a table.\nIn the next section, we will see what a UDF is and how we use that in Spark.\nUDFs in Apache Spark\nUDFs are a powerful feature in Apache Spark that allows you to extend the functionality of Spark \nby defining custom functions. UDFs are essential for transforming and manipulating data in ways \nnot directly supported by built-in Spark functions. In this section, we\u2019ll delve into the concepts, \nimplementation, and best practices for using UDFs in Spark.\nWhat are UDFs?\nUDFs are custom functions that are created by users to perform specific operations on data within \nSpark.", "mimetype": "text/plain", "start_char_idx": 153735, "end_char_idx": 154597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d603efb8-292d-4488-852a-4b2911132cf9": {"__data__": {"id_": "d603efb8-292d-4488-852a-4b2911132cf9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e1521ad-d30a-43ee-99c9-e977ad27b0d5", "node_type": "1", "metadata": {}, "hash": "29267659ceeb1bc5079f9a0e1759439cff859d42fe6ba05119cde9d238e03604", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b12523b1-6057-4ae1-96f5-6d68b51e5b0f", "node_type": "1", "metadata": {}, "hash": "0e0361131acc244ff6f934fbb395343738d264944566490e0e5a90fc4badf876", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "UDFs are essential for transforming and manipulating data in ways \nnot directly supported by built-in Spark functions. In this section, we\u2019ll delve into the concepts, \nimplementation, and best practices for using UDFs in Spark.\nWhat are UDFs?\nUDFs are custom functions that are created by users to perform specific operations on data within \nSpark. UDFs extend the range of transformations and operations you can apply to your data, making \nSpark more versatile for diverse use cases.", "mimetype": "text/plain", "start_char_idx": 154249, "end_char_idx": 154733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b12523b1-6057-4ae1-96f5-6d68b51e5b0f": {"__data__": {"id_": "b12523b1-6057-4ae1-96f5-6d68b51e5b0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d603efb8-292d-4488-852a-4b2911132cf9", "node_type": "1", "metadata": {}, "hash": "1222cbeb9cf76a40a073e5744ff5d6e3f5aba02fa7d07feccc1672480c534270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78", "node_type": "1", "metadata": {}, "hash": "b236513ae9fb6d7317e73c7504b59a1a0f85a67e1e40490846fba97066793f64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we\u2019ll delve into the concepts, \nimplementation, and best practices for using UDFs in Spark.\nWhat are UDFs?\nUDFs are custom functions that are created by users to perform specific operations on data within \nSpark. UDFs extend the range of transformations and operations you can apply to your data, making \nSpark more versatile for diverse use cases.\nHere are some of the key characteristics of UDFs:\n\u2022\t User-customized logic: UDFs allow you to apply user-specific logic or custom algorithms to \nyour data\n\u2022\t Support for various languages: Spark supports UDFs written in various programming languages, \nincluding Scala, Python, Java, and R\n\u2022\t Compatibility with DataFrames and resilient distributed datasets (RDDs): UDFs can be \nused with both DataFrames and RDDs\n\u2022\t Leverage external libraries: You can use external libraries within your UDFs to perform \nadvanced operations\nLet\u2019s see how UDFs are created.", "mimetype": "text/plain", "start_char_idx": 154368, "end_char_idx": 155290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78": {"__data__": {"id_": "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b12523b1-6057-4ae1-96f5-6d68b51e5b0f", "node_type": "1", "metadata": {}, "hash": "0e0361131acc244ff6f934fbb395343738d264944566490e0e5a90fc4badf876", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0119dc57-76f9-4054-ae8b-99be2122b676", "node_type": "1", "metadata": {}, "hash": "cf9a0f543f0bfb77764b99b6e74c96a2a5b5b7552bfc60a538a4c52af66f7905", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Creating and registering UDFs\nTo use UDFs in Spark, you need to create and register them. The process involves defining a function \nand registering it with Spark. You can define UDFs for both SQL and DataFrame operations. In this \nsection, you will see the basic syntax of defining a UDF in Spark and then registering that UDF with \nSpark. You can write any custom Python code in your UDF for your application\u2019s logic. The first \nexample is in Python; the next example is in Scala.\n\nAdvanced Operations and Optimizations in Spark\n\nCreating UDFs in Python\nWe can use the following code to create a UDF in Python:\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.", "mimetype": "text/plain", "start_char_idx": 155291, "end_char_idx": 155958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0119dc57-76f9-4054-ae8b-99be2122b676": {"__data__": {"id_": "0119dc57-76f9-4054-ae8b-99be2122b676", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78", "node_type": "1", "metadata": {}, "hash": "b236513ae9fb6d7317e73c7504b59a1a0f85a67e1e40490846fba97066793f64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ba2df28-af2e-4f36-8fba-aa3b89410093", "node_type": "1", "metadata": {}, "hash": "59e83c6fc90108dfc06db7f5d9295eb3b5e36046ebcb191a6751214df584533d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can write any custom Python code in your UDF for your application\u2019s logic. The first \nexample is in Python; the next example is in Scala.\n\nAdvanced Operations and Optimizations in Spark\n\nCreating UDFs in Python\nWe can use the following code to create a UDF in Python:\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n# Define a UDF in Python\ndef my_udf_function(input_param):\n# Your custom logic here\nreturn processed_value\n# Register the UDF with Spark\nmy_udf = udf(my_udf_function, IntegerType())\n# Using the UDF in a DataFrame operation\ndf = df.withColumn(\"new_column\", my_udf(df[\"input_column\"]))\nCreating UDFs in Scala\nWe can use the following code to create a UDF in Scala:\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.", "mimetype": "text/plain", "start_char_idx": 155631, "end_char_idx": 156410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ba2df28-af2e-4f36-8fba-aa3b89410093": {"__data__": {"id_": "2ba2df28-af2e-4f36-8fba-aa3b89410093", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0119dc57-76f9-4054-ae8b-99be2122b676", "node_type": "1", "metadata": {}, "hash": "cf9a0f543f0bfb77764b99b6e74c96a2a5b5b7552bfc60a538a4c52af66f7905", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed893ac2-63e3-4735-99b4-d2d5341f41e9", "node_type": "1", "metadata": {}, "hash": "865e07b17a1861e096955bd171e914c814ea096462aab5ca2423f876930f3594", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IntegerType())\n# Using the UDF in a DataFrame operation\ndf = df.withColumn(\"new_column\", my_udf(df[\"input_column\"]))\nCreating UDFs in Scala\nWe can use the following code to create a UDF in Scala:\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n// Define a UDF in Scala\nval myUDF: UserDefinedFunction = udf((inputParam: InputType) => {\n// Your custom logic here\nprocessedValue }, OutputType)\n// Using the UDF in a DataFrame operation\nval df = df.withColumn(\"newColumn\", myUDF(col(\"inputColumn\")))\nUse cases for UDFs\nUDFs are versatile and can be used in a wide range of scenarios, including, but not limited to, \nthe following:\n\u2022\t Data transformation: Applying custom logic to transform data, such as data enrichment, \ncleansing,", "mimetype": "text/plain", "start_char_idx": 156150, "end_char_idx": 156906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed893ac2-63e3-4735-99b4-d2d5341f41e9": {"__data__": {"id_": "ed893ac2-63e3-4735-99b4-d2d5341f41e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ba2df28-af2e-4f36-8fba-aa3b89410093", "node_type": "1", "metadata": {}, "hash": "59e83c6fc90108dfc06db7f5d9295eb3b5e36046ebcb191a6751214df584533d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ff3cea2-27d4-47a5-8116-4139af197c5f", "node_type": "1", "metadata": {}, "hash": "9389a434a54f7f4e29b24c8acf2dab39af09d745d453666fb038917bc3c646a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "OutputType)\n// Using the UDF in a DataFrame operation\nval df = df.withColumn(\"newColumn\", myUDF(col(\"inputColumn\")))\nUse cases for UDFs\nUDFs are versatile and can be used in a wide range of scenarios, including, but not limited to, \nthe following:\n\u2022\t Data transformation: Applying custom logic to transform data, such as data enrichment, \ncleansing, and feature engineering\n\u2022\t Complex calculations: Implementing complex mathematical or statistical operations not \navailable in Spark\u2019s standard functions\n\u2022\t String manipulation: Parsing and formatting strings, regular expressions, and text processing\n\u2022\t Machine learning: Creating custom functions for feature extraction, preprocessing, or post-\nprocessing in machine learning workflows\n\u2022\t Domain-specific logic: Implementing specific domain-related logic that is unique to your use case\n\nOptimizations in Apache Spark\n\nBest practices for using UDFs\nWhen working with UDFs in Spark,", "mimetype": "text/plain", "start_char_idx": 156557, "end_char_idx": 157489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ff3cea2-27d4-47a5-8116-4139af197c5f": {"__data__": {"id_": "2ff3cea2-27d4-47a5-8116-4139af197c5f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed893ac2-63e3-4735-99b4-d2d5341f41e9", "node_type": "1", "metadata": {}, "hash": "865e07b17a1861e096955bd171e914c814ea096462aab5ca2423f876930f3594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0f42204-a9f4-4e63-82d6-cd4b452ccf18", "node_type": "1", "metadata": {}, "hash": "6d220ab9d128a173a0063f3f8b34d5e7975ab0fa447a40b85648b9b73293515d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "regular expressions, and text processing\n\u2022\t Machine learning: Creating custom functions for feature extraction, preprocessing, or post-\nprocessing in machine learning workflows\n\u2022\t Domain-specific logic: Implementing specific domain-related logic that is unique to your use case\n\nOptimizations in Apache Spark\n\nBest practices for using UDFs\nWhen working with UDFs in Spark, consider the following best practices:\n\u2022\t Avoid performance bottlenecks: UDFs can impact performance, especially when used with \nlarge datasets. Profile and monitor your application to identify performance bottlenecks.\n\u2022\t Minimize UDF complexity: Keep UDFs simple and efficient to avoid slowing down your Spark \napplication. Complex operations can lead to longer execution times.\n\u2022\t Check for data type compatibility: Ensure that the UDF\u2019s output data type matches the column \ndata type to avoid errors and data type mismatches.", "mimetype": "text/plain", "start_char_idx": 157117, "end_char_idx": 158018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0f42204-a9f4-4e63-82d6-cd4b452ccf18": {"__data__": {"id_": "a0f42204-a9f4-4e63-82d6-cd4b452ccf18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ff3cea2-27d4-47a5-8116-4139af197c5f", "node_type": "1", "metadata": {}, "hash": "9389a434a54f7f4e29b24c8acf2dab39af09d745d453666fb038917bc3c646a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1cfb007-d9ab-4278-9294-ed1db025ec33", "node_type": "1", "metadata": {}, "hash": "545a720eb9f2d77fb5af8c7034a72c2c7962049081654afff1216e9fb3511a48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "especially when used with \nlarge datasets. Profile and monitor your application to identify performance bottlenecks.\n\u2022\t Minimize UDF complexity: Keep UDFs simple and efficient to avoid slowing down your Spark \napplication. Complex operations can lead to longer execution times.\n\u2022\t Check for data type compatibility: Ensure that the UDF\u2019s output data type matches the column \ndata type to avoid errors and data type mismatches.\n\u2022\t Optimize data processing: Consider using built-in Spark functions whenever possible as they \nare highly optimized for distributed data processing.\n\u2022\t Use vectorized UDFs: In some Spark versions, vectorized UDFs are available, which can \nsignificantly improve UDF performance by processing multiple values at once.\n\u2022\t Test and validate: Test your UDFs thoroughly on small subsets of data before applying them \nto the entire dataset. Ensure they produce the desired results.", "mimetype": "text/plain", "start_char_idx": 157592, "end_char_idx": 158494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1cfb007-d9ab-4278-9294-ed1db025ec33": {"__data__": {"id_": "c1cfb007-d9ab-4278-9294-ed1db025ec33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0f42204-a9f4-4e63-82d6-cd4b452ccf18", "node_type": "1", "metadata": {}, "hash": "6d220ab9d128a173a0063f3f8b34d5e7975ab0fa447a40b85648b9b73293515d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c38611bf-f02c-45bc-9b89-236ac7424030", "node_type": "1", "metadata": {}, "hash": "75a3bbea088f8fa1f868305ed943c603ef3448cf748dd7dda89af769702bb357", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Use vectorized UDFs: In some Spark versions, vectorized UDFs are available, which can \nsignificantly improve UDF performance by processing multiple values at once.\n\u2022\t Test and validate: Test your UDFs thoroughly on small subsets of data before applying them \nto the entire dataset. Ensure they produce the desired results.\n\u2022\t Document UDFs: Document your UDFs with comments and descriptions to make your code \nmore maintainable and understandable to others.\nIn this section, we explored the concept of UDFs in Apache Spark. UDFs are powerful tools for \nextending Spark\u2019s capabilities and performing custom data transformations and operations. When \nused judiciously and efficiently, UDFs can help you address a wide range of data processing challenges \nin Spark.\nNow that we\u2019ve covered the advanced operations in Spark, we will dive into the concept of \nSpark optimization.", "mimetype": "text/plain", "start_char_idx": 158169, "end_char_idx": 159045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c38611bf-f02c-45bc-9b89-236ac7424030": {"__data__": {"id_": "c38611bf-f02c-45bc-9b89-236ac7424030", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1cfb007-d9ab-4278-9294-ed1db025ec33", "node_type": "1", "metadata": {}, "hash": "545a720eb9f2d77fb5af8c7034a72c2c7962049081654afff1216e9fb3511a48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07adbcfb-e300-462a-8794-4d0d3adc0efc", "node_type": "1", "metadata": {}, "hash": "638a9ba4955e644c6117920f599fe0d1d87d80053d5681cd3f7baab3335bc120", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we explored the concept of UDFs in Apache Spark. UDFs are powerful tools for \nextending Spark\u2019s capabilities and performing custom data transformations and operations. When \nused judiciously and efficiently, UDFs can help you address a wide range of data processing challenges \nin Spark.\nNow that we\u2019ve covered the advanced operations in Spark, we will dive into the concept of \nSpark optimization.\nOptimizations in Apache Spark\nApache Spark, renowned for its distributed computing capabilities, offers a suite of advanced optimization \ntechniques that are crucial for maximizing performance, improving resource utilization, and enhancing \nthe efficiency of data processing jobs. These techniques go beyond basic optimizations, allowing users \nto fine-tune and optimize Spark applications for optimal execution.\nUnderstanding optimization in Spark\nOptimization in Spark aims to fine-tune the execution of jobs to improve speed, resource utilization, \nand overall performance.", "mimetype": "text/plain", "start_char_idx": 158630, "end_char_idx": 159622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07adbcfb-e300-462a-8794-4d0d3adc0efc": {"__data__": {"id_": "07adbcfb-e300-462a-8794-4d0d3adc0efc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c38611bf-f02c-45bc-9b89-236ac7424030", "node_type": "1", "metadata": {}, "hash": "75a3bbea088f8fa1f868305ed943c603ef3448cf748dd7dda89af769702bb357", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b640beeb-69a8-4b36-b994-3d53d5aa6d10", "node_type": "1", "metadata": {}, "hash": "b52c0755abde7427e009ce3c62c6b2256f5cd5704d2ea17a6cb1d701c9a94919", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These techniques go beyond basic optimizations, allowing users \nto fine-tune and optimize Spark applications for optimal execution.\nUnderstanding optimization in Spark\nOptimization in Spark aims to fine-tune the execution of jobs to improve speed, resource utilization, \nand overall performance.\n\nAdvanced Operations and Optimizations in Spark\n\nApache Spark is well-known for its powerful optimization capabilities, which significantly enhance \nthe performance of distributed data processing tasks. At the heart of this optimization framework lies \nthe Catalyst optimizer, an integral component that plays a pivotal role in enhancing query execution \nefficiency. This is achieved before the query is executed.\nThe Catalyst optimizer works primarily on static optimization plans that are generated during query \ncompilation. However, AQE, which was introduced in Spark 3.0, is a dynamic and adaptive approach to \noptimizing query plans at runtime based on the actual data characteristics and execution environment. \nWe will learn more about both these paradigms in the next section.", "mimetype": "text/plain", "start_char_idx": 159327, "end_char_idx": 160408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b640beeb-69a8-4b36-b994-3d53d5aa6d10": {"__data__": {"id_": "b640beeb-69a8-4b36-b994-3d53d5aa6d10", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07adbcfb-e300-462a-8794-4d0d3adc0efc", "node_type": "1", "metadata": {}, "hash": "638a9ba4955e644c6117920f599fe0d1d87d80053d5681cd3f7baab3335bc120", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cf877e8-cc97-40df-ad9f-aec4e3299025", "node_type": "1", "metadata": {}, "hash": "5e8c030d27083df71f6f332221ea05bf0560e51671a8dd56551845aaf49a8d48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is achieved before the query is executed.\nThe Catalyst optimizer works primarily on static optimization plans that are generated during query \ncompilation. However, AQE, which was introduced in Spark 3.0, is a dynamic and adaptive approach to \noptimizing query plans at runtime based on the actual data characteristics and execution environment. \nWe will learn more about both these paradigms in the next section.\nCatalyst optimizer\nThe Catalyst optimizer is an essential part of Apache Spark\u2019s query execution engine. It is a powerful \ntool that uses advanced techniques to optimize query plans, thus improving the performance of Spark \napplications. The term \u201ccatalyst\u201d refers to its ability to spark transformations in the query plan and \nmake it more efficient.\nLet\u2019s look at some of the key characteristics of the Catalyst optimizer:\n\u2022\t Rule-based optimization: The Catalyst optimizer employs a set of rules and optimizations to \ntransform and enhance query plans.", "mimetype": "text/plain", "start_char_idx": 159990, "end_char_idx": 160964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cf877e8-cc97-40df-ad9f-aec4e3299025": {"__data__": {"id_": "6cf877e8-cc97-40df-ad9f-aec4e3299025", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b640beeb-69a8-4b36-b994-3d53d5aa6d10", "node_type": "1", "metadata": {}, "hash": "b52c0755abde7427e009ce3c62c6b2256f5cd5704d2ea17a6cb1d701c9a94919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "365d6a92-e13a-463b-887d-b24c4a770740", "node_type": "1", "metadata": {}, "hash": "817884697f6bfa3a4eb53bca0c013a6768ece5352231f8a94e40d378874cbc33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is a powerful \ntool that uses advanced techniques to optimize query plans, thus improving the performance of Spark \napplications. The term \u201ccatalyst\u201d refers to its ability to spark transformations in the query plan and \nmake it more efficient.\nLet\u2019s look at some of the key characteristics of the Catalyst optimizer:\n\u2022\t Rule-based optimization: The Catalyst optimizer employs a set of rules and optimizations to \ntransform and enhance query plans. These rules cover a wide range of query optimization scenarios.\n\u2022\t Logical and physical query plans: It works with both logical and physical query plans. The \nlogical plan represents the abstract structure of a query, while the physical plan outlines how \nto execute it.\n\u2022\t Extensibility: Users can define custom rules and optimizations. This extensibility allows you \nto tailor the optimizer to your specific use case.", "mimetype": "text/plain", "start_char_idx": 160514, "end_char_idx": 161384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "365d6a92-e13a-463b-887d-b24c4a770740": {"__data__": {"id_": "365d6a92-e13a-463b-887d-b24c4a770740", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cf877e8-cc97-40df-ad9f-aec4e3299025", "node_type": "1", "metadata": {}, "hash": "5e8c030d27083df71f6f332221ea05bf0560e51671a8dd56551845aaf49a8d48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f", "node_type": "1", "metadata": {}, "hash": "a0b5f23ed1c7e562380ea8490a74d8a48cd95553b304274c3bbe1c70255f3d43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These rules cover a wide range of query optimization scenarios.\n\u2022\t Logical and physical query plans: It works with both logical and physical query plans. The \nlogical plan represents the abstract structure of a query, while the physical plan outlines how \nto execute it.\n\u2022\t Extensibility: Users can define custom rules and optimizations. This extensibility allows you \nto tailor the optimizer to your specific use case.\n\u2022\t Cost-based optimization: The Catalyst optimizer can evaluate the cost of different query plans \nand choose the most efficient one based on cost estimates. This is particularly useful when \ndealing with complex queries.\nLet\u2019s take a look at the different components that make up the Catalyst optimizer.\nCatalyst optimizer components\nTo gain a deeper understanding of the Catalyst optimizer, it\u2019s essential to examine its core components.\nLogical query plan\nThe logical query plan represents the high-level, abstract structure of a query.", "mimetype": "text/plain", "start_char_idx": 160965, "end_char_idx": 161924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f": {"__data__": {"id_": "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "365d6a92-e13a-463b-887d-b24c4a770740", "node_type": "1", "metadata": {}, "hash": "817884697f6bfa3a4eb53bca0c013a6768ece5352231f8a94e40d378874cbc33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f837f3-36c4-4da4-ab5d-24b1ff5feec1", "node_type": "1", "metadata": {}, "hash": "223aaa91989592ab367edb14101e15f93bb22177a48ff3ff76876cb2506365f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is particularly useful when \ndealing with complex queries.\nLet\u2019s take a look at the different components that make up the Catalyst optimizer.\nCatalyst optimizer components\nTo gain a deeper understanding of the Catalyst optimizer, it\u2019s essential to examine its core components.\nLogical query plan\nThe logical query plan represents the high-level, abstract structure of a query. It defines what you \nwant to accomplish without specifying how to achieve it. Spark\u2019s Catalyst optimizer works with this \nlogical plan to determine the optimal physical plan.\n\nOptimizations in Apache Spark\n\nRule-based optimization\nRule-based optimization is the backbone of the Catalyst optimizer. It comprises a set of rules that \ntransform the logical query plan into a more efficient version. Each rule focuses on a specific aspect \nof optimization, such as predicate pushdown, constant folding, or column pruning.\nPhysical query plan\nThe physical query plan defines how to execute the query.", "mimetype": "text/plain", "start_char_idx": 161543, "end_char_idx": 162520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46f837f3-36c4-4da4-ab5d-24b1ff5feec1": {"__data__": {"id_": "46f837f3-36c4-4da4-ab5d-24b1ff5feec1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f", "node_type": "1", "metadata": {}, "hash": "a0b5f23ed1c7e562380ea8490a74d8a48cd95553b304274c3bbe1c70255f3d43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "726fed06-4af9-45cd-9095-db49e93a1682", "node_type": "1", "metadata": {}, "hash": "5b4df9e1a576bdf36fb6b28cac0b26042fcffe982a0dc15df89e5cf1519d73af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Optimizations in Apache Spark\n\nRule-based optimization\nRule-based optimization is the backbone of the Catalyst optimizer. It comprises a set of rules that \ntransform the logical query plan into a more efficient version. Each rule focuses on a specific aspect \nof optimization, such as predicate pushdown, constant folding, or column pruning.\nPhysical query plan\nThe physical query plan defines how to execute the query. Once the logical plan is optimized using \nrule-based techniques, it\u2019s converted into a physical plan, taking into account the available resources \nand the execution environment. This phase ensures that the plan is executable in a distributed and \nparallel manner.\nCost-based optimization\nIn addition to rule-based optimization, the Catalyst optimizer can use cost-based optimization. It \nestimates the cost of different execution plans, taking into account factors such as data distribution, \njoin strategies, and available resources. This approach helps Spark choose the most efficient plan based \non actual execution characteristics.", "mimetype": "text/plain", "start_char_idx": 162101, "end_char_idx": 163156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "726fed06-4af9-45cd-9095-db49e93a1682": {"__data__": {"id_": "726fed06-4af9-45cd-9095-db49e93a1682", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46f837f3-36c4-4da4-ab5d-24b1ff5feec1", "node_type": "1", "metadata": {}, "hash": "223aaa91989592ab367edb14101e15f93bb22177a48ff3ff76876cb2506365f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f72c7f07-7c7a-440c-9f04-a726e5ca0abb", "node_type": "1", "metadata": {}, "hash": "7610fe448550d681d239d110362ffb61e0819a6137c38eb96e7ad565358b300f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This phase ensures that the plan is executable in a distributed and \nparallel manner.\nCost-based optimization\nIn addition to rule-based optimization, the Catalyst optimizer can use cost-based optimization. It \nestimates the cost of different execution plans, taking into account factors such as data distribution, \njoin strategies, and available resources. This approach helps Spark choose the most efficient plan based \non actual execution characteristics.\nCatalyst optimizer in action\nTo witness the Catalyst optimizer in action, let\u2019s consider a practical example using Spark\u2019s SQL API.\nIn this code example, we\u2019re loading data from a CSV file, applying a selection operation to pick specific \ncolumns, and filtering rows based on a condition. By calling explain() on the resulting DataFrame, \nwe can see the optimized query plan that was generated by the Catalyst optimizer.", "mimetype": "text/plain", "start_char_idx": 162699, "end_char_idx": 163577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f72c7f07-7c7a-440c-9f04-a726e5ca0abb": {"__data__": {"id_": "f72c7f07-7c7a-440c-9f04-a726e5ca0abb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "726fed06-4af9-45cd-9095-db49e93a1682", "node_type": "1", "metadata": {}, "hash": "5b4df9e1a576bdf36fb6b28cac0b26042fcffe982a0dc15df89e5cf1519d73af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a34583f-86a7-4439-80ff-5f60c350afe0", "node_type": "1", "metadata": {}, "hash": "b232772ed358e641c9602b414dc2fa30a4ed039946b269cdc9320c809130bff9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Catalyst optimizer in action\nTo witness the Catalyst optimizer in action, let\u2019s consider a practical example using Spark\u2019s SQL API.\nIn this code example, we\u2019re loading data from a CSV file, applying a selection operation to pick specific \ncolumns, and filtering rows based on a condition. By calling explain() on the resulting DataFrame, \nwe can see the optimized query plan that was generated by the Catalyst optimizer. The output provides \ninsights into the physical execution steps Spark will perform:\n# SparkSession setup\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"CatalystOptimizerExample\").", "mimetype": "text/plain", "start_char_idx": 163157, "end_char_idx": 163785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a34583f-86a7-4439-80ff-5f60c350afe0": {"__data__": {"id_": "4a34583f-86a7-4439-80ff-5f60c350afe0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f72c7f07-7c7a-440c-9f04-a726e5ca0abb", "node_type": "1", "metadata": {}, "hash": "7610fe448550d681d239d110362ffb61e0819a6137c38eb96e7ad565358b300f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66", "node_type": "1", "metadata": {}, "hash": "d7dd585159d5e81155d482ff3bd10e258c01383bc9ffd43d5d365da9adc603cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By calling explain() on the resulting DataFrame, \nwe can see the optimized query plan that was generated by the Catalyst optimizer. The output provides \ninsights into the physical execution steps Spark will perform:\n# SparkSession setup\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"CatalystOptimizerExample\").\ngetOrCreate()\n# Load data\ndf = spark.read.csv(\"/salary_data.csv\", header=True, inferSchema=True) \n# Query with Catalyst Optimizer \nresult_df = df.select(\"employee\", \"department\").filter(df[\"salary\"] > \n3500) \n# Explain the optimized query plan \nresult_df.explain() \nThis explanation from the explain() method often includes details about the physical execution \nplan, the use of specific optimizations, and the chosen strategies for query execution.\nBy examining the query plan and understanding how the Catalyst optimizer enhances it, you can gain \nvaluable insights into the inner workings of Spark\u2019s optimization engine.", "mimetype": "text/plain", "start_char_idx": 163446, "end_char_idx": 164409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66": {"__data__": {"id_": "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a34583f-86a7-4439-80ff-5f60c350afe0", "node_type": "1", "metadata": {}, "hash": "b232772ed358e641c9602b414dc2fa30a4ed039946b269cdc9320c809130bff9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7adf3fea-86b1-4bf2-ac04-1fff31ce545e", "node_type": "1", "metadata": {}, "hash": "8971c2974b09ae31c874ccd23714beedc2487a073314a84863cf0ba01dbe9a36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By examining the query plan and understanding how the Catalyst optimizer enhances it, you can gain \nvaluable insights into the inner workings of Spark\u2019s optimization engine.\n\nAdvanced Operations and Optimizations in Spark\n\nThis section provided a solid introduction to the Catalyst optimizer, its components, and a practical \nexample. You can expand on this foundation by delving deeper into rule-based and cost-based \noptimization techniques, as well as discussing real-world scenarios where the Catalyst optimizer can \nhave a substantial impact on query performance.\nNext, we will see how AQE takes optimizations to the next level in Spark.\nAdaptive Query Execution (AQE)\nApache Spark, a powerful distributed computing framework, offers a multitude of optimization \ntechniques to enhance the performance of data processing jobs. One such advanced optimization \nfeature is AQE, a dynamic approach that significantly improves query processing efficiency.\nAQE dynamically adjusts execution plans during runtime based on actual data statistics and hardware \nconditions.", "mimetype": "text/plain", "start_char_idx": 164236, "end_char_idx": 165303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7adf3fea-86b1-4bf2-ac04-1fff31ce545e": {"__data__": {"id_": "7adf3fea-86b1-4bf2-ac04-1fff31ce545e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66", "node_type": "1", "metadata": {}, "hash": "d7dd585159d5e81155d482ff3bd10e258c01383bc9ffd43d5d365da9adc603cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "727aee78-1ea7-49a4-88b6-03d5f68aad5c", "node_type": "1", "metadata": {}, "hash": "ba0d7081fb11172f7a1478ec937dba906de8d92d785000e2b508900502db3795", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Adaptive Query Execution (AQE)\nApache Spark, a powerful distributed computing framework, offers a multitude of optimization \ntechniques to enhance the performance of data processing jobs. One such advanced optimization \nfeature is AQE, a dynamic approach that significantly improves query processing efficiency.\nAQE dynamically adjusts execution plans during runtime based on actual data statistics and hardware \nconditions. It collects and utilizes runtime statistics to optimize join strategies, partitioning methods, \nand broadcast operations.\nLet\u2019s look at its key components:\n\u2022\t Runtime statistics collection: AQE collects runtime statistics, such as data size, skewness, and \npartitioning, during query execution\n\u2022\t Adaptive optimization rules: It utilizes collected statistics to adjust and optimize join strategies, \npartitioning methods, and broadcast operations dynamically\nNow, let\u2019s consider its benefits and significance:\n\u2022\t Improved performance: AQE significantly enhances performance by optimizing execution \nplans dynamically,", "mimetype": "text/plain", "start_char_idx": 164879, "end_char_idx": 165921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "727aee78-1ea7-49a4-88b6-03d5f68aad5c": {"__data__": {"id_": "727aee78-1ea7-49a4-88b6-03d5f68aad5c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7adf3fea-86b1-4bf2-ac04-1fff31ce545e", "node_type": "1", "metadata": {}, "hash": "8971c2974b09ae31c874ccd23714beedc2487a073314a84863cf0ba01dbe9a36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cbf0323-5071-4235-92af-2c3614f43de5", "node_type": "1", "metadata": {}, "hash": "3f74bbf551f9fbf74c3398c7c8ddfe83bbba1bf9475ad99121523a650d6a948a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "such as data size, skewness, and \npartitioning, during query execution\n\u2022\t Adaptive optimization rules: It utilizes collected statistics to adjust and optimize join strategies, \npartitioning methods, and broadcast operations dynamically\nNow, let\u2019s consider its benefits and significance:\n\u2022\t Improved performance: AQE significantly enhances performance by optimizing execution \nplans dynamically, leading to better resource utilization and reduced execution time\n\u2022\t Handling variability: It efficiently handles variations in data sizes, skewed data distributions, \nand changing hardware conditions during query execution\n\u2022\t Efficient resource utilization: It optimizes query plans in real time, leading to better resource \nutilization and reduced execution time\nAQE workflow\nLet\u2019s look at how AQE optimizes workflows in Spark 3.0:\n\u2022\t Runtime statistics collection: During query execution, Spark collects statistics related to data \ndistribution, partition sizes, and join keys\u2019 cardinality\n\u2022\t Adaptive optimization: Utilizing the collected statistics,", "mimetype": "text/plain", "start_char_idx": 165527, "end_char_idx": 166576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cbf0323-5071-4235-92af-2c3614f43de5": {"__data__": {"id_": "1cbf0323-5071-4235-92af-2c3614f43de5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "727aee78-1ea7-49a4-88b6-03d5f68aad5c", "node_type": "1", "metadata": {}, "hash": "ba0d7081fb11172f7a1478ec937dba906de8d92d785000e2b508900502db3795", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbcef697-a7a6-485c-b633-32b07ed65641", "node_type": "1", "metadata": {}, "hash": "9dbfda37b073ed0c6214f19db51d5a4adb5aeeb309570b5bda16cea886a7eea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "leading to better resource \nutilization and reduced execution time\nAQE workflow\nLet\u2019s look at how AQE optimizes workflows in Spark 3.0:\n\u2022\t Runtime statistics collection: During query execution, Spark collects statistics related to data \ndistribution, partition sizes, and join keys\u2019 cardinality\n\u2022\t Adaptive optimization: Utilizing the collected statistics, Spark dynamically adjusts the query \nexecution plan, optimizing join strategies, partitioning methods, and data redistribution techniques\n\u2022\t Enhanced performance: The adaptive optimization ensures that Spark adapts to changing data \nand runtime conditions, resulting in improved query performance and resource utilization\n\nOptimizations in Apache Spark\n\nAQE in Apache Spark represents a significant advancement in query optimization, moving beyond \nstatic planning to adapt to runtime conditions and data characteristics. By dynamically adjusting \nexecution plans based on real-time statistics, it optimizes query performance, ensuring efficient and \nscalable processing of large-scale datasets.", "mimetype": "text/plain", "start_char_idx": 166220, "end_char_idx": 167272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbcef697-a7a6-485c-b633-32b07ed65641": {"__data__": {"id_": "bbcef697-a7a6-485c-b633-32b07ed65641", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cbf0323-5071-4235-92af-2c3614f43de5", "node_type": "1", "metadata": {}, "hash": "3f74bbf551f9fbf74c3398c7c8ddfe83bbba1bf9475ad99121523a650d6a948a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27185349-25e4-4e6d-91da-e59d0b74a82f", "node_type": "1", "metadata": {}, "hash": "5887ee93b51ca55d1994fc72b180751146c111b66c240c5202bdc153eb372901", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "resulting in improved query performance and resource utilization\n\nOptimizations in Apache Spark\n\nAQE in Apache Spark represents a significant advancement in query optimization, moving beyond \nstatic planning to adapt to runtime conditions and data characteristics. By dynamically adjusting \nexecution plans based on real-time statistics, it optimizes query performance, ensuring efficient and \nscalable processing of large-scale datasets.\nNext, we will see how Spark does cost-based optimizations.\nCost-based optimization\nSpark estimates the cost of executing different query plans based on factors such as data size, join \noperations, and shuffle stages. It utilizes cost estimates to select the most efficient query execution plan.\nHere are the benefits:\n\u2022\t Optimal plan selection: Cost-based optimization chooses the most cost-effective execution \nplan while considering factors such as join strategies and data distribution\n\u2022\t Performance improvement: Minimizing unnecessary shuffling and computations improves \nquery performance\nNext, we will see how Spark utilizes memory management and tuning for optimizations.", "mimetype": "text/plain", "start_char_idx": 166834, "end_char_idx": 167952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27185349-25e4-4e6d-91da-e59d0b74a82f": {"__data__": {"id_": "27185349-25e4-4e6d-91da-e59d0b74a82f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbcef697-a7a6-485c-b633-32b07ed65641", "node_type": "1", "metadata": {}, "hash": "9dbfda37b073ed0c6214f19db51d5a4adb5aeeb309570b5bda16cea886a7eea8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65148b00-abc0-42fa-afc1-0f1e2bd69461", "node_type": "1", "metadata": {}, "hash": "8c457b49092f702eb0a5135e9dc7dc3b9f566c1e9f2bce8617d701eee796614a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It utilizes cost estimates to select the most efficient query execution plan.\nHere are the benefits:\n\u2022\t Optimal plan selection: Cost-based optimization chooses the most cost-effective execution \nplan while considering factors such as join strategies and data distribution\n\u2022\t Performance improvement: Minimizing unnecessary shuffling and computations improves \nquery performance\nNext, we will see how Spark utilizes memory management and tuning for optimizations.\nMemory management and tuning\nSpark also applies efficient memory allocation strategies, including storage and execution memory, to \navoid unnecessary spills and improve processing. It fine-tunes garbage collection settings to minimize \ninterruptions and improve overall job performance.", "mimetype": "text/plain", "start_char_idx": 167490, "end_char_idx": 168239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65148b00-abc0-42fa-afc1-0f1e2bd69461": {"__data__": {"id_": "65148b00-abc0-42fa-afc1-0f1e2bd69461", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27185349-25e4-4e6d-91da-e59d0b74a82f", "node_type": "1", "metadata": {}, "hash": "5887ee93b51ca55d1994fc72b180751146c111b66c240c5202bdc153eb372901", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31c25542-a888-4561-b4a9-aa0a07a08d85", "node_type": "1", "metadata": {}, "hash": "062dadaf3b320165d12e0ecf636932678152540f3162897cbbedf5372628d537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Memory management and tuning\nSpark also applies efficient memory allocation strategies, including storage and execution memory, to \navoid unnecessary spills and improve processing. It fine-tunes garbage collection settings to minimize \ninterruptions and improve overall job performance.\nHere are its benefits:\n\u2022\t Reduced overheads: Optimized memory usage minimizes unnecessary spills to disk, reducing \noverheads and improving job performance\n\u2022\t Stability and reliability: Tuned garbage collection settings enhance stability and reduce pauses, \nensuring more consistent job execution\nAdvanced Spark optimization techniques, including AQE, cost-based optimization, the Catalyst \noptimizer, and memory management, play a vital role in improving Spark job performance, resource \nutilization, and overall efficiency. By leveraging these techniques, users can optimize Spark applications \nto meet varying data processing demands and enhance their scalability and performance.\nSo far, we have seen how Spark optimizes its query plans internally.", "mimetype": "text/plain", "start_char_idx": 167953, "end_char_idx": 168992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31c25542-a888-4561-b4a9-aa0a07a08d85": {"__data__": {"id_": "31c25542-a888-4561-b4a9-aa0a07a08d85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65148b00-abc0-42fa-afc1-0f1e2bd69461", "node_type": "1", "metadata": {}, "hash": "8c457b49092f702eb0a5135e9dc7dc3b9f566c1e9f2bce8617d701eee796614a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8915a20-ad26-4ff9-bcd0-751c77caac3c", "node_type": "1", "metadata": {}, "hash": "88cf1fbac0868228295cc213bafd49c5d5678c0eee781a822e113eea336d3cee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By leveraging these techniques, users can optimize Spark applications \nto meet varying data processing demands and enhance their scalability and performance.\nSo far, we have seen how Spark optimizes its query plans internally. However, there are other \noptimizations that users can implement to make Spark\u2019s performance even better. We will discuss \nsome of these optimizations next.\n\nAdvanced Operations and Optimizations in Spark\n\nData-based optimizations in Apache Spark\nIn addition to Spark\u2019s inner optimizations, there are certain things we can take care of in terms of \nimplementation to make Spark more efficient. These are user-controlled optimizations. If we are aware \nof these challenges and how to handle them in real-world data applications, we can utilize Spark\u2019s \ndistributed architecture to its fullest.\nWe\u2019ll start by looking at a very common occurrence in distributed frameworks called the small \nfile problem.", "mimetype": "text/plain", "start_char_idx": 168766, "end_char_idx": 169694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8915a20-ad26-4ff9-bcd0-751c77caac3c": {"__data__": {"id_": "b8915a20-ad26-4ff9-bcd0-751c77caac3c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31c25542-a888-4561-b4a9-aa0a07a08d85", "node_type": "1", "metadata": {}, "hash": "062dadaf3b320165d12e0ecf636932678152540f3162897cbbedf5372628d537", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c", "node_type": "1", "metadata": {}, "hash": "9a8f936411516a218953aa3ba030ce2fd0828273550cb5d7d892a78a90e70082", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are user-controlled optimizations. If we are aware \nof these challenges and how to handle them in real-world data applications, we can utilize Spark\u2019s \ndistributed architecture to its fullest.\nWe\u2019ll start by looking at a very common occurrence in distributed frameworks called the small \nfile problem.\nAddressing the small file problem in Apache Spark\nThe small file problem poses a significant challenge in distributed computing frameworks such as \nApache Spark as it impacts performance and efficiency. It arises when data is stored in numerous small \nfiles rather than consolidated in larger files, leading to increased overhead and suboptimal resource \nutilization. In this section, we\u2019ll delve into the implications of the small file problem in Spark and \nexplore effective solutions to mitigate its effects.", "mimetype": "text/plain", "start_char_idx": 169387, "end_char_idx": 170206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c": {"__data__": {"id_": "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8915a20-ad26-4ff9-bcd0-751c77caac3c", "node_type": "1", "metadata": {}, "hash": "88cf1fbac0868228295cc213bafd49c5d5678c0eee781a822e113eea336d3cee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "529858d9-8603-4c08-ba49-00545531b1b4", "node_type": "1", "metadata": {}, "hash": "89af4ba0d213aedc764fbb0f5e7f974f8264eacbf2d5141ce6ce9c699995dc26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It arises when data is stored in numerous small \nfiles rather than consolidated in larger files, leading to increased overhead and suboptimal resource \nutilization. In this section, we\u2019ll delve into the implications of the small file problem in Spark and \nexplore effective solutions to mitigate its effects.\nThe key challenges associated with the small file problem are as follows:\n\u2022\t Increased metadata overhead: Storing data in numerous small files leads to higher metadata \noverhead as each file occupies a separate block and incurs additional I/O operations for \nfile handling\n\u2022\t Reduced throughput: Processing numerous small files is less efficient as it involves a high level \nof overhead for opening, reading, and closing files, resulting in reduced throughput\n\u2022\t Inefficient resource utilization: Spark\u2019s parallelism relies on data partitioning, and small files \ncan lead to inadequate partitioning, underutilizing resources, and hindering parallel processing\nNow that we\u2019ve discussed the key challenges, let\u2019s discuss some solutions to mitigate the small file problem:\n\u2022\t File concatenation or merging: Consolidating small files into larger files can significantly alleviate \nthe small file problem.", "mimetype": "text/plain", "start_char_idx": 169898, "end_char_idx": 171107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "529858d9-8603-4c08-ba49-00545531b1b4": {"__data__": {"id_": "529858d9-8603-4c08-ba49-00545531b1b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c", "node_type": "1", "metadata": {}, "hash": "9a8f936411516a218953aa3ba030ce2fd0828273550cb5d7d892a78a90e70082", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efca3ca7-2c18-4498-aa42-d647297b6ad7", "node_type": "1", "metadata": {}, "hash": "47ac06dfb3be6460288d5f0db8c0ca644cef5c60aadfc16c442db34a6127c030", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Techniques such as file concatenation or merging, either manually or \nthrough automated processes, help reduce the number of individual files.\n\u2022\t File compaction or coalescing: Tools or processes that compact or coalesce small files into \nfewer, more substantial files can streamline data storage. This consolidation reduces metadata \noverhead and enhances data access efficiency.\n\u2022\t File format optimization: Choosing efficient file formats such as Parquet or ORC, which \nsupport columnar storage and compression, can reduce the impact of small files. These formats \nfacilitate efficient data access and reduce storage space.\n\u2022\t Partitioning strategies: Applying appropriate partitioning strategies during data ingestion or \nprocessing in Spark can mitigate the effects of the small file problem. It involves organizing \ndata into larger partitions to improve parallelism.\n\nData-based optimizations in Apache Spark\n\n\u2022\t Data prefetching or caching: Prefetching or caching small files into memory before processing \ncan minimize I/O overhead.", "mimetype": "text/plain", "start_char_idx": 171108, "end_char_idx": 172149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efca3ca7-2c18-4498-aa42-d647297b6ad7": {"__data__": {"id_": "efca3ca7-2c18-4498-aa42-d647297b6ad7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "529858d9-8603-4c08-ba49-00545531b1b4", "node_type": "1", "metadata": {}, "hash": "89af4ba0d213aedc764fbb0f5e7f974f8264eacbf2d5141ce6ce9c699995dc26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "827a96ad-1d49-4efa-b344-7152368b2f5d", "node_type": "1", "metadata": {}, "hash": "c41ba464e46b154ce214c8abc41abbcce43b68adb8cd1952a13ff6d9ed227ce8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Partitioning strategies: Applying appropriate partitioning strategies during data ingestion or \nprocessing in Spark can mitigate the effects of the small file problem. It involves organizing \ndata into larger partitions to improve parallelism.\n\nData-based optimizations in Apache Spark\n\n\u2022\t Data prefetching or caching: Prefetching or caching small files into memory before processing \ncan minimize I/O overhead. Techniques such as caching or loading data into memory using \nSpark\u2019s capabilities can improve performance.\n\u2022\t AQE: Leveraging Spark\u2019s AQE features helps optimize query plans based on runtime statistics. \nThis can mitigate the impact of small files during query execution.\n\u2022\t Data lake architectural changes: Reevaluating the data lake architecture and adopting data \ningestion strategies that minimize the creation of small files can prevent the problem at its source.", "mimetype": "text/plain", "start_char_idx": 171735, "end_char_idx": 172619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "827a96ad-1d49-4efa-b344-7152368b2f5d": {"__data__": {"id_": "827a96ad-1d49-4efa-b344-7152368b2f5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efca3ca7-2c18-4498-aa42-d647297b6ad7", "node_type": "1", "metadata": {}, "hash": "47ac06dfb3be6460288d5f0db8c0ca644cef5c60aadfc16c442db34a6127c030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4fc6b85-7233-4ad0-8178-1217f14817a7", "node_type": "1", "metadata": {}, "hash": "b36a3a32cefe111c84d6efd3222735b1f241c0201087bcaf1f0ddb49a8cc86ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Techniques such as caching or loading data into memory using \nSpark\u2019s capabilities can improve performance.\n\u2022\t AQE: Leveraging Spark\u2019s AQE features helps optimize query plans based on runtime statistics. \nThis can mitigate the impact of small files during query execution.\n\u2022\t Data lake architectural changes: Reevaluating the data lake architecture and adopting data \ningestion strategies that minimize the creation of small files can prevent the problem at its source.\nLet\u2019s look at the best practices for handling small files:\n\u2022\t Regular monitoring and cleanup: Implement regular monitoring and cleanup processes to \nidentify and merge small files that are generated over time\n\u2022\t Optimize the storage layout: Design data storage layouts that minimize the creation of small \nfiles while considering factors such as block size and filesystem settings\n\u2022\t Automated processes: Use automated processes or tools to consolidate and manage small files \nefficiently, reducing manual effort\n\u2022\t Educate data producers: Educate data producers on the impact of small files and encourage \npractices that generate larger files or optimize file creation\nBy adopting these strategies and best practices, organizations can effectively mitigate the small file \nproblem in Apache Spark, ensuring improved performance, enhanced resource utilization, and efficient \ndata processing capabilities.", "mimetype": "text/plain", "start_char_idx": 172150, "end_char_idx": 173525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4fc6b85-7233-4ad0-8178-1217f14817a7": {"__data__": {"id_": "a4fc6b85-7233-4ad0-8178-1217f14817a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "827a96ad-1d49-4efa-b344-7152368b2f5d", "node_type": "1", "metadata": {}, "hash": "c41ba464e46b154ce214c8abc41abbcce43b68adb8cd1952a13ff6d9ed227ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "278365a7-7adb-420b-9c85-a10842275f43", "node_type": "1", "metadata": {}, "hash": "8c7ff1a0337642a3010cfa89b4639747da71dac2fbd3660bec9a0a64036a6a67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These approaches empower users to overcome the challenges posed by \nthe small file problem and optimize their Spark workflows for optimal performance and scalability.\nNext, we will see how data skew affects performance in Spark.\nTackling data skew in Apache Spark\nData skew presents a significant challenge in distributed data processing frameworks such as Apache \nSpark, causing uneven workload distribution and hindering parallelism.\nData skew occurs when certain keys or partitions hold significantly more data than others. This \nimbalance leads to unequal processing times for different partitions, causing stragglers. Skewed data \ndistribution can result in certain worker nodes being overloaded while others remain underutilized, \nleading to inefficient resource allocation. Tasks that deal with skewed data partitions take longer to \ncomplete, causing delays in job execution and affecting overall performance.", "mimetype": "text/plain", "start_char_idx": 173526, "end_char_idx": 174443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "278365a7-7adb-420b-9c85-a10842275f43": {"__data__": {"id_": "278365a7-7adb-420b-9c85-a10842275f43", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4fc6b85-7233-4ad0-8178-1217f14817a7", "node_type": "1", "metadata": {}, "hash": "b36a3a32cefe111c84d6efd3222735b1f241c0201087bcaf1f0ddb49a8cc86ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0300cdd-ff72-4a91-a797-1bec8fda1677", "node_type": "1", "metadata": {}, "hash": "55f103441669c509ef9c7b41b4cbb1fada07bfae0aa12d788c9f1e278cf340f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data skew occurs when certain keys or partitions hold significantly more data than others. This \nimbalance leads to unequal processing times for different partitions, causing stragglers. Skewed data \ndistribution can result in certain worker nodes being overloaded while others remain underutilized, \nleading to inefficient resource allocation. Tasks that deal with skewed data partitions take longer to \ncomplete, causing delays in job execution and affecting overall performance.\n\nAdvanced Operations and Optimizations in Spark\n\nHere are some of the solutions we can use to address data skew:\n\u2022\t Partitioning techniques:\n\t\u0082 Salting: Introduce randomness by adding a salt to keys to distribute data more evenly across \npartitions. This helps prevent hotspots and balances the workload.\n\t\u0082 Custom partitioning: Implement custom partitioning logic to redistribute skewed data by \ngrouping keys differently, ensuring a more balanced distribution across partitions.\n\u2022\t Skew-aware algorithms: Utilize techniques such as skew join optimization,", "mimetype": "text/plain", "start_char_idx": 173962, "end_char_idx": 175001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0300cdd-ff72-4a91-a797-1bec8fda1677": {"__data__": {"id_": "e0300cdd-ff72-4a91-a797-1bec8fda1677", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "278365a7-7adb-420b-9c85-a10842275f43", "node_type": "1", "metadata": {}, "hash": "8c7ff1a0337642a3010cfa89b4639747da71dac2fbd3660bec9a0a64036a6a67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b7b4486-5104-46f9-a50b-a08c8b5a6093", "node_type": "1", "metadata": {}, "hash": "bef14d001cb675c5ba926c7322d63b6613879ff393adf22a69f0a9ecb041667f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This helps prevent hotspots and balances the workload.\n\t\u0082 Custom partitioning: Implement custom partitioning logic to redistribute skewed data by \ngrouping keys differently, ensuring a more balanced distribution across partitions.\n\u2022\t Skew-aware algorithms: Utilize techniques such as skew join optimization, which handles \nskewed keys separately from regular joins, redistributing and processing them more efficiently\n\u2022\t Replicate small-skewed data: Replicate small skewed partitions across multiple nodes to \nparallelize processing and alleviate the load on individual nodes\n\u2022\t AQE: Leverage Spark\u2019s AQE capabilities to dynamically adjust execution plans based on runtime \nstatistics, mitigating the impact of data skew\n\u2022\t Sampling and filtering: Apply sampling and filtering techniques to identify skewed data \npartitions beforehand, allowing for proactive handling of skewed keys during processing\n\u2022\t Dynamic resource allocation: Implement dynamic resource allocation to allocate additional \nresources to tasks dealing with skewed data partitions,", "mimetype": "text/plain", "start_char_idx": 174694, "end_char_idx": 175744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b7b4486-5104-46f9-a50b-a08c8b5a6093": {"__data__": {"id_": "1b7b4486-5104-46f9-a50b-a08c8b5a6093", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0300cdd-ff72-4a91-a797-1bec8fda1677", "node_type": "1", "metadata": {}, "hash": "55f103441669c509ef9c7b41b4cbb1fada07bfae0aa12d788c9f1e278cf340f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ba9b9bd-8df4-4c5f-89c9-313880c233fd", "node_type": "1", "metadata": {}, "hash": "b92a2f1b1cea9c1603384d1f9d55acb419e408a81f66c4454362c8403e4c0532", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mitigating the impact of data skew\n\u2022\t Sampling and filtering: Apply sampling and filtering techniques to identify skewed data \npartitions beforehand, allowing for proactive handling of skewed keys during processing\n\u2022\t Dynamic resource allocation: Implement dynamic resource allocation to allocate additional \nresources to tasks dealing with skewed data partitions, optimizing resource utilization\nLet\u2019s discuss the best practices for handling data skew:\n\u2022\t Regular profiling: Continuously profile and monitor data distribution to identify and address \nskew issues early in the processing pipeline\n\u2022\t Optimized partitioning: Choose appropriate partitioning strategies based on data characteristics \nto prevent or mitigate data skew\n\u2022\t Distributed processing: Leverage distributed processing frameworks to distribute skewed data \nacross multiple nodes for parallel execution\n\u2022\t Task retry mechanisms: Implement retry mechanisms for tasks dealing with skewed data to \naccommodate potential delays and avoid job failures\n\u2022\t Data preprocessing: Apply preprocessing techniques to mitigate skew before data processing,", "mimetype": "text/plain", "start_char_idx": 175380, "end_char_idx": 176491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ba9b9bd-8df4-4c5f-89c9-313880c233fd": {"__data__": {"id_": "3ba9b9bd-8df4-4c5f-89c9-313880c233fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b7b4486-5104-46f9-a50b-a08c8b5a6093", "node_type": "1", "metadata": {}, "hash": "bef14d001cb675c5ba926c7322d63b6613879ff393adf22a69f0a9ecb041667f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2a6e367-9c40-4ed4-8868-565271494414", "node_type": "1", "metadata": {}, "hash": "be0e1e8d71ea4af419294ec3797a3df1c938223d3c848478c8e50d1418267e39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ensuring a more balanced workload\nBy employing these strategies and best practices, organizations can effectively combat data skew in \nApache Spark, ensuring more balanced workloads, improved resource utilization, and enhanced \noverall performance in distributed data processing workflows. These approaches empower users to \novercome the challenges posed by data skew and optimize Spark applications for efficient and scalable \ndata processing.\n\nData-based optimizations in Apache Spark\n\nAddressing data skew in Apache Spark is critical for optimizing performance and ensuring efficient \nresource utilization in distributed computing environments. By understanding the causes and impacts \nof data skew and employing mitigation strategies users can significantly improve the efficiency and \nreliability of Spark jobs, mitigating the adverse effects of data skew.\nIn the next section, we will talk about data spills in Spark and how to manage them.", "mimetype": "text/plain", "start_char_idx": 176493, "end_char_idx": 177439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2a6e367-9c40-4ed4-8868-565271494414": {"__data__": {"id_": "f2a6e367-9c40-4ed4-8868-565271494414", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ba9b9bd-8df4-4c5f-89c9-313880c233fd", "node_type": "1", "metadata": {}, "hash": "b92a2f1b1cea9c1603384d1f9d55acb419e408a81f66c4454362c8403e4c0532", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3", "node_type": "1", "metadata": {}, "hash": "e41621285535813ff4c1ea9c5e5d5b5511ac307bca5f76f8fe38142163237b53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data-based optimizations in Apache Spark\n\nAddressing data skew in Apache Spark is critical for optimizing performance and ensuring efficient \nresource utilization in distributed computing environments. By understanding the causes and impacts \nof data skew and employing mitigation strategies users can significantly improve the efficiency and \nreliability of Spark jobs, mitigating the adverse effects of data skew.\nIn the next section, we will talk about data spills in Spark and how to manage them.\nManaging data spills in Apache Spark\nData spill, something that\u2019s often encountered in distributed processing frameworks such as Apache \nSpark, occurs when the data being processed exceeds the available memory capacity, leading to data \nbeing written to disk. This phenomenon can significantly impact performance and overall efficiency. \nIn this section, we\u2019ll delve into the implications of data spill in Spark and effective strategies to mitigate \nits effects for optimized data processing.", "mimetype": "text/plain", "start_char_idx": 176939, "end_char_idx": 177932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3": {"__data__": {"id_": "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2a6e367-9c40-4ed4-8868-565271494414", "node_type": "1", "metadata": {}, "hash": "be0e1e8d71ea4af419294ec3797a3df1c938223d3c848478c8e50d1418267e39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebf56c4e-4e13-4b06-92b4-331a09d3b374", "node_type": "1", "metadata": {}, "hash": "2dfa33e5a98eb41d08c8b7bcb3d5df9a16c17eb85ba2789b2db2e886eb8cc94c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Managing data spills in Apache Spark\nData spill, something that\u2019s often encountered in distributed processing frameworks such as Apache \nSpark, occurs when the data being processed exceeds the available memory capacity, leading to data \nbeing written to disk. This phenomenon can significantly impact performance and overall efficiency. \nIn this section, we\u2019ll delve into the implications of data spill in Spark and effective strategies to mitigate \nits effects for optimized data processing.\nData spill occurs when Spark\u2019s memory capacity is exceeded, resulting in excessive data write operations \nto disk, which are significantly slower than in-memory operations. Writing data to disk incurs high \nI/O overhead, leading to a substantial degradation in processing performance due to increased latency. \nData spillage can cause resource contention as disk operations compete with other computing tasks, \nleading to inefficient resource utilization.", "mimetype": "text/plain", "start_char_idx": 177440, "end_char_idx": 178388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebf56c4e-4e13-4b06-92b4-331a09d3b374": {"__data__": {"id_": "ebf56c4e-4e13-4b06-92b4-331a09d3b374", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3", "node_type": "1", "metadata": {}, "hash": "e41621285535813ff4c1ea9c5e5d5b5511ac307bca5f76f8fe38142163237b53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f635381b-6b58-4a99-bed7-488cab94aefd", "node_type": "1", "metadata": {}, "hash": "f7f9a050e58a68cea6fc38c8769fce474556b4060b014552f1b50fa589b3994f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data spill occurs when Spark\u2019s memory capacity is exceeded, resulting in excessive data write operations \nto disk, which are significantly slower than in-memory operations. Writing data to disk incurs high \nI/O overhead, leading to a substantial degradation in processing performance due to increased latency. \nData spillage can cause resource contention as disk operations compete with other computing tasks, \nleading to inefficient resource utilization.\nHere are some of the solutions we can implement to address data spill:\n\u2022\t Memory management techniques:\n\t\u0082 Increase executor memory: Allocating more memory to Spark executors can help reduce \nthe likelihood of data spill by accommodating larger datasets in memory\n\t\u0082 Tune memory configuration: Optimize Spark\u2019s memory configurations, such as adjusting \nmemory fractions for storage and execution,", "mimetype": "text/plain", "start_char_idx": 177933, "end_char_idx": 178785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f635381b-6b58-4a99-bed7-488cab94aefd": {"__data__": {"id_": "f635381b-6b58-4a99-bed7-488cab94aefd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebf56c4e-4e13-4b06-92b4-331a09d3b374", "node_type": "1", "metadata": {}, "hash": "2dfa33e5a98eb41d08c8b7bcb3d5df9a16c17eb85ba2789b2db2e886eb8cc94c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe", "node_type": "1", "metadata": {}, "hash": "1d7cffdf50e0883c107d81c0cd2ee765741fc08b97f0bd3acdca56ebf67fcd21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here are some of the solutions we can implement to address data spill:\n\u2022\t Memory management techniques:\n\t\u0082 Increase executor memory: Allocating more memory to Spark executors can help reduce \nthe likelihood of data spill by accommodating larger datasets in memory\n\t\u0082 Tune memory configuration: Optimize Spark\u2019s memory configurations, such as adjusting \nmemory fractions for storage and execution, to better utilize available memory\n\u2022\t Partitioning and caching strategies:\n\t\u0082 Repartitioning: Repartitioning data into an optimal number of partitions can help manage \nmemory usage and minimize data spills by ensuring better data distribution across nodes\n\t\u0082 Caching intermediate results: Caching or persisting intermediate datasets in memory can \nprevent recomputation and reduce the chances of data spill during subsequent operations\n\u2022\t Advanced optimization techniques:\n\t\u0082 Shuffle tuning: Tune shuffle operations by adjusting parameters such as shuffle partitions \nand buffer sizes to reduce the likelihood of data spill during shuffle phases\n\t\u0082 Data compression: Utilize data compression techniques when storing intermediate data in \nmemory or on disk to reduce the storage footprint and alleviate memory pressure\n\nAdvanced Operations and Optimizations in Spark\n\n\u2022\t AQE: Leverage Spark\u2019s AQE capabilities to dynamically adjust execution plans based on runtime \nstatistics,", "mimetype": "text/plain", "start_char_idx": 178389, "end_char_idx": 179762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe": {"__data__": {"id_": "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f635381b-6b58-4a99-bed7-488cab94aefd", "node_type": "1", "metadata": {}, "hash": "f7f9a050e58a68cea6fc38c8769fce474556b4060b014552f1b50fa589b3994f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "542fe48c-dee1-47dc-8ca7-75936d21d08f", "node_type": "1", "metadata": {}, "hash": "578c9c87394e9718c482c28fd9502bed50b04d03161744dc48e12d49681616c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "optimizing memory usage and reducing spillage.\n\u2022\t Task and data skew handling: Apply techniques to mitigate task and data skew. Skewed data \ncan exacerbate memory pressure and increase the chances of data spill.", "mimetype": "text/plain", "start_char_idx": 179763, "end_char_idx": 179974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "542fe48c-dee1-47dc-8ca7-75936d21d08f": {"__data__": {"id_": "542fe48c-dee1-47dc-8ca7-75936d21d08f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe", "node_type": "1", "metadata": {}, "hash": "1d7cffdf50e0883c107d81c0cd2ee765741fc08b97f0bd3acdca56ebf67fcd21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c63f546a-24f7-4733-a52c-013e7b99cb9a", "node_type": "1", "metadata": {}, "hash": "280635a5aaf56305113720bd91a9639ea527a8d986a75cadc8a473d6f3ddb627", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "optimizing memory usage and reducing spillage.\n\u2022\t Task and data skew handling: Apply techniques to mitigate task and data skew. Skewed data \ncan exacerbate memory pressure and increase the chances of data spill.\nHere are the best practices for handling data spills:\n\u2022\t Resource monitoring: Regularly monitor memory usage and resource allocation to identify \nand preempt potential data spillage issues\n\u2022\t Optimized data structures: Utilize optimized data structures and formats (such as Parquet or \nORC) to reduce memory overhead and storage requirements\n\u2022\t Efficient caching strategies: Strategically cache or persist intermediate results to minimize \nrecomputation and reduce the probability of data spill\n\u2022\t Incremental processing: Employ incremental processing techniques to handle large datasets \nin manageable chunks, reducing memory pressure\nBy adopting these strategies and best practices, organizations can effectively manage data spillage \nin Apache Spark, ensuring efficient memory utilization, optimized processing performance, and \nenhanced overall scalability in distributed data processing workflows.", "mimetype": "text/plain", "start_char_idx": 179763, "end_char_idx": 180877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c63f546a-24f7-4733-a52c-013e7b99cb9a": {"__data__": {"id_": "c63f546a-24f7-4733-a52c-013e7b99cb9a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "542fe48c-dee1-47dc-8ca7-75936d21d08f", "node_type": "1", "metadata": {}, "hash": "578c9c87394e9718c482c28fd9502bed50b04d03161744dc48e12d49681616c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a59884e-fd56-4b15-a9ea-4947007b65b7", "node_type": "1", "metadata": {}, "hash": "5ae0408cd150117425f267659d7873eb8111c6882b7b0b522d8e37a092f95df2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These approaches empower \nusers to proactively address data spillage challenges and optimize Spark applications for improved \nefficiency and performance.\nIn the next section, we will talk about what data shuffle is and how to handle it to optimize performance.\nManaging data shuffle in Apache Spark\nData shuffle, a fundamental operation in distributed processing frameworks such as Apache Spark, \ninvolves moving data across nodes in the cluster. While shuffle operations are essential for various \ntransformations, such as joins and aggregations, they can also introduce performance bottlenecks and \nresource overhead. In this section, we\u2019ll explore the implications of data shuffle in Spark and effective \nstrategies to optimize and mitigate its impact for efficient data processing.\nData shuffle involves extensive network and disk I/O operations, leading to increased latency and \nresource utilization. Shuffling large amounts of data across nodes can introduce performance \nbottlenecks due to excessive data movement and processing.", "mimetype": "text/plain", "start_char_idx": 180878, "end_char_idx": 181915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a59884e-fd56-4b15-a9ea-4947007b65b7": {"__data__": {"id_": "3a59884e-fd56-4b15-a9ea-4947007b65b7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c63f546a-24f7-4733-a52c-013e7b99cb9a", "node_type": "1", "metadata": {}, "hash": "280635a5aaf56305113720bd91a9639ea527a8d986a75cadc8a473d6f3ddb627", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ec86be3-a5a9-42db-b554-9b804aa096d3", "node_type": "1", "metadata": {}, "hash": "39383f516164c2a2142c924abb28151b25ee6e3dc7ba1e583192176585b0cfb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we\u2019ll explore the implications of data shuffle in Spark and effective \nstrategies to optimize and mitigate its impact for efficient data processing.\nData shuffle involves extensive network and disk I/O operations, leading to increased latency and \nresource utilization. Shuffling large amounts of data across nodes can introduce performance \nbottlenecks due to excessive data movement and processing. Intensive shuffle operations can cause \nresource contention among nodes, impacting overall cluster performance.\nLet\u2019s discuss the solutions for optimizing data shuffle:\n\u2022\t Data partitioning techniques: Implement optimized data partitioning strategies to reduce \nshuffle overhead,", "mimetype": "text/plain", "start_char_idx": 181498, "end_char_idx": 182195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ec86be3-a5a9-42db-b554-9b804aa096d3": {"__data__": {"id_": "4ec86be3-a5a9-42db-b554-9b804aa096d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a59884e-fd56-4b15-a9ea-4947007b65b7", "node_type": "1", "metadata": {}, "hash": "5ae0408cd150117425f267659d7873eb8111c6882b7b0b522d8e37a092f95df2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca2672f-a0f1-437f-a468-a331230b858d", "node_type": "1", "metadata": {}, "hash": "4261515efd0a4d1ef7ce0fb819191047c859fb59daea878cd41c9dabb7e9fc2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Shuffling large amounts of data across nodes can introduce performance \nbottlenecks due to excessive data movement and processing. Intensive shuffle operations can cause \nresource contention among nodes, impacting overall cluster performance.\nLet\u2019s discuss the solutions for optimizing data shuffle:\n\u2022\t Data partitioning techniques: Implement optimized data partitioning strategies to reduce \nshuffle overhead, ensuring a more balanced workload distribution\n\u2022\t Skew handling: Mitigate data skew by employing techniques such as salting or custom \npartitioning to prevent hotspots and balance data distribution\n\nData-based optimizations in Apache Spark\n\n\u2022\t Shuffle partitions adjustment: Tune the number of shuffle partitions based on data characteristics \nand job requirements to optimize shuffle performance and reduce overhead\n\u2022\t Memory management: Optimize memory allocation for shuffle operations to minimize spills \nto disk and improve overall shuffle performance\n\u2022\t Data filtering and pruning: Apply filtering or pruning techniques to reduce the amount of \ndata shuffled across nodes,", "mimetype": "text/plain", "start_char_idx": 181785, "end_char_idx": 182874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dca2672f-a0f1-437f-a468-a331230b858d": {"__data__": {"id_": "dca2672f-a0f1-437f-a468-a331230b858d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ec86be3-a5a9-42db-b554-9b804aa096d3", "node_type": "1", "metadata": {}, "hash": "39383f516164c2a2142c924abb28151b25ee6e3dc7ba1e583192176585b0cfb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92701578-19f9-410c-9b3b-68375798b1ca", "node_type": "1", "metadata": {}, "hash": "4304a115e1bc30c8a475dd1ec5a52522254dc0b2a5feb0af97a917c47db5f93b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "focusing only on relevant subsets of data\n\u2022\t Join optimization:\n\t\u0082 Broadcast joins: Utilize broadcast joins for smaller datasets to replicate them across nodes,", "mimetype": "text/plain", "start_char_idx": 182875, "end_char_idx": 183035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92701578-19f9-410c-9b3b-68375798b1ca": {"__data__": {"id_": "92701578-19f9-410c-9b3b-68375798b1ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca2672f-a0f1-437f-a468-a331230b858d", "node_type": "1", "metadata": {}, "hash": "4261515efd0a4d1ef7ce0fb819191047c859fb59daea878cd41c9dabb7e9fc2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14b60704-7953-4ed0-8cd9-1a4746ad1992", "node_type": "1", "metadata": {}, "hash": "7c81552c30877f11612fd54cefa5458881d400cf07711497ff15f45a24137195", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "focusing only on relevant subsets of data\n\u2022\t Join optimization:\n\t\u0082 Broadcast joins: Utilize broadcast joins for smaller datasets to replicate them across nodes, \nminimizing data shuffling and improving join performance\n\t\u0082 Sort-merge joins: Employ sort-merge join algorithms for large datasets to minimize data \nmovement during join operations\n\u2022\t AQE: Leverage Spark\u2019s AQE capabilities to dynamically optimize shuffle operations based on \nruntime statistics and data distribution\nThe best practices for managing data shuffle are as follows:\n\u2022\t Profile and monitor: Continuously profile and monitor shuffle operations to identify bottlenecks \nand optimize configurations\n\u2022\t Optimized partition sizes: Determine optimal partition sizes based on data characteristics \nand adjust shuffle partitioning accordingly\n\u2022\t Caching and persistence: Cache or persist intermediate shuffle results to reduce recomputation \nand mitigate shuffle overhead\n\u2022\t Regular tuning: Regularly tune Spark configurations related to shuffle operations based on \nworkload requirements and cluster resources\nBy implementing these strategies and best practices,", "mimetype": "text/plain", "start_char_idx": 182875, "end_char_idx": 184003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14b60704-7953-4ed0-8cd9-1a4746ad1992": {"__data__": {"id_": "14b60704-7953-4ed0-8cd9-1a4746ad1992", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92701578-19f9-410c-9b3b-68375798b1ca", "node_type": "1", "metadata": {}, "hash": "4304a115e1bc30c8a475dd1ec5a52522254dc0b2a5feb0af97a917c47db5f93b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41e3336e-316b-481c-a93f-8bdf0880e291", "node_type": "1", "metadata": {}, "hash": "7c545fa7b8d64eee77488744bb17322f69232e3a724d26a0a29741e4ada8d3a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "organizations can effectively optimize data \nshuffle operations in Apache Spark, ensuring improved performance, reduced resource contention, \nand enhanced overall efficiency in distributed data processing workflows. These approaches empower \nusers to proactively manage and optimize shuffle operations for streamlined data processing and \nimproved cluster performance.\nDespite all the data-related challenges that users need to be aware of, there are certain types of joins \nthat Spark has available in its internal working that we can utilize for better performance. We\u2019ll take \na look at these next.\n\nAdvanced Operations and Optimizations in Spark\n\nShuffle and broadcast joins\nApache Spark offers two fundamental approaches for performing join operations: shuffle joins and \nbroadcast joins. Each method has its advantages and use cases, and understanding when to use them \nis crucial for optimizing your Spark applications. Note that these joins are done by Spark automatically \nto join different datasets together. You can enforce some of the join types in your code but Spark takes \ncare of the execution.", "mimetype": "text/plain", "start_char_idx": 184004, "end_char_idx": 185114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41e3336e-316b-481c-a93f-8bdf0880e291": {"__data__": {"id_": "41e3336e-316b-481c-a93f-8bdf0880e291", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14b60704-7953-4ed0-8cd9-1a4746ad1992", "node_type": "1", "metadata": {}, "hash": "7c81552c30877f11612fd54cefa5458881d400cf07711497ff15f45a24137195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6678ef83-b426-4666-a2d3-72d33cbe5e87", "node_type": "1", "metadata": {}, "hash": "08680de64b09a3c9f1c05f93ebc3652ca173d8231bfee77d88cd99ecfae94ad4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each method has its advantages and use cases, and understanding when to use them \nis crucial for optimizing your Spark applications. Note that these joins are done by Spark automatically \nto join different datasets together. You can enforce some of the join types in your code but Spark takes \ncare of the execution.\nShuffle joins\nShuffle joins are a common method for joining large datasets in distributed computing environments. \nThese joins redistribute data across partitions, ensuring that matching keys end up on the same worker \nnodes. Spark performs shuffle joins efficiently thanks to its underlying execution engine.\nHere are some of the key characteristics of shuffle joins:\n\u2022\t Data redistribution: Shuffle joins redistribute data to ensure that rows with matching keys are \nco-located on the same worker nodes. This process may require substantial network and disk I/O.\n\u2022\t Suitable for large datasets: Shuffle joins are well-suited for joining large DataFrames with \ncomparable sizes.", "mimetype": "text/plain", "start_char_idx": 184798, "end_char_idx": 185794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6678ef83-b426-4666-a2d3-72d33cbe5e87": {"__data__": {"id_": "6678ef83-b426-4666-a2d3-72d33cbe5e87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41e3336e-316b-481c-a93f-8bdf0880e291", "node_type": "1", "metadata": {}, "hash": "7c545fa7b8d64eee77488744bb17322f69232e3a724d26a0a29741e4ada8d3a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcb43b7e-7506-4082-abd0-0e8c312a1966", "node_type": "1", "metadata": {}, "hash": "19df5ccad4138ec990ff50823d6812d96ab4f67e9aba2f0dc37b9d52b079929d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark performs shuffle joins efficiently thanks to its underlying execution engine.\nHere are some of the key characteristics of shuffle joins:\n\u2022\t Data redistribution: Shuffle joins redistribute data to ensure that rows with matching keys are \nco-located on the same worker nodes. This process may require substantial network and disk I/O.\n\u2022\t Suitable for large datasets: Shuffle joins are well-suited for joining large DataFrames with \ncomparable sizes.\n\u2022\t Replicating data: During a shuffle join, data may be temporarily replicated on worker nodes \nto facilitate efficient joins.\n\u2022\t Costly in terms of network and disk I/O: Shuffle joins can be resource-intensive due to data \nshuffling, making them slower compared to other join techniques for smaller datasets.\n\u2022\t Examples: Inner join, left join, right join, and full outer join are often implemented as shuffle joins.\nUse case\nShuffle joins are typically used when joining two large DataFrames with no significant size difference.", "mimetype": "text/plain", "start_char_idx": 185341, "end_char_idx": 186325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcb43b7e-7506-4082-abd0-0e8c312a1966": {"__data__": {"id_": "dcb43b7e-7506-4082-abd0-0e8c312a1966", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6678ef83-b426-4666-a2d3-72d33cbe5e87", "node_type": "1", "metadata": {}, "hash": "08680de64b09a3c9f1c05f93ebc3652ca173d8231bfee77d88cd99ecfae94ad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11b26dad-728e-44b1-8a97-aa93e0ea750e", "node_type": "1", "metadata": {}, "hash": "f3502fbd6d201ee4b28a6b9e4a82e65e1b0cccaed4732ea1e115b831e832969f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Costly in terms of network and disk I/O: Shuffle joins can be resource-intensive due to data \nshuffling, making them slower compared to other join techniques for smaller datasets.\n\u2022\t Examples: Inner join, left join, right join, and full outer join are often implemented as shuffle joins.\nUse case\nShuffle joins are typically used when joining two large DataFrames with no significant size difference.\nShuffle sort-merge joins\nA shuffle sort-merge join is a type of shuffle join that leverages a combination of sorting and merging \ntechniques to perform the join operation. It sorts both DataFrames based on the join key and then \nmerges them efficiently.", "mimetype": "text/plain", "start_char_idx": 185922, "end_char_idx": 186579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11b26dad-728e-44b1-8a97-aa93e0ea750e": {"__data__": {"id_": "11b26dad-728e-44b1-8a97-aa93e0ea750e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcb43b7e-7506-4082-abd0-0e8c312a1966", "node_type": "1", "metadata": {}, "hash": "19df5ccad4138ec990ff50823d6812d96ab4f67e9aba2f0dc37b9d52b079929d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e83b06-b357-4f31-a650-ce86436d5a83", "node_type": "1", "metadata": {}, "hash": "b7b7761f427b5295f225bc43a114c4bf4ada4854c7c2408a5664c9dcdd3347b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use case\nShuffle joins are typically used when joining two large DataFrames with no significant size difference.\nShuffle sort-merge joins\nA shuffle sort-merge join is a type of shuffle join that leverages a combination of sorting and merging \ntechniques to perform the join operation. It sorts both DataFrames based on the join key and then \nmerges them efficiently.\nHere are some of the key features of shuffle sort-merge joins:\n\u2022\t Data sorting: Shuffle sort-merge joins sort the data on both sides to ensure efficient merging\n\u2022\t Suitable for large datasets: They are efficient for joining large DataFrames with skewed \ndata distribution\n\nData-based optimizations in Apache Spark\n\n\u2022\t Complexity: This type of shuffle join is more complex than a simple shuffle join as it involves \nsorting operations\nUse case\nShuffle sort-merge joins are effective for large-scale joins, especially when the data distribution is \nskewed, and a balanced distribution of data across partitions is essential.", "mimetype": "text/plain", "start_char_idx": 186213, "end_char_idx": 187202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96e83b06-b357-4f31-a650-ce86436d5a83": {"__data__": {"id_": "96e83b06-b357-4f31-a650-ce86436d5a83", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11b26dad-728e-44b1-8a97-aa93e0ea750e", "node_type": "1", "metadata": {}, "hash": "f3502fbd6d201ee4b28a6b9e4a82e65e1b0cccaed4732ea1e115b831e832969f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c64efb4e-fce5-4301-8d52-723b6cf02edc", "node_type": "1", "metadata": {}, "hash": "75a88f1aabcbd689dd349377c8cfb301f7c3ab38459b531296183a4be03a610b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s look at broadcast joins next.\nBroadcast joins\nBroadcast joins are a highly efficient technique for joining a small DataFrame with a larger one. In this \napproach, the smaller DataFrame is broadcast to all worker nodes, eliminating the need for shuffling \ndata across the network. A broadcast join is a specific optimization technique that can be applied \nwhen one of the DataFrames is small enough to fit in memory. In this case, the small DataFrame is \nbroadcast to all worker nodes, avoiding costly shuffling.", "mimetype": "text/plain", "start_char_idx": 187203, "end_char_idx": 187720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c64efb4e-fce5-4301-8d52-723b6cf02edc": {"__data__": {"id_": "c64efb4e-fce5-4301-8d52-723b6cf02edc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e83b06-b357-4f31-a650-ce86436d5a83", "node_type": "1", "metadata": {}, "hash": "b7b7761f427b5295f225bc43a114c4bf4ada4854c7c2408a5664c9dcdd3347b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3635ae71-2ec9-4b0d-b2cf-565873e2018a", "node_type": "1", "metadata": {}, "hash": "61ca35c897b95132ea59ac190736fb7bd93d0e9f79c0d88bd7babf0e272dc2e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this \napproach, the smaller DataFrame is broadcast to all worker nodes, eliminating the need for shuffling \ndata across the network. A broadcast join is a specific optimization technique that can be applied \nwhen one of the DataFrames is small enough to fit in memory. In this case, the small DataFrame is \nbroadcast to all worker nodes, avoiding costly shuffling.\nLet\u2019s look at some of the key characteristics of broadcast joins:\n\u2022\t Small DataFrame broadcast: The smaller DataFrame is broadcast to all worker nodes, ensuring \nthat it is available locally\n\u2022\t Reduced network overhead: Broadcast joins significantly reduce network and disk I/O because \nthey avoid data shuffling\n\u2022\t Ideal for dimension tables: Broadcast joins are commonly used when joining a fact table with \nsmaller dimension tables, such as in data warehousing scenarios\n\u2022\t Efficient for small-to-large joins: They are efficient for joins where one DataFrame is significantly \nsmaller than the other\nUse case\nBroadcast joins are useful when you\u2019re joining a large DataFrame with a much smaller one, such as \njoining a fact table with dimension tables in a data warehouse.", "mimetype": "text/plain", "start_char_idx": 187353, "end_char_idx": 188495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3635ae71-2ec9-4b0d-b2cf-565873e2018a": {"__data__": {"id_": "3635ae71-2ec9-4b0d-b2cf-565873e2018a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c64efb4e-fce5-4301-8d52-723b6cf02edc", "node_type": "1", "metadata": {}, "hash": "75a88f1aabcbd689dd349377c8cfb301f7c3ab38459b531296183a4be03a610b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a50eded-d210-4ff3-aad8-33e5223d4030", "node_type": "1", "metadata": {}, "hash": "44b61bc43497e8b5bad56d8122bf5fbea133512533eecdd2d0847474f1e5db47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Broadcast hash joins\nA specific type of broadcast join is the broadcast hash join. In this variant, the smaller DataFrame is \nbroadcast as a hash table to all worker nodes, which allows for efficient lookups in the larger DataFrame.\nUse case\nBroadcast hash joins are suitable for scenarios where one DataFrame is small enough to be broadcast, \nand you need to perform equality-based joins.\n\nAdvanced Operations and Optimizations in Spark\n\nIn this section, we discussed two fundamental join techniques in Spark \u2013 shuffle joins and broadcast \njoins \u2013 including specific variants, such as the broadcast hash join and the shuffle sort-merge join. \nChoosing the right join method depends on the size of your DataFrames, data distribution, and network \nconsiderations, and it\u2019s essential to make informed decisions to optimize your Spark applications. In \nthe next section, we will cover different types of transformations that exist in Spark.", "mimetype": "text/plain", "start_char_idx": 188496, "end_char_idx": 189433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a50eded-d210-4ff3-aad8-33e5223d4030": {"__data__": {"id_": "9a50eded-d210-4ff3-aad8-33e5223d4030", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3635ae71-2ec9-4b0d-b2cf-565873e2018a", "node_type": "1", "metadata": {}, "hash": "61ca35c897b95132ea59ac190736fb7bd93d0e9f79c0d88bd7babf0e272dc2e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5", "node_type": "1", "metadata": {}, "hash": "20ba4e18119f125dd86d4d121f8fa2ee032edb0b9597975d9fe3355dcf41c718", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Choosing the right join method depends on the size of your DataFrames, data distribution, and network \nconsiderations, and it\u2019s essential to make informed decisions to optimize your Spark applications. In \nthe next section, we will cover different types of transformations that exist in Spark.\nNarrow and wide transformations in Apache Spark\nAs discussed in Chapter 3, transformations are the core operations for processing data. Transformations \nare categorized into two main types: narrow transformations and wide transformations. Understanding \nthe distinction between these two types of transformations is essential for optimizing the performance \nof your Spark applications.\nNarrow transformations\nNarrow transformations are operations that do not require data shuffling or extensive data movement \nacross partitions. They can be executed on a single partition without the need to communicate with other \npartitions. This inherent locality makes narrow transformations highly efficient and faster to execute.", "mimetype": "text/plain", "start_char_idx": 189140, "end_char_idx": 190153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5": {"__data__": {"id_": "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a50eded-d210-4ff3-aad8-33e5223d4030", "node_type": "1", "metadata": {}, "hash": "44b61bc43497e8b5bad56d8122bf5fbea133512533eecdd2d0847474f1e5db47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "518b0288-4861-42ab-8a85-d57486b02147", "node_type": "1", "metadata": {}, "hash": "a9b496ccd32d4af839eb687ec26cf7d7327b2274c7639aa4fc97c8475f5d1672", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding \nthe distinction between these two types of transformations is essential for optimizing the performance \nof your Spark applications.\nNarrow transformations\nNarrow transformations are operations that do not require data shuffling or extensive data movement \nacross partitions. They can be executed on a single partition without the need to communicate with other \npartitions. This inherent locality makes narrow transformations highly efficient and faster to execute.\nThe following are some of the key characteristics of narrow transformations:\n\u2022\t Single-partition processing: Narrow transformations operate on a single partition of the data \nindependently, which minimizes communication overhead.\n\u2022\t Speed and efficiency: Due to their partition-wise nature, narrow transformations are fast \nand efficient.\nmap(), filter(), union(), and groupBy() are typical examples of narrow transformations.\nWide transformations\nWide transformations, in contrast, involve data shuffling, which necessitates the exchange of data \nbetween partitions. These transformations require communication between multiple partitions and \ncan be resource-intensive.", "mimetype": "text/plain", "start_char_idx": 189673, "end_char_idx": 190825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "518b0288-4861-42ab-8a85-d57486b02147": {"__data__": {"id_": "518b0288-4861-42ab-8a85-d57486b02147", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5", "node_type": "1", "metadata": {}, "hash": "20ba4e18119f125dd86d4d121f8fa2ee032edb0b9597975d9fe3355dcf41c718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6", "node_type": "1", "metadata": {}, "hash": "e9025ec21ce9408c9cd688afdb94647a35a35235921afa0d7993748b2bab0092", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Speed and efficiency: Due to their partition-wise nature, narrow transformations are fast \nand efficient.\nmap(), filter(), union(), and groupBy() are typical examples of narrow transformations.\nWide transformations\nWide transformations, in contrast, involve data shuffling, which necessitates the exchange of data \nbetween partitions. These transformations require communication between multiple partitions and \ncan be resource-intensive. As a result, they tend to be slower and more costly in terms of computation.\nHere are a few of the key characteristics of wide transformations:\n\u2022\t Data shuffling: Wide transformations involve the reorganization of data across partitions, \nrequiring data exchange between different workers.\n\u2022\t Slower execution: Due to the need for shuffling, wide transformations are relatively slower \nand resource-intensive compared to narrow transformations.\ngroupByKey(), reduceByKey(), and join() are common examples of wide transformations.\nLet\u2019s discuss which transformation works best, depending on the operation.", "mimetype": "text/plain", "start_char_idx": 190384, "end_char_idx": 191430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6": {"__data__": {"id_": "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "518b0288-4861-42ab-8a85-d57486b02147", "node_type": "1", "metadata": {}, "hash": "a9b496ccd32d4af839eb687ec26cf7d7327b2274c7639aa4fc97c8475f5d1672", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7", "node_type": "1", "metadata": {}, "hash": "22087fdf24aad8401bea83557200137848461d1402acfd43c411fd682ce584f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Slower execution: Due to the need for shuffling, wide transformations are relatively slower \nand resource-intensive compared to narrow transformations.\ngroupByKey(), reduceByKey(), and join() are common examples of wide transformations.\nLet\u2019s discuss which transformation works best, depending on the operation.\n\nNarrow and wide transformations in Apache Spark\n\nChoosing between narrow and wide transformations\nSelecting the appropriate type of transformation depends on the specific use case and the data at hand. \nHere are some considerations for choosing between narrow and wide transformations:\n\u2022\t Data size: If your data is small enough to fit comfortably within a single partition, it\u2019s preferable \nto use narrow transformations. This minimizes the overhead associated with shuffling.\n\u2022\t Data distribution: If your data is distributed unevenly across partitions, wide transformations \nmight be necessary to reorganize and balance the data.\n\u2022\t Performance: Narrow transformations are typically faster and more efficient, so if performance \nis a critical concern, they are preferred.", "mimetype": "text/plain", "start_char_idx": 191116, "end_char_idx": 192206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7": {"__data__": {"id_": "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6", "node_type": "1", "metadata": {}, "hash": "e9025ec21ce9408c9cd688afdb94647a35a35235921afa0d7993748b2bab0092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3da216f-0f16-4b1c-b84b-d36f973e35b8", "node_type": "1", "metadata": {}, "hash": "6d1bc9402a85dc4786f3cb0386813f0665f42d58c7ba89cb155c018a2a3267a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This minimizes the overhead associated with shuffling.\n\u2022\t Data distribution: If your data is distributed unevenly across partitions, wide transformations \nmight be necessary to reorganize and balance the data.\n\u2022\t Performance: Narrow transformations are typically faster and more efficient, so if performance \nis a critical concern, they are preferred.\n\u2022\t Complex operations: Some operations, such as joining large DataFrames, often require wide \ntransformations. In such cases, the performance trade-off is inevitable.\n\u2022\t Cluster resources: Consider the available cluster resources. Resource-intensive wide transformations \nmay lead to resource contention in a shared cluster.\nNext, we\u2019ll learn how to optimize wide transformations in cases where it is necessary to implement them.\nOptimizing wide transformations\nWhile wide transformations are necessary for certain operations, it\u2019s crucial to optimize them to \nreduce their impact on performance.", "mimetype": "text/plain", "start_char_idx": 191855, "end_char_idx": 192803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3da216f-0f16-4b1c-b84b-d36f973e35b8": {"__data__": {"id_": "a3da216f-0f16-4b1c-b84b-d36f973e35b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7", "node_type": "1", "metadata": {}, "hash": "22087fdf24aad8401bea83557200137848461d1402acfd43c411fd682ce584f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "938f102a-8d57-44d2-affb-e138b54e0297", "node_type": "1", "metadata": {}, "hash": "8bf31909dd858b63e3d86a483deaddd3e31c6e1c477e64b7d14dc4d99a4904af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In such cases, the performance trade-off is inevitable.\n\u2022\t Cluster resources: Consider the available cluster resources. Resource-intensive wide transformations \nmay lead to resource contention in a shared cluster.\nNext, we\u2019ll learn how to optimize wide transformations in cases where it is necessary to implement them.\nOptimizing wide transformations\nWhile wide transformations are necessary for certain operations, it\u2019s crucial to optimize them to \nreduce their impact on performance. Here are some strategies for optimizing wide transformations:\n\u2022\t Minimize data shuffling: Whenever possible, use techniques to minimize data shuffling. For \nexample, consider using broadcast joins for small DataFrames.\n\u2022\t Partitioning: Carefully choose the number of partitions and partitioning keys to ensure even \ndata distribution, reducing the need for extensive shuffling.\n\u2022\t Caching and persistence: Caching frequently used DataFrames can help reduce the need for \nrecomputation and shuffling in subsequent stages.", "mimetype": "text/plain", "start_char_idx": 192318, "end_char_idx": 193324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "938f102a-8d57-44d2-affb-e138b54e0297": {"__data__": {"id_": "938f102a-8d57-44d2-affb-e138b54e0297", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3da216f-0f16-4b1c-b84b-d36f973e35b8", "node_type": "1", "metadata": {}, "hash": "6d1bc9402a85dc4786f3cb0386813f0665f42d58c7ba89cb155c018a2a3267a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c53f415-0e52-48a9-9476-927d9fd4a8c8", "node_type": "1", "metadata": {}, "hash": "611bf36e6f8b89552b8efafc020126b4074724c816d77e0bab87a5c45d50f786", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For \nexample, consider using broadcast joins for small DataFrames.\n\u2022\t Partitioning: Carefully choose the number of partitions and partitioning keys to ensure even \ndata distribution, reducing the need for extensive shuffling.\n\u2022\t Caching and persistence: Caching frequently used DataFrames can help reduce the need for \nrecomputation and shuffling in subsequent stages.\n\u2022\t Tuning cluster resources: Adjust cluster configurations, such as the number of executors and \nmemory allocation, to meet the demands of wide transformations.\n\u2022\t Profiling and monitoring: Regularly profile and monitor your Spark applications to identify \nperformance bottlenecks, especially in the case of wide transformations.\nIn this section, we explored the concepts of narrow and wide transformations in Apache Spark. \nUnderstanding when and how to use these transformations is critical for optimizing the performance \nof your Spark applications, especially when dealing with large datasets and complex operations.\nIn the next section, we will cover the persist and cache operations in Spark.", "mimetype": "text/plain", "start_char_idx": 192956, "end_char_idx": 194023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c53f415-0e52-48a9-9476-927d9fd4a8c8": {"__data__": {"id_": "0c53f415-0e52-48a9-9476-927d9fd4a8c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "938f102a-8d57-44d2-affb-e138b54e0297", "node_type": "1", "metadata": {}, "hash": "8bf31909dd858b63e3d86a483deaddd3e31c6e1c477e64b7d14dc4d99a4904af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8ad1767-2616-42d5-96d2-06ebee597123", "node_type": "1", "metadata": {}, "hash": "b4c05c2804734a3fb79235cc258c0fe9eb9cb87491313727aefbe02b25bbf5fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we explored the concepts of narrow and wide transformations in Apache Spark. \nUnderstanding when and how to use these transformations is critical for optimizing the performance \nof your Spark applications, especially when dealing with large datasets and complex operations.\nIn the next section, we will cover the persist and cache operations in Spark.\n\nAdvanced Operations and Optimizations in Spark\n\nPersisting and caching in Apache Spark\nIn Apache Spark, optimizing the performance of your data processing operations is essential, especially \nwhen working with large datasets and complex workflows. Caching and persistence are techniques that \nallow you to store intermediate or frequently used data in memory or on disk, reducing the need for \nrecomputation and enhancing overall performance. This section explores the concepts of persisting \nand caching in Spark.\nUnderstanding data persistence\nData persistence is the process of storing the intermediate or final results of Spark transformations \nin memory or on disk.", "mimetype": "text/plain", "start_char_idx": 193655, "end_char_idx": 194695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8ad1767-2616-42d5-96d2-06ebee597123": {"__data__": {"id_": "f8ad1767-2616-42d5-96d2-06ebee597123", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c53f415-0e52-48a9-9476-927d9fd4a8c8", "node_type": "1", "metadata": {}, "hash": "611bf36e6f8b89552b8efafc020126b4074724c816d77e0bab87a5c45d50f786", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "695dbcd0-7698-4b83-928d-b40a7b2b33cc", "node_type": "1", "metadata": {}, "hash": "159060bdb7899e1bc4838b033206f0ce97134fbe89071dbcd52e86a08794edd8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Caching and persistence are techniques that \nallow you to store intermediate or frequently used data in memory or on disk, reducing the need for \nrecomputation and enhancing overall performance. This section explores the concepts of persisting \nand caching in Spark.\nUnderstanding data persistence\nData persistence is the process of storing the intermediate or final results of Spark transformations \nin memory or on disk. By persisting data, you reduce the need to recompute it from the source data, \nthereby improving query performance.\nThe following key concepts are related to data persistence:\n\u2022\t Storage levels: Spark offers multiple storage levels for data, ranging from memory-only to \ndisk, depending on your needs. Each storage level comes with its trade-offs in terms of speed \nand durability.\n\u2022\t Lazy evaluation: Spark follows a lazy evaluation model, meaning transformations are not \nexecuted until an action is called. Data persistence ensures that the intermediate results are \navailable for reuse without recomputation.", "mimetype": "text/plain", "start_char_idx": 194273, "end_char_idx": 195308, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "695dbcd0-7698-4b83-928d-b40a7b2b33cc": {"__data__": {"id_": "695dbcd0-7698-4b83-928d-b40a7b2b33cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8ad1767-2616-42d5-96d2-06ebee597123", "node_type": "1", "metadata": {}, "hash": "b4c05c2804734a3fb79235cc258c0fe9eb9cb87491313727aefbe02b25bbf5fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd", "node_type": "1", "metadata": {}, "hash": "ed5692434f0ffb7e67f68a3495b26ed0f0566d742d4fe95de6d54faa03720c4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each storage level comes with its trade-offs in terms of speed \nand durability.\n\u2022\t Lazy evaluation: Spark follows a lazy evaluation model, meaning transformations are not \nexecuted until an action is called. Data persistence ensures that the intermediate results are \navailable for reuse without recomputation.\n\u2022\t Caching versus persistence: Caching is a specific form of data persistence that stores data in \nmemory, while persistence encompasses both in-memory and on-disk storage.\nCaching data\nCaching is a form of data persistence that stores DataFrames, RDDs, or datasets in memory for fast \naccess. It is an essential optimization technique that improves the performance of Spark applications, \nparticularly when dealing with iterative algorithms or repeated computations.\nTo cache a DataFrame or an RDD, you can use the .cache() or .persist() method while \nspecifying the storage level:\n\u2022\t Memory-only: This option stores data in memory but does not replicate it for fault tolerance.", "mimetype": "text/plain", "start_char_idx": 194998, "end_char_idx": 195988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd": {"__data__": {"id_": "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "695dbcd0-7698-4b83-928d-b40a7b2b33cc", "node_type": "1", "metadata": {}, "hash": "159060bdb7899e1bc4838b033206f0ce97134fbe89071dbcd52e86a08794edd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1aa8bf93-4bcf-4895-9c36-4f186903949f", "node_type": "1", "metadata": {}, "hash": "bac64d1d8622e47cf6ffb4a1619564d28493921c9160ea05f4be97870ad6a0d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is an essential optimization technique that improves the performance of Spark applications, \nparticularly when dealing with iterative algorithms or repeated computations.\nTo cache a DataFrame or an RDD, you can use the .cache() or .persist() method while \nspecifying the storage level:\n\u2022\t Memory-only: This option stores data in memory but does not replicate it for fault tolerance. \nUse .cache() or .persist(StorageLevel.MEMORY_ONLY).\n\u2022\t Memory-only, serialized: This option stores data in memory in a serialized form, reducing \nmemory usage. Use .persist(StorageLevel.MEMORY_ONLY_SER).\n\u2022\t Memory and disk: This option stores data in memory and spills excess data to disk when \nmemory is full. Use .persist(StorageLevel.MEMORY_AND_DISK).\n\nPersisting and caching in Apache Spark\n\n\u2022\t Disk-only: This option stores data only on disk, avoiding memory usage. \nUse .persist(StorageLevel.DISK_ONLY).", "mimetype": "text/plain", "start_char_idx": 195603, "end_char_idx": 196499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1aa8bf93-4bcf-4895-9c36-4f186903949f": {"__data__": {"id_": "1aa8bf93-4bcf-4895-9c36-4f186903949f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd", "node_type": "1", "metadata": {}, "hash": "ed5692434f0ffb7e67f68a3495b26ed0f0566d742d4fe95de6d54faa03720c4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "403ce972-c506-48d0-8f0b-7ab4eb14daae", "node_type": "1", "metadata": {}, "hash": "0a1b249c332025b69d35d175d36be410803051cc0e8046724f0b6004a69d266d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use .persist(StorageLevel.MEMORY_ONLY_SER).\n\u2022\t Memory and disk: This option stores data in memory and spills excess data to disk when \nmemory is full. Use .persist(StorageLevel.MEMORY_AND_DISK).\n\nPersisting and caching in Apache Spark\n\n\u2022\t Disk-only: This option stores data only on disk, avoiding memory usage. \nUse .persist(StorageLevel.DISK_ONLY).\nCaching is particularly beneficial in the following scenarios:\n\u2022\t Iterative algorithms: Caching is vital for iterative algorithms such as machine learning, graph \nprocessing, and optimization problems, where the same data is used repeatedly\n\u2022\t Multiple actions: When a DataFrame is used for multiple actions, caching it after the first \naction can improve performance\n\u2022\t Avoiding recomputation: Caching helps avoid recomputing the same data when multiple \ntransformations depend on it\n\u2022\t Interactive queries: In interactive data exploration or querying, caching frequently used \nintermediate results can speed up ad hoc analysis\nUnpersisting data\nCaching consumes memory, and in a cluster environment, it\u2019s essential to manage memory efficiently.", "mimetype": "text/plain", "start_char_idx": 196150, "end_char_idx": 197246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "403ce972-c506-48d0-8f0b-7ab4eb14daae": {"__data__": {"id_": "403ce972-c506-48d0-8f0b-7ab4eb14daae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1aa8bf93-4bcf-4895-9c36-4f186903949f", "node_type": "1", "metadata": {}, "hash": "bac64d1d8622e47cf6ffb4a1619564d28493921c9160ea05f4be97870ad6a0d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "435e7b89-847f-4f05-8b16-0cad3200a604", "node_type": "1", "metadata": {}, "hash": "1e8c1f4c06a9f044ddb4805875740f4fcffb633c03dfe4bad81be58160398431", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can release cached data from memory using the .unpersist() method. This method allows \nyou to specify whether to release the data immediately or only when it is no longer needed.", "mimetype": "text/plain", "start_char_idx": 197248, "end_char_idx": 197430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "435e7b89-847f-4f05-8b16-0cad3200a604": {"__data__": {"id_": "435e7b89-847f-4f05-8b16-0cad3200a604", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "403ce972-c506-48d0-8f0b-7ab4eb14daae", "node_type": "1", "metadata": {}, "hash": "0a1b249c332025b69d35d175d36be410803051cc0e8046724f0b6004a69d266d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "099615d1-37f0-435c-8582-005f9b081529", "node_type": "1", "metadata": {}, "hash": "ae795c86e99ef68bf5a100a2a880421ab675dd9e5d815bb5c3d99d7f69d94173", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can release cached data from memory using the .unpersist() method. This method allows \nyou to specify whether to release the data immediately or only when it is no longer needed.\nHere\u2019s an example of unpersisting data:\n# Cache a DataFrame\ndf.cache()\n# Unpersist the cached DataFrame\ndf.unpersist()\nBest practices\nTo use caching and persistence effectively in your Spark applications, consider the following best practices:\n\u2022\t Cache only what\u2019s necessary: Caching consumes memory, so cache only the data that is \nfrequently used or costly to compute\n\u2022\t Monitor memory usage: Regularly monitor memory usage to avoid running out of memory \nor excessive disk spills\n\u2022\t Automate unpersistence: If you have limited memory resources, automate the unpersistence \nof less frequently used data to free up memory for more critical operations\n\u2022\t Consider serialization: Depending on your use case, consider using serialized storage levels \nto reduce memory overhead\n\nAdvanced Operations and Optimizations in Spark\n\nIn this section, we explored the concepts of persistence and caching in Apache Spark.", "mimetype": "text/plain", "start_char_idx": 197248, "end_char_idx": 198340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "099615d1-37f0-435c-8582-005f9b081529": {"__data__": {"id_": "099615d1-37f0-435c-8582-005f9b081529", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "435e7b89-847f-4f05-8b16-0cad3200a604", "node_type": "1", "metadata": {}, "hash": "1e8c1f4c06a9f044ddb4805875740f4fcffb633c03dfe4bad81be58160398431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913", "node_type": "1", "metadata": {}, "hash": "81bd734ac38653c695fabc00ace8c18cdd84a7beacd1c631dfa567fc79ef87ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Caching and \npersistence are powerful techniques for optimizing performance in Spark applications, particularly when \ndealing with iterative algorithms or scenarios where the same data is used repeatedly. Understanding when \nand how to use these techniques can significantly improve the efficiency of your data processing workflows.\nIn the next section, we\u2019ll learn how repartition and coalesce work in Spark.\nRepartitioning and coalescing in Apache Spark\nEfficient data partitioning plays a crucial role in optimizing data processing workflows in Apache \nSpark. Repartitioning and coalescing are operations that allow you to control the distribution of data \nacross partitions. In this section, we\u2019ll explore the concepts of repartitioning and coalescing and their \nsignificance in Spark applications.\nUnderstanding data partitioning\nData partitioning in Apache Spark involves dividing a dataset into smaller, manageable units called \npartitions.", "mimetype": "text/plain", "start_char_idx": 198341, "end_char_idx": 199288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913": {"__data__": {"id_": "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099615d1-37f0-435c-8582-005f9b081529", "node_type": "1", "metadata": {}, "hash": "ae795c86e99ef68bf5a100a2a880421ab675dd9e5d815bb5c3d99d7f69d94173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfffcb70-34aa-4ae4-a6a5-685eae048205", "node_type": "1", "metadata": {}, "hash": "073ada689fa25ef4dcc9bc2cef1857b281fdf949d9d8b7f084cc340ab0010d9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Repartitioning and coalescing are operations that allow you to control the distribution of data \nacross partitions. In this section, we\u2019ll explore the concepts of repartitioning and coalescing and their \nsignificance in Spark applications.\nUnderstanding data partitioning\nData partitioning in Apache Spark involves dividing a dataset into smaller, manageable units called \npartitions. Each partition contains a subset of the data and is processed independently by different \nworker nodes in a distributed cluster. Proper data partitioning can significantly impact the efficiency \nand performance of Spark applications.\nRepartitioning data\nRepartitioning is the process of redistributing data across a different number of partitions. This \noperation can help balance data distribution, improve parallelism, and optimize data processing. You \ncan use the .repartition() method to specify the number of desired partitions.", "mimetype": "text/plain", "start_char_idx": 198904, "end_char_idx": 199823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfffcb70-34aa-4ae4-a6a5-685eae048205": {"__data__": {"id_": "dfffcb70-34aa-4ae4-a6a5-685eae048205", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913", "node_type": "1", "metadata": {}, "hash": "81bd734ac38653c695fabc00ace8c18cdd84a7beacd1c631dfa567fc79ef87ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee2be34e-8be5-41b8-a7ba-93cd0d853104", "node_type": "1", "metadata": {}, "hash": "a4043511a18eb2c1da7f5e19f714c1b07486f3c20c83dc5d726bc5c7991de770", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proper data partitioning can significantly impact the efficiency \nand performance of Spark applications.\nRepartitioning data\nRepartitioning is the process of redistributing data across a different number of partitions. This \noperation can help balance data distribution, improve parallelism, and optimize data processing. You \ncan use the .repartition() method to specify the number of desired partitions.\nHere are some key points related to repartitioning data:\n\u2022\t Increasing or decreasing partitions: Repartitioning allows you to increase or decrease the \nnumber of partitions to suit your processing needs.\n\u2022\t Data shuffling: Repartitioning often involves data shuffling, which can be resource-intensive. \nTherefore, it should be used judiciously.\n\u2022\t Even data distribution: Repartitioning is useful when the original data is unevenly distributed \nacross partitions, causing skewed workloads.", "mimetype": "text/plain", "start_char_idx": 199418, "end_char_idx": 200313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee2be34e-8be5-41b8-a7ba-93cd0d853104": {"__data__": {"id_": "ee2be34e-8be5-41b8-a7ba-93cd0d853104", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfffcb70-34aa-4ae4-a6a5-685eae048205", "node_type": "1", "metadata": {}, "hash": "073ada689fa25ef4dcc9bc2cef1857b281fdf949d9d8b7f084cc340ab0010d9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c17b870-b05b-4dc0-bb9f-5befcb47075f", "node_type": "1", "metadata": {}, "hash": "ed6bb649f1e24549559f46b6c9d3bf7617b4c44f63203a0410a329c550c5d043", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Data shuffling: Repartitioning often involves data shuffling, which can be resource-intensive. \nTherefore, it should be used judiciously.\n\u2022\t Even data distribution: Repartitioning is useful when the original data is unevenly distributed \nacross partitions, causing skewed workloads.\n\u2022\t Optimizing for joins: Repartitioning can be beneficial when performing joins to minimize \ndata shuffling.\nHere\u2019s an example of repartitioning data:\n# Repartition a DataFrame into 8 partitions\ndf.repartition(8)\n\nRepartitioning and coalescing in Apache Spark\n\nCoalescing data\nCoalescing is the process of reducing the number of partitions while preserving data locality. It is a \nmore efficient operation than repartitioning because it avoids unnecessary data shuffling whenever \npossible. You can use the .coalesce() method to specify the target number of partitions.", "mimetype": "text/plain", "start_char_idx": 200028, "end_char_idx": 200883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c17b870-b05b-4dc0-bb9f-5befcb47075f": {"__data__": {"id_": "5c17b870-b05b-4dc0-bb9f-5befcb47075f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee2be34e-8be5-41b8-a7ba-93cd0d853104", "node_type": "1", "metadata": {}, "hash": "a4043511a18eb2c1da7f5e19f714c1b07486f3c20c83dc5d726bc5c7991de770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b68c5cc-5fca-448c-96df-961092921b05", "node_type": "1", "metadata": {}, "hash": "05c0982187bff0a8e479c69d6a128507ac4e196802a8cc663920140020a6a290", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is a \nmore efficient operation than repartitioning because it avoids unnecessary data shuffling whenever \npossible. You can use the .coalesce() method to specify the target number of partitions.\nHere are some key points related to coalescing data:\n\u2022\t Decreasing partitions: Coalescing is used when you want to decrease the number of partitions \nto optimize data processing\n\u2022\t Minimizing data movement: Unlike repartitioning, coalescing minimizes data shuffling by \nmerging partitions locally whenever possible\n\u2022\t Efficient for data reduction: Coalescing is efficient when you need to reduce the number of \npartitions without incurring the full cost of data shuffling\nHere\u2019s an example of coalescing data:\n# Coalesce a DataFrame to 4 partitions\ndf.coalesce(4)\nUse cases for repartitioning and coalescing\nUnderstanding when to repartition and coalesce is critical for optimizing your Spark applications.", "mimetype": "text/plain", "start_char_idx": 200686, "end_char_idx": 201590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b68c5cc-5fca-448c-96df-961092921b05": {"__data__": {"id_": "4b68c5cc-5fca-448c-96df-961092921b05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c17b870-b05b-4dc0-bb9f-5befcb47075f", "node_type": "1", "metadata": {}, "hash": "ed6bb649f1e24549559f46b6c9d3bf7617b4c44f63203a0410a329c550c5d043", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46676818-433d-4dda-84e2-020463a3af95", "node_type": "1", "metadata": {}, "hash": "61e6a185feeb76f74eef74f6880f17af311596800eb7aaf3afbf44a1fc68cf81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following are some use cases for repartitioning:\n\u2022\t Data skew: When data is skewed across partitions, repartitioning can balance the workload\n\u2022\t Join optimization: For optimizing join operations by ensuring that the joining keys are collocated\n\u2022\t Parallelism control: Adjusting the level of parallelism to optimize resource utilization\nNow, let\u2019s look at some use cases for coalescing:\n\u2022\t Reducing data: When you need to reduce the number of partitions to save memory and \nreduce overhead\n\u2022\t Minimizing shuffling: To avoid unnecessary data shuffling and minimize network communication\n\u2022\t Post-filtering: After applying a filter or transformation that significantly reduces the dataset size\nBest practices\nTo repartition and coalesce effectively in your Spark applications, consider these best practices:\n\u2022\t Profile and monitor: Profile your application to identify performance bottlenecks related to \ndata partitioning.", "mimetype": "text/plain", "start_char_idx": 201591, "end_char_idx": 202514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46676818-433d-4dda-84e2-020463a3af95": {"__data__": {"id_": "46676818-433d-4dda-84e2-020463a3af95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b68c5cc-5fca-448c-96df-961092921b05", "node_type": "1", "metadata": {}, "hash": "05c0982187bff0a8e479c69d6a128507ac4e196802a8cc663920140020a6a290", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ed7bf9-0514-40fd-8b16-ce142d3a610e", "node_type": "1", "metadata": {}, "hash": "66bc05778c0ed1e8da51f9b105ca950a8cf7cf4779c987d3b5e323d17c77c370", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use Spark\u2019s UI and monitoring tools to track data shuffling.\n\nAdvanced Operations and Optimizations in Spark\n\n\u2022\t Consider data size: Consider the size of your dataset and the available cluster resources when \ndeciding on the number of partitions.\n\u2022\t Balance workloads: Aim for a balanced workload distribution across partitions to \noptimize parallelism.\n\u2022\t Coalesce where possible: When reducing the number of partitions, prefer coalescing over \nrepartitioning to minimize data shuffling.\n\u2022\t Plan for joins: When performing joins, plan for the optimal number of partitions to minimize \nshuffle overhead.\nIn this section, we explored the concepts of repartitioning and coalescing in Apache Spark. Understanding \nhow to efficiently control data partitioning can significantly impact the performance of your Spark \napplications, especially when you\u2019re working with large datasets and complex operations.", "mimetype": "text/plain", "start_char_idx": 202515, "end_char_idx": 203415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30ed7bf9-0514-40fd-8b16-ce142d3a610e": {"__data__": {"id_": "30ed7bf9-0514-40fd-8b16-ce142d3a610e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46676818-433d-4dda-84e2-020463a3af95", "node_type": "1", "metadata": {}, "hash": "61e6a185feeb76f74eef74f6880f17af311596800eb7aaf3afbf44a1fc68cf81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39884f18-d161-41e0-bd57-93c788fc3788", "node_type": "1", "metadata": {}, "hash": "e92f54b5ad49e010aec5f4bcad7d7bd657ebaa679bdd97d47b06a156ca6dae86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Plan for joins: When performing joins, plan for the optimal number of partitions to minimize \nshuffle overhead.\nIn this section, we explored the concepts of repartitioning and coalescing in Apache Spark. Understanding \nhow to efficiently control data partitioning can significantly impact the performance of your Spark \napplications, especially when you\u2019re working with large datasets and complex operations.\nSummary\nIn this chapter, we delved into advanced data processing capabilities in Apache Spark, enhancing \nyour understanding of key concepts and techniques. We explored the intricacies of Spark\u2019s Catalyst \noptimizer, the power of different types of Spark joins, the importance of data persistence and caching, \nthe significance of narrow and wide transformations, and the role of data partitioning using repartition \nand coalesce. Additionally, we discovered the versatility and utility of UDFs.", "mimetype": "text/plain", "start_char_idx": 203004, "end_char_idx": 203911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39884f18-d161-41e0-bd57-93c788fc3788": {"__data__": {"id_": "39884f18-d161-41e0-bd57-93c788fc3788", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30ed7bf9-0514-40fd-8b16-ce142d3a610e", "node_type": "1", "metadata": {}, "hash": "66bc05778c0ed1e8da51f9b105ca950a8cf7cf4779c987d3b5e323d17c77c370", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e5ef68d-a7a4-4077-9c37-1649f46c629b", "node_type": "1", "metadata": {}, "hash": "0421499420776cda5dd547e99166473b54a726c7cb9d7969b6c4cbadf03d3f44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We explored the intricacies of Spark\u2019s Catalyst \noptimizer, the power of different types of Spark joins, the importance of data persistence and caching, \nthe significance of narrow and wide transformations, and the role of data partitioning using repartition \nand coalesce. Additionally, we discovered the versatility and utility of UDFs.\nAs you advance in your journey with Apache Spark, these advanced capabilities will prove invaluable \nfor optimizing and customizing your data processing workflows. By harnessing the potential of the \nCatalyst optimizer, you can fine-tune query execution for improved performance. Understanding the \nnuances of Spark joins empowers you to make informed decisions on which type of join to employ \nfor specific use cases. Data persistence and caching become indispensable when you seek to reduce \nrecomputation and expedite iterative processes.\nNarrow and wide transformations play a pivotal role in achieving the desired parallelism and resource \nefficiency in Spark applications.", "mimetype": "text/plain", "start_char_idx": 203573, "end_char_idx": 204590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e5ef68d-a7a4-4077-9c37-1649f46c629b": {"__data__": {"id_": "2e5ef68d-a7a4-4077-9c37-1649f46c629b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39884f18-d161-41e0-bd57-93c788fc3788", "node_type": "1", "metadata": {}, "hash": "e92f54b5ad49e010aec5f4bcad7d7bd657ebaa679bdd97d47b06a156ca6dae86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bccef02b-9e01-4803-9211-0917a54f8a91", "node_type": "1", "metadata": {}, "hash": "0a8e6764c488d0d43a465edd0117975625b0e0f4a520635e46deec9751e71293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding the \nnuances of Spark joins empowers you to make informed decisions on which type of join to employ \nfor specific use cases. Data persistence and caching become indispensable when you seek to reduce \nrecomputation and expedite iterative processes.\nNarrow and wide transformations play a pivotal role in achieving the desired parallelism and resource \nefficiency in Spark applications. Proper data partitioning through repartition and coalesce ensures \nbalanced workloads and optimal data distribution.\nUDFs open the door to limitless possibilities, enabling you to implement custom data processing \nlogic, from data cleansing and feature engineering to complex calculations and domain-specific \noperations. However, it is crucial to use UDFs judiciously, optimizing them for performance and \nadhering to best practices.\nWith this chapter\u2019s knowledge, you are better equipped to tackle complex data processing challenges \nin Apache Spark, enabling you to extract valuable insights from your data efficiently and effectively.", "mimetype": "text/plain", "start_char_idx": 204192, "end_char_idx": 205229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bccef02b-9e01-4803-9211-0917a54f8a91": {"__data__": {"id_": "bccef02b-9e01-4803-9211-0917a54f8a91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e5ef68d-a7a4-4077-9c37-1649f46c629b", "node_type": "1", "metadata": {}, "hash": "0421499420776cda5dd547e99166473b54a726c7cb9d7969b6c4cbadf03d3f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ae4f16-7cce-424e-bc91-5102891a8aeb", "node_type": "1", "metadata": {}, "hash": "c9624ffe675e2cc6079b42449f7bfe9222b2c19d692c174952961123884f37c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, it is crucial to use UDFs judiciously, optimizing them for performance and \nadhering to best practices.\nWith this chapter\u2019s knowledge, you are better equipped to tackle complex data processing challenges \nin Apache Spark, enabling you to extract valuable insights from your data efficiently and effectively. \nThese advanced capabilities empower you to leverage the full potential of Spark and achieve optimal \nperformance in your data-driven endeavors.\n\n\nSQL Queries in Spark\nIn this chapter, we will explore the vast capabilities of Spark SQL for structured data processing. We \nwill dive into loading and manipulating data, executing SQL queries, performing advanced analytics, \nand integrating Spark SQL with external systems. By the end of this chapter, you will have a solid \nunderstanding of Spark SQL\u2019s features and be equipped with the knowledge to leverage its power in \nyour data processing tasks.\nWe will cover the following topics:\n\u2022\t What is Spark SQL?", "mimetype": "text/plain", "start_char_idx": 204913, "end_char_idx": 205887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28ae4f16-7cce-424e-bc91-5102891a8aeb": {"__data__": {"id_": "28ae4f16-7cce-424e-bc91-5102891a8aeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bccef02b-9e01-4803-9211-0917a54f8a91", "node_type": "1", "metadata": {}, "hash": "0a8e6764c488d0d43a465edd0117975625b0e0f4a520635e46deec9751e71293", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7258df4-833f-412e-bdc2-024a826cba20", "node_type": "1", "metadata": {}, "hash": "345d14cb5788a9a531a5446e2306201b8a391cf497bb5059124c4cf994810e96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \nwill dive into loading and manipulating data, executing SQL queries, performing advanced analytics, \nand integrating Spark SQL with external systems. By the end of this chapter, you will have a solid \nunderstanding of Spark SQL\u2019s features and be equipped with the knowledge to leverage its power in \nyour data processing tasks.\nWe will cover the following topics:\n\u2022\t What is Spark SQL?\n\u2022\t Getting Started with Spark SQL\n\u2022\t Advanced Spark SQL operations\nWhat is Spark SQL?\nSpark SQL is a powerful module within the Apache Spark ecosystem that allows for the efficient \nprocessing and analysis of structured data. It provides a higher-level interface for working with \nstructured data compared to the traditional RDD-based API of Apache Spark. Spark SQL combines \nthe benefits of both relational and procedural processing, enabling users to seamlessly integrate SQL \nqueries with complex analytics. By leveraging Spark\u2019s distributed computing capabilities, Spark SQL \nenables scalable and high-performance data processing.", "mimetype": "text/plain", "start_char_idx": 205498, "end_char_idx": 206522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7258df4-833f-412e-bdc2-024a826cba20": {"__data__": {"id_": "d7258df4-833f-412e-bdc2-024a826cba20", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28ae4f16-7cce-424e-bc91-5102891a8aeb", "node_type": "1", "metadata": {}, "hash": "c9624ffe675e2cc6079b42449f7bfe9222b2c19d692c174952961123884f37c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54f633a2-ea36-4a31-a64d-acbaea796743", "node_type": "1", "metadata": {}, "hash": "91a29da659d54ddba97459315fd72ddeb4ae651d1f1f90d2a11bc2e4100c5ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It provides a higher-level interface for working with \nstructured data compared to the traditional RDD-based API of Apache Spark. Spark SQL combines \nthe benefits of both relational and procedural processing, enabling users to seamlessly integrate SQL \nqueries with complex analytics. By leveraging Spark\u2019s distributed computing capabilities, Spark SQL \nenables scalable and high-performance data processing.\nIt provides a programming interface to work with structured data using SQL queries, DataFrame \nAPI, and Datasets API.\nIt allows users to query data using SQL-like syntax and provides a powerful engine for executing SQL \nqueries on large datasets. Spark SQL also supports reading and writing data from various structured \nsources such as Hive tables, Parquet files, and JDBC databases.\n\nSQL Queries in Spark\n\nAdvantages of Spark SQL\nSpark SQL offers several key advantages that make it a popular choice for structured data processing:\nUnified data processing with Spark SQL\nWith Spark SQL, users can process both structured and unstructured data using a single engine.", "mimetype": "text/plain", "start_char_idx": 206114, "end_char_idx": 207190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54f633a2-ea36-4a31-a64d-acbaea796743": {"__data__": {"id_": "54f633a2-ea36-4a31-a64d-acbaea796743", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7258df4-833f-412e-bdc2-024a826cba20", "node_type": "1", "metadata": {}, "hash": "345d14cb5788a9a531a5446e2306201b8a391cf497bb5059124c4cf994810e96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "983c1fe3-f347-48a0-ab56-42b5631800d3", "node_type": "1", "metadata": {}, "hash": "382827c1e46602b149497caf646c38ee9bb79a94c2438d8db98133283fbe6f3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark SQL also supports reading and writing data from various structured \nsources such as Hive tables, Parquet files, and JDBC databases.\n\nSQL Queries in Spark\n\nAdvantages of Spark SQL\nSpark SQL offers several key advantages that make it a popular choice for structured data processing:\nUnified data processing with Spark SQL\nWith Spark SQL, users can process both structured and unstructured data using a single engine. This \nmeans that users can use the same programming interface to query data stored in different formats \nsuch as JSON, CSV, and Parquet.\nUsers can seamlessly switch between SQL queries, DataFrame transformations, and Spark\u2019s machine \nlearning APIs. This unified data processing approach allows for the easier integration of different data \nprocessing tasks within a single application, reducing development complexity.\nPerformance and scalability\nSpark SQL leverages the distributed computing capabilities of Apache Spark, enabling the processing \nof large-scale datasets across a cluster of machines.", "mimetype": "text/plain", "start_char_idx": 206770, "end_char_idx": 207792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "983c1fe3-f347-48a0-ab56-42b5631800d3": {"__data__": {"id_": "983c1fe3-f347-48a0-ab56-42b5631800d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54f633a2-ea36-4a31-a64d-acbaea796743", "node_type": "1", "metadata": {}, "hash": "91a29da659d54ddba97459315fd72ddeb4ae651d1f1f90d2a11bc2e4100c5ac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9d90b45-2a94-4b72-a1b6-8c78c390c260", "node_type": "1", "metadata": {}, "hash": "84a5a229719201e2459083b8e5288a31910c763d605bbdaeb1c8f1930e18bb54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Users can seamlessly switch between SQL queries, DataFrame transformations, and Spark\u2019s machine \nlearning APIs. This unified data processing approach allows for the easier integration of different data \nprocessing tasks within a single application, reducing development complexity.\nPerformance and scalability\nSpark SQL leverages the distributed computing capabilities of Apache Spark, enabling the processing \nof large-scale datasets across a cluster of machines. It utilizes advanced query optimization techniques, \nsuch a the Catalyst optimizer (discussed in detail in Chapter 5), to optimize and accelerate query \nexecution. Additionally, Spark SQL supports data partitioning and caching mechanisms, further \nenhancing performance and scalability.\nSpark SQL uses an optimized execution engine that can process queries much faster than traditional \nSQL engines. It achieves this by using in-memory caching and optimized query execution plans.\nSpark SQL is designed to scale horizontally across a cluster of machines. It can handle large datasets \nby partitioning them across multiple machines and processing them in parallel.", "mimetype": "text/plain", "start_char_idx": 207328, "end_char_idx": 208456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9d90b45-2a94-4b72-a1b6-8c78c390c260": {"__data__": {"id_": "e9d90b45-2a94-4b72-a1b6-8c78c390c260", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "983c1fe3-f347-48a0-ab56-42b5631800d3", "node_type": "1", "metadata": {}, "hash": "382827c1e46602b149497caf646c38ee9bb79a94c2438d8db98133283fbe6f3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf3f631f-86ca-464f-8ca4-ba18ee455221", "node_type": "1", "metadata": {}, "hash": "6094d4a2de0dbe848186bfcf6bee3eae79494d1f2ae2bfa812e7973a1ddef7c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, Spark SQL supports data partitioning and caching mechanisms, further \nenhancing performance and scalability.\nSpark SQL uses an optimized execution engine that can process queries much faster than traditional \nSQL engines. It achieves this by using in-memory caching and optimized query execution plans.\nSpark SQL is designed to scale horizontally across a cluster of machines. It can handle large datasets \nby partitioning them across multiple machines and processing them in parallel.\nSeamless integration with existing infrastructure\nSpark SQL integrates seamlessly with existing Apache Spark infrastructure and tools. It provides \ninteroperability with other Spark components, such as Spark Streaming for real-time data processing \nand Spark MLlib for machine learning tasks. Furthermore, Spark SQL integrates with popular storage \nsystems and data formats, including Parquet, Avro, ORC, and Hive, making it compatible with a wide \nrange of data sources.\nAdvanced analytics capabilities\nSpark SQL extends traditional SQL capabilities by using advanced analytics features.", "mimetype": "text/plain", "start_char_idx": 207957, "end_char_idx": 209045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf3f631f-86ca-464f-8ca4-ba18ee455221": {"__data__": {"id_": "cf3f631f-86ca-464f-8ca4-ba18ee455221", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9d90b45-2a94-4b72-a1b6-8c78c390c260", "node_type": "1", "metadata": {}, "hash": "84a5a229719201e2459083b8e5288a31910c763d605bbdaeb1c8f1930e18bb54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9303bed2-daf6-46a7-af36-967b548f27f0", "node_type": "1", "metadata": {}, "hash": "5c3c1ed15abbd291b9ec7f89a2ddb73203c2494bc41e8c244990fb8b6cddbd73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It provides \ninteroperability with other Spark components, such as Spark Streaming for real-time data processing \nand Spark MLlib for machine learning tasks. Furthermore, Spark SQL integrates with popular storage \nsystems and data formats, including Parquet, Avro, ORC, and Hive, making it compatible with a wide \nrange of data sources.\nAdvanced analytics capabilities\nSpark SQL extends traditional SQL capabilities by using advanced analytics features. It supports window \nfunctions, which enable users to perform complex analytical operations, such as ranking, aggregation \nover sliding windows, and cumulative aggregations. The integration with machine learning libraries \nin Spark allows for the seamless integration of predictive analytics and data science workflows.\n\nGetting started with Spark SQL\n\nEase of use\nSpark SQL provides a simple programming interface that allows users to query data using SQL-like \nsyntax. This makes it easy for users who are familiar with SQL to get started with Spark SQL.", "mimetype": "text/plain", "start_char_idx": 208592, "end_char_idx": 209601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9303bed2-daf6-46a7-af36-967b548f27f0": {"__data__": {"id_": "9303bed2-daf6-46a7-af36-967b548f27f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf3f631f-86ca-464f-8ca4-ba18ee455221", "node_type": "1", "metadata": {}, "hash": "6094d4a2de0dbe848186bfcf6bee3eae79494d1f2ae2bfa812e7973a1ddef7c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a8054c-f853-44db-8e14-f6078e68b90d", "node_type": "1", "metadata": {}, "hash": "44c6b203f40d1a0396191384f72b85041b07df52373fbd8e850421598456a361", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The integration with machine learning libraries \nin Spark allows for the seamless integration of predictive analytics and data science workflows.\n\nGetting started with Spark SQL\n\nEase of use\nSpark SQL provides a simple programming interface that allows users to query data using SQL-like \nsyntax. This makes it easy for users who are familiar with SQL to get started with Spark SQL.\nIntegration with Apache Spark\nSpark SQL is an integral part of the Apache Spark framework and works seamlessly with other Spark \ncomponents. It leverages Spark\u2019s core functionalities, such as fault tolerance, data parallelism, and \ndistributed computing, to provide scalable and efficient data processing. Spark SQL can read data from \na variety of sources, including distributed file systems (such as HDFS), object stores (like Amazon S3), \nand relational databases (via JDBC). It also integrates with external systems such as Hive, allowing \nusers to leverage existing Hive metadata and queries.", "mimetype": "text/plain", "start_char_idx": 209219, "end_char_idx": 210199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2a8054c-f853-44db-8e14-f6078e68b90d": {"__data__": {"id_": "e2a8054c-f853-44db-8e14-f6078e68b90d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9303bed2-daf6-46a7-af36-967b548f27f0", "node_type": "1", "metadata": {}, "hash": "5c3c1ed15abbd291b9ec7f89a2ddb73203c2494bc41e8c244990fb8b6cddbd73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a4a01bf-2da2-45c4-9308-a0105bb17ddb", "node_type": "1", "metadata": {}, "hash": "c817bc03649348ebd73d4eb8f544961e7a9c572ef97481de683ebece1cd1bdfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark SQL can read data from \na variety of sources, including distributed file systems (such as HDFS), object stores (like Amazon S3), \nand relational databases (via JDBC). It also integrates with external systems such as Hive, allowing \nusers to leverage existing Hive metadata and queries.\nNow let\u2019s take a look at some basic constructs of Spark SQL.\nKey concepts \u2013 DataFrames and datasets\nSpark SQL introduces two fundamental abstractions for working with structured data: DataFrames \nand Datasets.\nDataFrames\nDataFrames represent distributed collections of data organized into named columns. They provide \na higher-level interface for working with structured data and offer rich APIs for data manipulation, \nfiltering, aggregation, and querying. DataFrames are immutable and lazily evaluated, enabling optimized \nexecution plans through Spark\u2019s Catalyst optimizer. They can be created from various data sources, \nincluding structured files (CSV, JSON, and Parquet), Hive tables, and existing RDDs.", "mimetype": "text/plain", "start_char_idx": 209908, "end_char_idx": 210909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a4a01bf-2da2-45c4-9308-a0105bb17ddb": {"__data__": {"id_": "8a4a01bf-2da2-45c4-9308-a0105bb17ddb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a8054c-f853-44db-8e14-f6078e68b90d", "node_type": "1", "metadata": {}, "hash": "44c6b203f40d1a0396191384f72b85041b07df52373fbd8e850421598456a361", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829", "node_type": "1", "metadata": {}, "hash": "506483ba66f2bc80530c382fd9e0b11461e2a35e61af8d527e0784f0ff6a052e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They provide \na higher-level interface for working with structured data and offer rich APIs for data manipulation, \nfiltering, aggregation, and querying. DataFrames are immutable and lazily evaluated, enabling optimized \nexecution plans through Spark\u2019s Catalyst optimizer. They can be created from various data sources, \nincluding structured files (CSV, JSON, and Parquet), Hive tables, and existing RDDs.\nDatasets\nDatasets are an extension of DataFrames and provide a type-safe, object-oriented programming \ninterface. Datasets combine the benefits of Spark\u2019s RDDs (strong typing and user-defined functions) \nwith the performance optimizations of DataFrames. Datasets enable compile-time type checking \nand can be seamlessly converted to DataFrames, allowing for flexible and efficient data processing.\nNow that we know what DataFrames and Datasets are, we\u2019ll see how to apply different Spark SQL \noperations to these constructs in the next section.", "mimetype": "text/plain", "start_char_idx": 210504, "end_char_idx": 211454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829": {"__data__": {"id_": "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a4a01bf-2da2-45c4-9308-a0105bb17ddb", "node_type": "1", "metadata": {}, "hash": "c817bc03649348ebd73d4eb8f544961e7a9c572ef97481de683ebece1cd1bdfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b753f5f-0758-4880-992c-183d9bd368a5", "node_type": "1", "metadata": {}, "hash": "9a82671452f3960373e73bd19db3005b03d727b5b97923882f019439a675c4f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datasets combine the benefits of Spark\u2019s RDDs (strong typing and user-defined functions) \nwith the performance optimizations of DataFrames. Datasets enable compile-time type checking \nand can be seamlessly converted to DataFrames, allowing for flexible and efficient data processing.\nNow that we know what DataFrames and Datasets are, we\u2019ll see how to apply different Spark SQL \noperations to these constructs in the next section.\nGetting started with Spark SQL\nTo get started with Spark SQL operations, we would first need to load data into a DataFrame. We\u2019ll \nsee how to do that next. Then, we will see how we can switch between PySpark and Spark SQL data \nand apply different transformations to it.\n\nSQL Queries in Spark\n\nLoading and saving data\nIn this section, we will explore various techniques for loading data into Spark SQL from different \nsources and saving this as a table.", "mimetype": "text/plain", "start_char_idx": 211024, "end_char_idx": 211908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b753f5f-0758-4880-992c-183d9bd368a5": {"__data__": {"id_": "5b753f5f-0758-4880-992c-183d9bd368a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829", "node_type": "1", "metadata": {}, "hash": "506483ba66f2bc80530c382fd9e0b11461e2a35e61af8d527e0784f0ff6a052e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2386b8b7-b35d-48d1-b683-bf4d97667f12", "node_type": "1", "metadata": {}, "hash": "026c093f5fe74301238db556ac73ce746258ad6c0b237eb30ed7176f4317b7c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We\u2019ll \nsee how to do that next. Then, we will see how we can switch between PySpark and Spark SQL data \nand apply different transformations to it.\n\nSQL Queries in Spark\n\nLoading and saving data\nIn this section, we will explore various techniques for loading data into Spark SQL from different \nsources and saving this as a table. We will delve into Python code examples that demonstrate how to \neffectively load data into Spark SQL, perform the necessary transformations, and save the processed \ndata as a table for further analysis.\nExecuting SQL queries in Spark SQL allows us to leverage the familiar SQL syntax and take advantage \nof its expressive power.", "mimetype": "text/plain", "start_char_idx": 211579, "end_char_idx": 212238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2386b8b7-b35d-48d1-b683-bf4d97667f12": {"__data__": {"id_": "2386b8b7-b35d-48d1-b683-bf4d97667f12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b753f5f-0758-4880-992c-183d9bd368a5", "node_type": "1", "metadata": {}, "hash": "9a82671452f3960373e73bd19db3005b03d727b5b97923882f019439a675c4f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "807b3734-458d-44ff-a08c-26d72136d815", "node_type": "1", "metadata": {}, "hash": "8dd1c44a341b5483f2f452e973d9e111e4cb43f605e1ac2314aaa9f756be728b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will delve into Python code examples that demonstrate how to \neffectively load data into Spark SQL, perform the necessary transformations, and save the processed \ndata as a table for further analysis.\nExecuting SQL queries in Spark SQL allows us to leverage the familiar SQL syntax and take advantage \nof its expressive power. Let\u2019s take a look at the syntax and an example of executing an SQL query \nusing Spark SQL:\nTo execute an SQL query in Spark SQL, we use the spark.sql() method as follows:\nresults = spark.sql(\"SELECT * FROM tableName\")\n\u2022\t The spark.sql() method is used to execute SQL queries in Spark SQL\n\u2022\t Inside the method, we provide the SQL query as a string argument. In this example, we select \nall columns from the tableName table\n\u2022\t The results of the query are stored in the results variable, which can be further processed \nor displayed as desired\nTo start with code examples in this chapter,", "mimetype": "text/plain", "start_char_idx": 211909, "end_char_idx": 212825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "807b3734-458d-44ff-a08c-26d72136d815": {"__data__": {"id_": "807b3734-458d-44ff-a08c-26d72136d815", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2386b8b7-b35d-48d1-b683-bf4d97667f12", "node_type": "1", "metadata": {}, "hash": "026c093f5fe74301238db556ac73ce746258ad6c0b237eb30ed7176f4317b7c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21c2eb3b-bceb-4a85-acc3-5545e6f8a985", "node_type": "1", "metadata": {}, "hash": "2dca60af81c67a3de74d77ea227cd5a6cefb843cd09f7074492f4e276023b283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this example, we select \nall columns from the tableName table\n\u2022\t The results of the query are stored in the results variable, which can be further processed \nor displayed as desired\nTo start with code examples in this chapter, we will use the DataFrame we created in Chapter 4:\nsalary_data_with_id = [(1, \"John\", \"Field-eng\", 3500, 40), \\\n(2, \"Robert\", \"Sales\", 4000, 38), \\\n(3, \"Maria\", \"Finance\", 3500, 28), \\\n(4, \"Michael\", \"Sales\", 3000, 20), \\\n(5, \"Kelly\", \"Finance\", 3500, 35), \\\n(6, \"Kate\", \"Finance\", 3000, 45), \\\n(7, \"Martin\", \"Finance\", 3500, 26), \\\n(8, \"Kiran\", \"Sales\", 2200,", "mimetype": "text/plain", "start_char_idx": 212596, "end_char_idx": 213186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21c2eb3b-bceb-4a85-acc3-5545e6f8a985": {"__data__": {"id_": "21c2eb3b-bceb-4a85-acc3-5545e6f8a985", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "807b3734-458d-44ff-a08c-26d72136d815", "node_type": "1", "metadata": {}, "hash": "8dd1c44a341b5483f2f452e973d9e111e4cb43f605e1ac2314aaa9f756be728b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9799d198-6519-4412-a983-efff37170970", "node_type": "1", "metadata": {}, "hash": "5df6bf46a28edfa417460b1d75a72b78138a2f563430285db1f4005be90f3721", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3500, 28), \\\n(4, \"Michael\", \"Sales\", 3000, 20), \\\n(5, \"Kelly\", \"Finance\", 3500, 35), \\\n(6, \"Kate\", \"Finance\", 3000, 45), \\\n(7, \"Martin\", \"Finance\", 3500, 26), \\\n(8, \"Kiran\", \"Sales\", 2200, 35), \\\n]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\", \"Age\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_\nid, schema = columns)\nsalary_data_with_id.", "mimetype": "text/plain", "start_char_idx": 212998, "end_char_idx": 213366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9799d198-6519-4412-a983-efff37170970": {"__data__": {"id_": "9799d198-6519-4412-a983-efff37170970", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21c2eb3b-bceb-4a85-acc3-5545e6f8a985", "node_type": "1", "metadata": {}, "hash": "2dca60af81c67a3de74d77ea227cd5a6cefb843cd09f7074492f4e276023b283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17", "node_type": "1", "metadata": {}, "hash": "e68992fb907af5d31cc72b9a7ea798964e1c7f86b7246f37fb0d80c56add153e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Finance\", 3000, 45), \\\n(7, \"Martin\", \"Finance\", 3500, 26), \\\n(8, \"Kiran\", \"Sales\", 2200, 35), \\\n]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\", \"Age\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_\nid, schema = columns)\nsalary_data_with_id.show()\n\nGetting started with Spark SQL\n\nThe output will be the following:\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|1|John| Field-eng|3500| 40|\n|2|Robert|Sales|4000| 38|\n|3|Maria|Finance|3500| 28|\n|4| Michael|Sales|3000| 20|\n|5|Kelly|Finance|3500| 35|\n|6|Kate|Finance|3000| 45|\n|7|Martin|Finance|3500| 26|\n|8|Kiran|Sales|2200| 35|\n+---+--------+----------+------+---+\nI have added an Age column in this DataFrame for further processing.", "mimetype": "text/plain", "start_char_idx": 213097, "end_char_idx": 213872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17": {"__data__": {"id_": "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9799d198-6519-4412-a983-efff37170970", "node_type": "1", "metadata": {}, "hash": "5df6bf46a28edfa417460b1d75a72b78138a2f563430285db1f4005be90f3721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076", "node_type": "1", "metadata": {}, "hash": "d9a8924c02f5e95eca7b76086b4308a51c92e585b2822e79fc19eacf564c399d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Remember, in Chapter 4, we saved this DataFrame as a CSV file. The code snippet we used to read \nCSV files can be seen in the following code.\nAs you might recall, we write CSV files with Spark using this line of code:\nsalary_data_with_id.write.format(\"csv\").mode(\"overwrite\").\noption(\"header\", \"true\").save(\"salary_data.", "mimetype": "text/plain", "start_char_idx": 213873, "end_char_idx": 214193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076": {"__data__": {"id_": "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17", "node_type": "1", "metadata": {}, "hash": "e68992fb907af5d31cc72b9a7ea798964e1c7f86b7246f37fb0d80c56add153e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40c989b7-a919-43d8-96bb-b8f3f4663938", "node_type": "1", "metadata": {}, "hash": "a1dff6dbaacaf1bd8d0191f20d4a702a9eac44dd52db5389c50cc6d57530f028", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Remember, in Chapter 4, we saved this DataFrame as a CSV file. The code snippet we used to read \nCSV files can be seen in the following code.\nAs you might recall, we write CSV files with Spark using this line of code:\nsalary_data_with_id.write.format(\"csv\").mode(\"overwrite\").\noption(\"header\", \"true\").save(\"salary_data.csv\")\nThe output will be the following:\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|1|John| Field-eng|3500| 40|\n|2|Robert|Sales|4000| 38|\n|3|Maria|Finance|3500| 28|\n|4| Michael|Sales|3000| 20|\n|5|Kelly|Finance|3500| 35|\n|6|Kate|Finance|3000| 45|\n|7|Martin|Finance|3500| 26|\n|8|Kiran|Sales|2200| 35|\n+---+--------+----------+------+---+\n\nSQL Queries in Spark\n\nNow that we have the DataFrame,", "mimetype": "text/plain", "start_char_idx": 213873, "end_char_idx": 214651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40c989b7-a919-43d8-96bb-b8f3f4663938": {"__data__": {"id_": "40c989b7-a919-43d8-96bb-b8f3f4663938", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076", "node_type": "1", "metadata": {}, "hash": "d9a8924c02f5e95eca7b76086b4308a51c92e585b2822e79fc19eacf564c399d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84", "node_type": "1", "metadata": {}, "hash": "30e59884349d6c6856ed30ca370a18f4afb7376ff4c1e89d16760d16c06303dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can use SQL operations on it:\n# Perform transformations on the loaded data\nprocessed_data = csv_data.filter(csv_data[\"Salary\"] > 3000)\n# Save the processed data as a table\nprocessed_data.createOrReplaceTempView(\"high_salary_employees\")\n# Perform SQL queries on the saved table\nresults = spark.sql(\"SELECT * FROM high_salary_employees \")\nresults.", "mimetype": "text/plain", "start_char_idx": 214652, "end_char_idx": 215000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84": {"__data__": {"id_": "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40c989b7-a919-43d8-96bb-b8f3f4663938", "node_type": "1", "metadata": {}, "hash": "a1dff6dbaacaf1bd8d0191f20d4a702a9eac44dd52db5389c50cc6d57530f028", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddab7232-fd5b-474f-b865-d333d3ed0af5", "node_type": "1", "metadata": {}, "hash": "5d106ab366d08d3b0c5a16dad166ffc69a7e4acb3788ad884b1e479d3d06e787", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can use SQL operations on it:\n# Perform transformations on the loaded data\nprocessed_data = csv_data.filter(csv_data[\"Salary\"] > 3000)\n# Save the processed data as a table\nprocessed_data.createOrReplaceTempView(\"high_salary_employees\")\n# Perform SQL queries on the saved table\nresults = spark.sql(\"SELECT * FROM high_salary_employees \")\nresults.show()\nThe output will be the following:\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|1|John| Field-eng|3500| 40|\n|2|Robert|Sales|4000| 38|\n|3|Maria|Finance|3500| 28|\n|5|Kelly|Finance|3500| 35|\n|7|Martin|Finance|3500| 26|\n+---+--------+----------+------+---+\nThe preceding code snippet shows how to perform a transformation on the loaded data.", "mimetype": "text/plain", "start_char_idx": 214652, "end_char_idx": 215410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddab7232-fd5b-474f-b865-d333d3ed0af5": {"__data__": {"id_": "ddab7232-fd5b-474f-b865-d333d3ed0af5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84", "node_type": "1", "metadata": {}, "hash": "30e59884349d6c6856ed30ca370a18f4afb7376ff4c1e89d16760d16c06303dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c4f1ec5-b7fd-4412-9b82-ef84d2320779", "node_type": "1", "metadata": {}, "hash": "7863cf65400974cffd69702bd0e7f89c3263a2babf9013fc8198e7a0e27bba56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, \nwe filter the data to only include rows where the Salary column is greater than 3,000.\nBy using the filter() function, we can apply specific conditions to select the desired subset of data.\nThe transformed data will be stored in the results variable and ready for further analysis.\nSaving transformed data as a view\nOnce we have performed the necessary transformations, it is often useful to save the processed data \nas a view for easier access and future analysis. Let\u2019s see how we can accomplish this in Spark SQL:\nThe createOrReplaceTempView() method allows us to save the processed data as a view in \nSpark SQL. We provide a name for the view, in this case, high_salary_employees.\nBy giving the table a meaningful name, we can easily refer to it in subsequent operations and queries. \nThe saved table acts as a structured representation of the processed data, facilitating further analysis \nand exploration.", "mimetype": "text/plain", "start_char_idx": 215411, "end_char_idx": 216337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c4f1ec5-b7fd-4412-9b82-ef84d2320779": {"__data__": {"id_": "5c4f1ec5-b7fd-4412-9b82-ef84d2320779", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddab7232-fd5b-474f-b865-d333d3ed0af5", "node_type": "1", "metadata": {}, "hash": "5d106ab366d08d3b0c5a16dad166ffc69a7e4acb3788ad884b1e479d3d06e787", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68511911-f243-49f7-9a48-79753276a56e", "node_type": "1", "metadata": {}, "hash": "ecf127715b34749a98c4003654ccdbd0546f781e6bba9ef82360fc0d0eff31f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We provide a name for the view, in this case, high_salary_employees.\nBy giving the table a meaningful name, we can easily refer to it in subsequent operations and queries. \nThe saved table acts as a structured representation of the processed data, facilitating further analysis \nand exploration.\nWith the transformed data saved as a table, we can leverage the power of SQL queries to gain insights \nand extract valuable information.\nBy using the spark.sql() method, we can execute SQL queries on the saved view high_salary_\nemployees.\n\nGetting started with Spark SQL\n\nIn the preceding example, we perform a simple query to select all columns from the view based on \na filter condition.\nThe show() function displays the results of the SQL query, allowing us to examine the desired \ninformation extracted from the dataset.\nAnother method to create a view in Spark SQL is createTempView().", "mimetype": "text/plain", "start_char_idx": 216042, "end_char_idx": 216928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68511911-f243-49f7-9a48-79753276a56e": {"__data__": {"id_": "68511911-f243-49f7-9a48-79753276a56e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c4f1ec5-b7fd-4412-9b82-ef84d2320779", "node_type": "1", "metadata": {}, "hash": "7863cf65400974cffd69702bd0e7f89c3263a2babf9013fc8198e7a0e27bba56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a11b71cc-ff01-4d32-97f8-27970c8a22e4", "node_type": "1", "metadata": {}, "hash": "516f9abe3d67ebb13a423090b35a02431687b328728fb171c3845bbae38c76b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Getting started with Spark SQL\n\nIn the preceding example, we perform a simple query to select all columns from the view based on \na filter condition.\nThe show() function displays the results of the SQL query, allowing us to examine the desired \ninformation extracted from the dataset.\nAnother method to create a view in Spark SQL is createTempView(). The difference between \nthis method and createOrReplaceTempView() method is that createTempView() \nwould only try to create a view. If that view name already exists in a catalog, then it would throw a \nTempTableAlreadyExistsException exception.\nUtilizing Spark SQL to filter and select data based on specific \ncriteria\nIn this section, we will explore the syntax and practical examples of executing SQL queries and applying \ntransformations using Spark SQL.\nLet\u2019s consider a practical example where we execute an SQL query to filter and select specific data \nfrom a table:\n# Save the processed data as a view\nsalary_data_with_id.", "mimetype": "text/plain", "start_char_idx": 216578, "end_char_idx": 217558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a11b71cc-ff01-4d32-97f8-27970c8a22e4": {"__data__": {"id_": "a11b71cc-ff01-4d32-97f8-27970c8a22e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68511911-f243-49f7-9a48-79753276a56e", "node_type": "1", "metadata": {}, "hash": "ecf127715b34749a98c4003654ccdbd0546f781e6bba9ef82360fc0d0eff31f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c77e3048-a464-477d-94b0-4a31e7c96a55", "node_type": "1", "metadata": {}, "hash": "eea46ca5a712c84b4fed76aacf980e4d7e6206247f9811c3bc68b960252e9e56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Utilizing Spark SQL to filter and select data based on specific \ncriteria\nIn this section, we will explore the syntax and practical examples of executing SQL queries and applying \ntransformations using Spark SQL.\nLet\u2019s consider a practical example where we execute an SQL query to filter and select specific data \nfrom a table:\n# Save the processed data as a view\nsalary_data_with_id.createOrReplaceTempView(\"employees\")\n#Apply filtering on data\nfiltered_data = spark.sql(\"SELECT Employee, Department, Salary, Age \nFROM employees WHERE age > 30\")\n# Display the results\nfiltered_data.", "mimetype": "text/plain", "start_char_idx": 217174, "end_char_idx": 217757, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c77e3048-a464-477d-94b0-4a31e7c96a55": {"__data__": {"id_": "c77e3048-a464-477d-94b0-4a31e7c96a55", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a11b71cc-ff01-4d32-97f8-27970c8a22e4", "node_type": "1", "metadata": {}, "hash": "516f9abe3d67ebb13a423090b35a02431687b328728fb171c3845bbae38c76b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "167a1ef0-9ed4-4cc8-a523-dfccf6e98066", "node_type": "1", "metadata": {}, "hash": "42557684170b4f4131ec723e9cab8092c44fcd7afe83d2cca372dc0b67c15824", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s consider a practical example where we execute an SQL query to filter and select specific data \nfrom a table:\n# Save the processed data as a view\nsalary_data_with_id.createOrReplaceTempView(\"employees\")\n#Apply filtering on data\nfiltered_data = spark.sql(\"SELECT Employee, Department, Salary, Age \nFROM employees WHERE age > 30\")\n# Display the results\nfiltered_data.show()\nThe output will be the following:\n+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|John| Field-eng|3500| 40|\n|Robert|Sales|4000| 38|\n|Kelly|Finance|3500| 35|\n|Kate|Finance|3000| 45|\n|Kiran|Sales|2200| 35|\n+--------+----------+------+---+\nIn this example,", "mimetype": "text/plain", "start_char_idx": 217387, "end_char_idx": 218069, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "167a1ef0-9ed4-4cc8-a523-dfccf6e98066": {"__data__": {"id_": "167a1ef0-9ed4-4cc8-a523-dfccf6e98066", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c77e3048-a464-477d-94b0-4a31e7c96a55", "node_type": "1", "metadata": {}, "hash": "eea46ca5a712c84b4fed76aacf980e4d7e6206247f9811c3bc68b960252e9e56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2085a59e-fb6b-46be-b090-7bf1c92ea15e", "node_type": "1", "metadata": {}, "hash": "86389a8485f947866ff59f4657fa96fb71a463aad103bf63956a7692f91942f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we create a temp view with the names of employees and execute an SQL query \nusing Spark SQL to filter and select specific columns from the employees table.\nThe query selects the employee, department, salary, and age columns from the table where \nage is greater than 30. The results of the query are stored in the filtered_data variable.\n\nSQL Queries in Spark\n\nFinally, we call the show() method to display the filtered data.\nExploring sorting and aggregation operations using Spark SQL\nSpark SQL provides a rich set of transformation functions that can be applied to manipulate and \ntransform data. Let\u2019s explore some of the practical examples of transformations in Spark SQL:\nAggregation\nIn this example, we perform an aggregation operation using Spark SQL to calculate the average salary \nfrom the employees table.", "mimetype": "text/plain", "start_char_idx": 218070, "end_char_idx": 218886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2085a59e-fb6b-46be-b090-7bf1c92ea15e": {"__data__": {"id_": "2085a59e-fb6b-46be-b090-7bf1c92ea15e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "167a1ef0-9ed4-4cc8-a523-dfccf6e98066", "node_type": "1", "metadata": {}, "hash": "42557684170b4f4131ec723e9cab8092c44fcd7afe83d2cca372dc0b67c15824", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df9ff41f-0747-475a-8d43-7caae5e3b199", "node_type": "1", "metadata": {}, "hash": "0ac73f7d34799e99858cee5aefd7d7c0fe78c33c1ba3fd48c9c475dd9971be88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SQL Queries in Spark\n\nFinally, we call the show() method to display the filtered data.\nExploring sorting and aggregation operations using Spark SQL\nSpark SQL provides a rich set of transformation functions that can be applied to manipulate and \ntransform data. Let\u2019s explore some of the practical examples of transformations in Spark SQL:\nAggregation\nIn this example, we perform an aggregation operation using Spark SQL to calculate the average salary \nfrom the employees table.\n# Perform an aggregation to calculate the average salary\naverage_salary = spark.sql(\"SELECT AVG(Salary) AS average_salary FROM \nemployees\")\n# Display the average salary\naverage_salary.show()\nThe output will be the following:\n+--------------+\n|average_salary|\n+--------------+\n|3275.0|\n+--------------+\nThe AVG() function calculates the average of the salary column. We alias the result as average_\nsalary using the AS keyword.", "mimetype": "text/plain", "start_char_idx": 218408, "end_char_idx": 219313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df9ff41f-0747-475a-8d43-7caae5e3b199": {"__data__": {"id_": "df9ff41f-0747-475a-8d43-7caae5e3b199", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2085a59e-fb6b-46be-b090-7bf1c92ea15e", "node_type": "1", "metadata": {}, "hash": "86389a8485f947866ff59f4657fa96fb71a463aad103bf63956a7692f91942f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26094046-8c48-46b7-9ba4-9271dc0fef0c", "node_type": "1", "metadata": {}, "hash": "056f95195c42740ee31373af8e57eac29ca9c6fbecf2de5b81207a854413898f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Perform an aggregation to calculate the average salary\naverage_salary = spark.sql(\"SELECT AVG(Salary) AS average_salary FROM \nemployees\")\n# Display the average salary\naverage_salary.show()\nThe output will be the following:\n+--------------+\n|average_salary|\n+--------------+\n|3275.0|\n+--------------+\nThe AVG() function calculates the average of the salary column. We alias the result as average_\nsalary using the AS keyword.\nThe results are stored in the average_salary variable and displayed using the show() method:\nSorting\nIn this example, we apply a sorting transformation to the employees table using Spark SQL.\n# Sort the data based on the salary column in descending order\nsorted_data = spark.sql(\"SELECT * FROM employees ORDER BY Salary \nDESC\")\n# Display the sorted data\nsorted_data.", "mimetype": "text/plain", "start_char_idx": 218887, "end_char_idx": 219680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26094046-8c48-46b7-9ba4-9271dc0fef0c": {"__data__": {"id_": "26094046-8c48-46b7-9ba4-9271dc0fef0c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df9ff41f-0747-475a-8d43-7caae5e3b199", "node_type": "1", "metadata": {}, "hash": "0ac73f7d34799e99858cee5aefd7d7c0fe78c33c1ba3fd48c9c475dd9971be88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a", "node_type": "1", "metadata": {}, "hash": "d0776f1eb4683f4a217d036bbe7e5ed3ae5294716f882a4c71f2cb3684d3b205", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We alias the result as average_\nsalary using the AS keyword.\nThe results are stored in the average_salary variable and displayed using the show() method:\nSorting\nIn this example, we apply a sorting transformation to the employees table using Spark SQL.\n# Sort the data based on the salary column in descending order\nsorted_data = spark.sql(\"SELECT * FROM employees ORDER BY Salary \nDESC\")\n# Display the sorted data\nsorted_data.show()\n\nGetting started with Spark SQL\n\nThe output will be the following:\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|2|Robert|Sales|4000| 38|\n|1|John| Field-eng|3500| 40|\n|5|Kelly|Finance|3500| 35|\n|3|Maria|Finance|3500| 28|\n|7|Martin|Finance|3500| 26|\n|6|Kate|Finance|3000| 45|\n|4| Michael|Sales|3000| 20|\n|8|Kiran|Sales|2200| 35|\n+---+--------+----------+------+---+\nThe ORDER BY clause is used to specify the sorting criteria,", "mimetype": "text/plain", "start_char_idx": 219253, "end_char_idx": 220178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a": {"__data__": {"id_": "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26094046-8c48-46b7-9ba4-9271dc0fef0c", "node_type": "1", "metadata": {}, "hash": "056f95195c42740ee31373af8e57eac29ca9c6fbecf2de5b81207a854413898f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "691d35b6-2000-4e81-ac07-6a1905056a7c", "node_type": "1", "metadata": {}, "hash": "db4ffaabb591338fa58b44e45663fd65b09bd2c1e69675115ba105ed2b788366", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in this case, the salary column in \ndescending order.\nThe sorted data are stored in the sorted_data variable and displayed using the show() method.", "mimetype": "text/plain", "start_char_idx": 220179, "end_char_idx": 220326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "691d35b6-2000-4e81-ac07-6a1905056a7c": {"__data__": {"id_": "691d35b6-2000-4e81-ac07-6a1905056a7c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a", "node_type": "1", "metadata": {}, "hash": "d0776f1eb4683f4a217d036bbe7e5ed3ae5294716f882a4c71f2cb3684d3b205", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c3228df-0c16-4652-81e5-70005bbb573a", "node_type": "1", "metadata": {}, "hash": "72f1ea941174f4d407f77ec568f3e24bd5640efdea94b3e5a6cc83b3222552ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in this case, the salary column in \ndescending order.\nThe sorted data are stored in the sorted_data variable and displayed using the show() method.\nCombining aggregations\nWe can also combine different aggregations in one SQL command, such as in the following code example:\n# Sort the data based on the salary column in descending order\nfiltered_data = spark.sql(\"SELECT Employee, Department, Salary, Age \nFROM employees WHERE age > 30 AND Salary > 3000 ORDER BY Salary DESC\")\n# Display the results\nfiltered_data.show()\nThe output will be the following:\n+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|Robert|Sales|4000| 38|\n|Kelly|Finance|3500| 35|\n|John| Field-eng|3500| 40|\n+--------+----------+------+---+\nIn this example, we combine different transformations to the employees table using Spark SQL.", "mimetype": "text/plain", "start_char_idx": 220179, "end_char_idx": 221033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c3228df-0c16-4652-81e5-70005bbb573a": {"__data__": {"id_": "4c3228df-0c16-4652-81e5-70005bbb573a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "691d35b6-2000-4e81-ac07-6a1905056a7c", "node_type": "1", "metadata": {}, "hash": "db4ffaabb591338fa58b44e45663fd65b09bd2c1e69675115ba105ed2b788366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86be2ce0-124b-413d-a8bd-6d3436f50424", "node_type": "1", "metadata": {}, "hash": "db2a2d9efad11c2cacfca048dd9c58828ce9439dad7812f3107b911aa5a6978b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we select those employees whose age is greater than 30 and who have a salary greater than \n3,000. The ORDER BY clause is used to specify the sorting criteria; in this case, the salary column \nin descending order.\n\nSQL Queries in Spark\n\nThe resulting data are stored in the \u201cfiltered_data\u201d variable and displayed using the show() method.\nIn this section, we explored the process of executing SQL queries and applying transformations using \nSpark SQL. We learned about the syntax for executing SQL queries and demonstrated practical examples \nof executing queries, filtering data, performing aggregations, and sorting data. By leveraging the \nexpressive power of SQL and the flexibility of Spark SQL, you can efficiently analyze and manipulate \nstructured data for a wide range of data analysis tasks.", "mimetype": "text/plain", "start_char_idx": 221035, "end_char_idx": 221841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86be2ce0-124b-413d-a8bd-6d3436f50424": {"__data__": {"id_": "86be2ce0-124b-413d-a8bd-6d3436f50424", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c3228df-0c16-4652-81e5-70005bbb573a", "node_type": "1", "metadata": {}, "hash": "72f1ea941174f4d407f77ec568f3e24bd5640efdea94b3e5a6cc83b3222552ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae", "node_type": "1", "metadata": {}, "hash": "00d7cb2e7325199f1ac7bbed5898e73d26eaf1bc2ed1366c30ff1849052efdba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we explored the process of executing SQL queries and applying transformations using \nSpark SQL. We learned about the syntax for executing SQL queries and demonstrated practical examples \nof executing queries, filtering data, performing aggregations, and sorting data. By leveraging the \nexpressive power of SQL and the flexibility of Spark SQL, you can efficiently analyze and manipulate \nstructured data for a wide range of data analysis tasks.\nGrouping and aggregating data \u2013 grouping data based on \nspecific columns and performing aggregate functions\nIn Spark SQL, grouping and aggregating data are common operations that are performed to gain \ninsights and summarize information from large datasets. This section will explore how to group data \nbased on specific columns and perform various aggregate functions using Spark SQL. We will walk \nthrough code examples that demonstrate the capabilities of Spark SQL in this regard.", "mimetype": "text/plain", "start_char_idx": 221379, "end_char_idx": 222326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae": {"__data__": {"id_": "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86be2ce0-124b-413d-a8bd-6d3436f50424", "node_type": "1", "metadata": {}, "hash": "db2a2d9efad11c2cacfca048dd9c58828ce9439dad7812f3107b911aa5a6978b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62dd54ac-704c-4ee0-a449-966779825fdd", "node_type": "1", "metadata": {}, "hash": "34fe3ef9bfbfcc64092567d25534ed4b42ee7b27c82e79ef846244c80a6a857d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Grouping and aggregating data \u2013 grouping data based on \nspecific columns and performing aggregate functions\nIn Spark SQL, grouping and aggregating data are common operations that are performed to gain \ninsights and summarize information from large datasets. This section will explore how to group data \nbased on specific columns and perform various aggregate functions using Spark SQL. We will walk \nthrough code examples that demonstrate the capabilities of Spark SQL in this regard.\nGrouping data\nWhen we want to group data based on specific columns, we can utilize the GROUP BY clause in \nSQL queries. Let\u2019s consider an example where we have a DataFrame of employees with the columns \ndepartment and salary.", "mimetype": "text/plain", "start_char_idx": 221842, "end_char_idx": 222552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62dd54ac-704c-4ee0-a449-966779825fdd": {"__data__": {"id_": "62dd54ac-704c-4ee0-a449-966779825fdd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae", "node_type": "1", "metadata": {}, "hash": "00d7cb2e7325199f1ac7bbed5898e73d26eaf1bc2ed1366c30ff1849052efdba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85479058-7981-4c8c-a0a4-987f8cb82697", "node_type": "1", "metadata": {}, "hash": "f4559753071ffd47346d93f67e8690bad3f5b443dd72857f1d3f19a2f272095e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This section will explore how to group data \nbased on specific columns and perform various aggregate functions using Spark SQL. We will walk \nthrough code examples that demonstrate the capabilities of Spark SQL in this regard.\nGrouping data\nWhen we want to group data based on specific columns, we can utilize the GROUP BY clause in \nSQL queries. Let\u2019s consider an example where we have a DataFrame of employees with the columns \ndepartment and salary. We want to calculate the average salary for each department:\n# Group the data based on the Department column and take average \nsalary for each department\ngrouped_data = spark.sql(\"SELECT Department, avg(Salary) FROM \nemployees GROUP BY Department\")\n# Display the results\ngrouped_data.show()\nThe output will be the following:\n+----------+------------------+\n|Department|avg(Salary)|\n+----------+------------------+\n| Field-eng|3500.0|\n|Sales|3066.6666666666665|\n|Finance|3375.0|\n+----------+------------------+\nIn this example, we group the data based on different transformations to the employees table using \nSpark SQL.", "mimetype": "text/plain", "start_char_idx": 222100, "end_char_idx": 223173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85479058-7981-4c8c-a0a4-987f8cb82697": {"__data__": {"id_": "85479058-7981-4c8c-a0a4-987f8cb82697", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62dd54ac-704c-4ee0-a449-966779825fdd", "node_type": "1", "metadata": {}, "hash": "34fe3ef9bfbfcc64092567d25534ed4b42ee7b27c82e79ef846244c80a6a857d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e69f2877-a20a-4d40-9bf5-68e22ade50a7", "node_type": "1", "metadata": {}, "hash": "91b39ccec4ab01b4637784a4e481c5a1ccf3fa0cf15e474dce54da64b979c105", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we group employees based on the Department column. We take the average salary \nof each department from the employees table.\nThe resulting data are stored in the grouped_data variable and displayed using the show() method.\n\nAdvanced Spark SQL operations\n\nAggregating data\nSpark SQL provides a wide range of aggregate functions to calculate summary statistics on grouped \ndata.", "mimetype": "text/plain", "start_char_idx": 223174, "end_char_idx": 223556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e69f2877-a20a-4d40-9bf5-68e22ade50a7": {"__data__": {"id_": "e69f2877-a20a-4d40-9bf5-68e22ade50a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85479058-7981-4c8c-a0a4-987f8cb82697", "node_type": "1", "metadata": {}, "hash": "f4559753071ffd47346d93f67e8690bad3f5b443dd72857f1d3f19a2f272095e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8e69be8-2bf0-4b01-8c62-71fda435f610", "node_type": "1", "metadata": {}, "hash": "7891c57f969d5d0ca4d6f01830aa36350e14caa7ba84bf45a49e2f519a28eb35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we group employees based on the Department column. We take the average salary \nof each department from the employees table.\nThe resulting data are stored in the grouped_data variable and displayed using the show() method.\n\nAdvanced Spark SQL operations\n\nAggregating data\nSpark SQL provides a wide range of aggregate functions to calculate summary statistics on grouped \ndata. Let\u2019s consider another example where we want to calculate the total salary and the maximum \nsalary for each department:\n# Perform grouping and multiple aggregations\naggregated_data = spark.sql(\"SELECT Department, sum(Salary) AS total_\nsalary, max(Salary) AS max_salary FROM employees GROUP BY Department\")\n# Display the results\naggregated_data.show()\nThe output will be the following:\n+----------+-----------+-----------+\n|Department|sum(Salary)|max(Salary)|\n+----------+-----------+-----------+\n| Field-eng|3500|3500|\n|Sales|9200|4000|\n|Finance|13500|3500|\n+----------+-----------+-----------+\nIn this example, we combine and group the data based on different transformations to the employees \ntable using Spark SQL.", "mimetype": "text/plain", "start_char_idx": 223174, "end_char_idx": 224274, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8e69be8-2bf0-4b01-8c62-71fda435f610": {"__data__": {"id_": "c8e69be8-2bf0-4b01-8c62-71fda435f610", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e69f2877-a20a-4d40-9bf5-68e22ade50a7", "node_type": "1", "metadata": {}, "hash": "91b39ccec4ab01b4637784a4e481c5a1ccf3fa0cf15e474dce54da64b979c105", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120", "node_type": "1", "metadata": {}, "hash": "7307332290dc9d777029dab10c2a50cfb484d36aa090406d08f9c49af2cf81b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we group the employees based on Department column. We take the \ntotal salary and maximum salary of each department from the employees table. We also use an alias \nfor these aggregated columns.\nThe resulting data are stored in the aggregated_data variable and displayed using the \nshow() method.\nIn this section, we have explored the capabilities of Spark SQL in grouping and aggregating data. We \nhave seen examples of how to group data based on specific columns and perform various aggregate \nfunctions. Spark SQL provides a wide range of aggregate functions and allows for the creation of \ncustom aggregate functions to suit specific requirements. With these capabilities, you can efficiently \nsummarize and gain insights from large datasets using Spark SQL.\nIn the next section, we will look at advanced Spark SQL functions for complex data \nmanipulation operations.\nAdvanced Spark SQL operations\nLet's explore the key capabilities of Apache Spark's advanced operations.", "mimetype": "text/plain", "start_char_idx": 224275, "end_char_idx": 225255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120": {"__data__": {"id_": "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8e69be8-2bf0-4b01-8c62-71fda435f610", "node_type": "1", "metadata": {}, "hash": "7891c57f969d5d0ca4d6f01830aa36350e14caa7ba84bf45a49e2f519a28eb35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d85dd27c-afa6-472f-93a6-0dd68a5f93a3", "node_type": "1", "metadata": {}, "hash": "3243ba0ae8b591d8b335f915a277e4186ece32a21e4208b3ffa5247169a0d9f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark SQL provides a wide range of aggregate functions and allows for the creation of \ncustom aggregate functions to suit specific requirements. With these capabilities, you can efficiently \nsummarize and gain insights from large datasets using Spark SQL.\nIn the next section, we will look at advanced Spark SQL functions for complex data \nmanipulation operations.\nAdvanced Spark SQL operations\nLet's explore the key capabilities of Apache Spark's advanced operations.\n\nSQL Queries in Spark\n\nLeveraging window functions to perform advanced analytical \noperations on DataFrames\nIn this section, we will explore the powerful capabilities of window functions in Spark SQL for \nperforming advanced analytical operations on DataFrames. Window functions provide a way to \nperform calculations across a set of rows within a partition, allowing us to derive insights and perform \ncomplex computations efficiently. In this section, we will dive into the topic of window functions and \nshowcase code examples that demonstrate their usage in Spark SQL queries.", "mimetype": "text/plain", "start_char_idx": 224787, "end_char_idx": 225836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d85dd27c-afa6-472f-93a6-0dd68a5f93a3": {"__data__": {"id_": "d85dd27c-afa6-472f-93a6-0dd68a5f93a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120", "node_type": "1", "metadata": {}, "hash": "7307332290dc9d777029dab10c2a50cfb484d36aa090406d08f9c49af2cf81b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64e6f783-13db-4080-92b8-1686925a45e5", "node_type": "1", "metadata": {}, "hash": "1818b3f6cc4d8868f0101d79193485354efe3d1846327266bd58fb5cd19d22f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Window functions provide a way to \nperform calculations across a set of rows within a partition, allowing us to derive insights and perform \ncomplex computations efficiently. In this section, we will dive into the topic of window functions and \nshowcase code examples that demonstrate their usage in Spark SQL queries.\nUnderstanding window functions\nWindow functions in Spark SQL enable advanced analytical operations by dividing a dataset into \ngroups or partitions based on specified criteria. These functions operate on a sliding window of rows \nwithin each partition, performing calculations or aggregations. \nThe general syntax for using window functions in Spark SQL is as follows:\nfunction().over(Window.partitionBy(\"column1\", \"column2\").\norderBy(\"column3\").rowsBetween(start, end))\nThe function() represents the window function that you want to apply, such as sum, avg, \nrow_number, or custom-defined functions. The over() clause defines the window to which the \nfunction is applied.", "mimetype": "text/plain", "start_char_idx": 225518, "end_char_idx": 226509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64e6f783-13db-4080-92b8-1686925a45e5": {"__data__": {"id_": "64e6f783-13db-4080-92b8-1686925a45e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d85dd27c-afa6-472f-93a6-0dd68a5f93a3", "node_type": "1", "metadata": {}, "hash": "3243ba0ae8b591d8b335f915a277e4186ece32a21e4208b3ffa5247169a0d9f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9", "node_type": "1", "metadata": {}, "hash": "bafc568b7d184f804c48d9dd0790cd333cc37c7430f10c3784addebdcca0478e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The general syntax for using window functions in Spark SQL is as follows:\nfunction().over(Window.partitionBy(\"column1\", \"column2\").\norderBy(\"column3\").rowsBetween(start, end))\nThe function() represents the window function that you want to apply, such as sum, avg, \nrow_number, or custom-defined functions. The over() clause defines the window to which the \nfunction is applied. Window.partitionBy() specifies the columns used to divide the dataset \ninto partitions. It also determines the order of rows within each partition. rowsBetween(start, \nend) specifies the range of rows included in the window. It can be unbounded or defined relative \nto the current row.\nCalculating cumulative sum using window functions\nLet\u2019s explore a practical example that demonstrates the usage of window functions to calculate a \ncumulative sum of a column in a DataFrame:\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,", "mimetype": "text/plain", "start_char_idx": 226132, "end_char_idx": 227063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9": {"__data__": {"id_": "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64e6f783-13db-4080-92b8-1686925a45e5", "node_type": "1", "metadata": {}, "hash": "1818b3f6cc4d8868f0101d79193485354efe3d1846327266bd58fb5cd19d22f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04955313-5e98-4dac-ac7e-8be543354f66", "node_type": "1", "metadata": {}, "hash": "46c6ca855d86a7bf77732033071d3813f0f35614ba26d82754ce089640f52d79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "rowsBetween(start, \nend) specifies the range of rows included in the window. It can be unbounded or defined relative \nto the current row.\nCalculating cumulative sum using window functions\nLet\u2019s explore a practical example that demonstrates the usage of window functions to calculate a \ncumulative sum of a column in a DataFrame:\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, sum\n# Define the window specification\nwindow_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n# Calculate the cumulative sum using window function\ndf_with_cumulative_sum = salary_data_with_id.withColumn(\"cumulative_\nsum\", sum(col(\"Salary\")).over(window_spec))\n# Display the result\ndf_with_cumulative_sum.", "mimetype": "text/plain", "start_char_idx": 226658, "end_char_idx": 227374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04955313-5e98-4dac-ac7e-8be543354f66": {"__data__": {"id_": "04955313-5e98-4dac-ac7e-8be543354f66", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9", "node_type": "1", "metadata": {}, "hash": "bafc568b7d184f804c48d9dd0790cd333cc37c7430f10c3784addebdcca0478e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f", "node_type": "1", "metadata": {}, "hash": "24ce77abf9301585059ea3db3a635d6225b3322326d50d89a0e7587d1fa19ad5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sql.window import Window\nfrom pyspark.sql.functions import col, sum\n# Define the window specification\nwindow_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n# Calculate the cumulative sum using window function\ndf_with_cumulative_sum = salary_data_with_id.withColumn(\"cumulative_\nsum\", sum(col(\"Salary\")).over(window_spec))\n# Display the result\ndf_with_cumulative_sum.show()\n\nAdvanced Spark SQL operations\n\nThe output will be the following:\n+---+--------+----------+------+---+--------------+\n| ID|Employee|Department|Salary|Age|cumulative_sum|\n+---+--------+----------+------+---+--------------+\n|1|John| Field-eng|3500| 40|3500|\n|7|Martin|Finance|3500| 26|3500|\n|3|Maria|Finance|3500| 28|7000|\n|5|Kelly|Finance|3500| 35|10500|\n|6|Kate|Finance|3000| 45|13500|\n|4| Michael|Sales|3000| 20|3000|\n|8|Kiran|Sales|2200| 35|5200|\n|2|Robert|Sales|4000| 38|9200|\n+---+--------+----------+------+---+--------------+\nIn this example,", "mimetype": "text/plain", "start_char_idx": 227000, "end_char_idx": 227929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f": {"__data__": {"id_": "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04955313-5e98-4dac-ac7e-8be543354f66", "node_type": "1", "metadata": {}, "hash": "46c6ca855d86a7bf77732033071d3813f0f35614ba26d82754ce089640f52d79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd7f50aa-239d-4bed-8ebb-2e891e2f7699", "node_type": "1", "metadata": {}, "hash": "dbdcf45c96e0d821d55eee467dc5ff819b9678ec780a8e12965f59d0b3c13841", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we start by importing the necessary libraries. We use the same DataFrame as our \nprevious examples: salary_data_with_id.\nNext, we define a window specification using Window.partitionBy(\"Department\").\norderBy(\"Age\"), which partitions the data according to the Department column and orders \nthe rows within each partition according to the Age column.\nWe then use the sum() function as a window function, applied over the defined window specification, \nto calculate the cumulative sum of the Salary column. The result is stored in a new column \ncalled cumulative_sum.\nFinally, we call the show() method to display the DataFrame with the added cumulative sum column. \nBy leveraging window functions, we can efficiently calculate cumulative sums, running totals, rolling \naverages, and other complex analytical calculations over defined windows in Spark SQL.\nIn this section, we explored the powerful capabilities of window functions in Spark SQL for advanced \nanalytics.", "mimetype": "text/plain", "start_char_idx": 227930, "end_char_idx": 228896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd7f50aa-239d-4bed-8ebb-2e891e2f7699": {"__data__": {"id_": "fd7f50aa-239d-4bed-8ebb-2e891e2f7699", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f", "node_type": "1", "metadata": {}, "hash": "24ce77abf9301585059ea3db3a635d6225b3322326d50d89a0e7587d1fa19ad5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3456c28-65fd-43a7-bb74-95c655782de6", "node_type": "1", "metadata": {}, "hash": "5ee115d825afed3c8a817249dac03be622ca3c61ad7dc79bb302ea87ced79ccf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The result is stored in a new column \ncalled cumulative_sum.\nFinally, we call the show() method to display the DataFrame with the added cumulative sum column. \nBy leveraging window functions, we can efficiently calculate cumulative sums, running totals, rolling \naverages, and other complex analytical calculations over defined windows in Spark SQL.\nIn this section, we explored the powerful capabilities of window functions in Spark SQL for advanced \nanalytics. We discussed the syntax and usage of window functions, allowing us to perform complex \ncalculations and aggregations within defined partitions and windows. By incorporating window \nfunctions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding \nof your data for advanced analytical operations.\nIn the next section, we will explore Spark user-defined functions.\nUser-defined functions\nIn this section, we will delve into the topic of user-defined functions (UDFs) in Spark SQL.", "mimetype": "text/plain", "start_char_idx": 228434, "end_char_idx": 229409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3456c28-65fd-43a7-bb74-95c655782de6": {"__data__": {"id_": "a3456c28-65fd-43a7-bb74-95c655782de6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7f50aa-239d-4bed-8ebb-2e891e2f7699", "node_type": "1", "metadata": {}, "hash": "dbdcf45c96e0d821d55eee467dc5ff819b9678ec780a8e12965f59d0b3c13841", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e7518a7-74db-47af-a5a7-88798742d86b", "node_type": "1", "metadata": {}, "hash": "0878b7220b436038c389b98d5e3ec376aa9400ccd2a5558114d6893932656049", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By incorporating window \nfunctions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding \nof your data for advanced analytical operations.\nIn the next section, we will explore Spark user-defined functions.\nUser-defined functions\nIn this section, we will delve into the topic of user-defined functions (UDFs) in Spark SQL. UDFs \nallow us to extend the functionality of Spark SQL by defining our custom functions that can be applied \nto DataFrames or SQL queries. In this section, we will explore the concept of UDFs and provide code \nexamples to demonstrate their usage and benefits in Spark SQL.\n\nSQL Queries in Spark\n\nUDFs in Spark SQL enable us to create custom functions to perform transformations or computations \non columns in a DataFrame or in SQL queries. UDFs are particularly useful when Spark\u2019s built-in \nfunctions do not meet our specific requirements.", "mimetype": "text/plain", "start_char_idx": 229053, "end_char_idx": 229951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e7518a7-74db-47af-a5a7-88798742d86b": {"__data__": {"id_": "4e7518a7-74db-47af-a5a7-88798742d86b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3456c28-65fd-43a7-bb74-95c655782de6", "node_type": "1", "metadata": {}, "hash": "5ee115d825afed3c8a817249dac03be622ca3c61ad7dc79bb302ea87ced79ccf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a4bd78-ce21-4557-bd0e-3820c4bf251f", "node_type": "1", "metadata": {}, "hash": "700c0d709d90c23c320054d6ef531872d7a29937ec48657c97c1bc0dc710c98e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we will explore the concept of UDFs and provide code \nexamples to demonstrate their usage and benefits in Spark SQL.\n\nSQL Queries in Spark\n\nUDFs in Spark SQL enable us to create custom functions to perform transformations or computations \non columns in a DataFrame or in SQL queries. UDFs are particularly useful when Spark\u2019s built-in \nfunctions do not meet our specific requirements. \nTo define a UDF in Spark SQL, we use the udf() function from the pyspark.sql.\nfunctions module. The general syntax is as follows:\nfrom pyspark.sql.functions import udf\nudf_name = udf(lambda_function, return_type)\nFirst, we import the udf() function from the pyspark.sql.functions module. Next, we define \nthe UDF by providing a lambda function or a regular Python function as the lambda_function \nargument. This function encapsulates the custom logic we want to apply.", "mimetype": "text/plain", "start_char_idx": 229550, "end_char_idx": 230421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32a4bd78-ce21-4557-bd0e-3820c4bf251f": {"__data__": {"id_": "32a4bd78-ce21-4557-bd0e-3820c4bf251f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e7518a7-74db-47af-a5a7-88798742d86b", "node_type": "1", "metadata": {}, "hash": "0878b7220b436038c389b98d5e3ec376aa9400ccd2a5558114d6893932656049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cc8e828-cd04-4209-ba93-02a1493f6031", "node_type": "1", "metadata": {}, "hash": "a282f67b621792414d6006cf9daa76bee0a00523ee74f647229871e0222442f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "functions module. The general syntax is as follows:\nfrom pyspark.sql.functions import udf\nudf_name = udf(lambda_function, return_type)\nFirst, we import the udf() function from the pyspark.sql.functions module. Next, we define \nthe UDF by providing a lambda function or a regular Python function as the lambda_function \nargument. This function encapsulates the custom logic we want to apply.\nWe also specify return_type for the UDF, which represents the data type that the UDF will return.\nApplying a UDF to a DataFrame\nLet\u2019s explore a practical example that demonstrates the usage of UDFs in Spark SQL by applying a \ncustom function to a DataFrame:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n# Define a UDF to capitalize a string\ncapitalize_udf = udf(lambda x: x.upper(),", "mimetype": "text/plain", "start_char_idx": 230031, "end_char_idx": 230875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5cc8e828-cd04-4209-ba93-02a1493f6031": {"__data__": {"id_": "5cc8e828-cd04-4209-ba93-02a1493f6031", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a4bd78-ce21-4557-bd0e-3820c4bf251f", "node_type": "1", "metadata": {}, "hash": "700c0d709d90c23c320054d6ef531872d7a29937ec48657c97c1bc0dc710c98e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c0c627e-ff85-47fb-82aa-9b8137d37c2a", "node_type": "1", "metadata": {}, "hash": "13798c7b3efff5c35790194481d6e81821248f0a8a40c63df308f8685708a25a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Applying a UDF to a DataFrame\nLet\u2019s explore a practical example that demonstrates the usage of UDFs in Spark SQL by applying a \ncustom function to a DataFrame:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n# Define a UDF to capitalize a string\ncapitalize_udf = udf(lambda x: x.upper(), StringType())\n# Apply the UDF to a column\ndf_with_capitalized_names = salary_data_with_\nid.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n# Display the result\ndf_with_capitalized_names.show()\n\nAdvanced Spark SQL operations\n\nThe output will be the following:\n+---+--------+----------+------+---+----------------+\n| ID|Employee|Department|Salary|Age|capitalized_name|\n+---+--------+----------+------+---+----------------+\n|1|John|", "mimetype": "text/plain", "start_char_idx": 230520, "end_char_idx": 231316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c0c627e-ff85-47fb-82aa-9b8137d37c2a": {"__data__": {"id_": "6c0c627e-ff85-47fb-82aa-9b8137d37c2a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cc8e828-cd04-4209-ba93-02a1493f6031", "node_type": "1", "metadata": {}, "hash": "a282f67b621792414d6006cf9daa76bee0a00523ee74f647229871e0222442f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6644d37-8b95-468b-b51c-05dca239d21b", "node_type": "1", "metadata": {}, "hash": "76dc7431dc49a692e98ece576097df06e26319e4a13845da0cbf152357c7558e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n# Display the result\ndf_with_capitalized_names.show()\n\nAdvanced Spark SQL operations\n\nThe output will be the following:\n+---+--------+----------+------+---+----------------+\n| ID|Employee|Department|Salary|Age|capitalized_name|\n+---+--------+----------+------+---+----------------+\n|1|John| Field-eng|3500| 40|JOHN|\n|2|Robert|Sales|4000| 38|ROBERT|\n|3|Maria|Finance|3500| 28|MARIA|\n|4| Michael|Sales|3000| 20|MICHAEL|\n|5|Kelly|Finance|3500| 35|KELLY|\n|6|Kate|Finance|3000| 45|KATE|\n|7|Martin|Finance|3500|", "mimetype": "text/plain", "start_char_idx": 230967, "end_char_idx": 231531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6644d37-8b95-468b-b51c-05dca239d21b": {"__data__": {"id_": "c6644d37-8b95-468b-b51c-05dca239d21b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c0c627e-ff85-47fb-82aa-9b8137d37c2a", "node_type": "1", "metadata": {}, "hash": "13798c7b3efff5c35790194481d6e81821248f0a8a40c63df308f8685708a25a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c64b9672-2727-499b-ba52-390a5cf914a4", "node_type": "1", "metadata": {}, "hash": "9431f54ac246edb50ca6e6ef7d9c4b05c2dd6828364a39eb3fab6ac28d056038", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "38|ROBERT|\n|3|Maria|Finance|3500| 28|MARIA|\n|4| Michael|Sales|3000| 20|MICHAEL|\n|5|Kelly|Finance|3500| 35|KELLY|\n|6|Kate|Finance|3000| 45|KATE|\n|7|Martin|Finance|3500| 26|MARTIN|\n|8|Kiran|Sales|2200| 35|KIRAN|\n+---+--------+----------+------+---+----------------+\nIn this example, we start by defining a UDF called capitalize_udf using the udf() function. It \napplies a lambda function that changes the input string to upper case. We use the withColumn() \nmethod to apply the UDF capitalize_udf to the name column, creating a new column called \ncapitalized_name in the resulting DataFrame.", "mimetype": "text/plain", "start_char_idx": 231364, "end_char_idx": 231953, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c64b9672-2727-499b-ba52-390a5cf914a4": {"__data__": {"id_": "c64b9672-2727-499b-ba52-390a5cf914a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6644d37-8b95-468b-b51c-05dca239d21b", "node_type": "1", "metadata": {}, "hash": "76dc7431dc49a692e98ece576097df06e26319e4a13845da0cbf152357c7558e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91d7b649-b55d-4ad0-b106-e58cf62417dc", "node_type": "1", "metadata": {}, "hash": "2169718964a763b0c77d1300e1ed0a4c30083e529a5927190946ebe36464994e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "this example, we start by defining a UDF called capitalize_udf using the udf() function. It \napplies a lambda function that changes the input string to upper case. We use the withColumn() \nmethod to apply the UDF capitalize_udf to the name column, creating a new column called \ncapitalized_name in the resulting DataFrame.\nFinally, we call the show() method to display the DataFrame with the transformed column.\nUDFs allow us to apply custom logic and transformations to columns in DataFrames, enabling us to \nhandle complex computations, perform string manipulations, or apply domain-specific operations \nthat are not available in Spark\u2019s built-in functions.\nIn this section, we explored the concept of UDFs in Spark SQL. We discussed the syntax for defining \nUDFs and demonstrated their usage through a code example.", "mimetype": "text/plain", "start_char_idx": 231631, "end_char_idx": 232449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91d7b649-b55d-4ad0-b106-e58cf62417dc": {"__data__": {"id_": "91d7b649-b55d-4ad0-b106-e58cf62417dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c64b9672-2727-499b-ba52-390a5cf914a4", "node_type": "1", "metadata": {}, "hash": "9431f54ac246edb50ca6e6ef7d9c4b05c2dd6828364a39eb3fab6ac28d056038", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bc7111c-8802-4ece-b0a5-039dd241f8f9", "node_type": "1", "metadata": {}, "hash": "31319b5e421e93cc221186a9c02f479056d170443b687f52515a8114193ab9de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "UDFs allow us to apply custom logic and transformations to columns in DataFrames, enabling us to \nhandle complex computations, perform string manipulations, or apply domain-specific operations \nthat are not available in Spark\u2019s built-in functions.\nIn this section, we explored the concept of UDFs in Spark SQL. We discussed the syntax for defining \nUDFs and demonstrated their usage through a code example. UDFs provide a powerful mechanism to \nextend Spark SQL\u2019s functionality by allowing us to apply custom transformations and computations to \nDataFrames or SQL queries. By incorporating UDFs into your Spark SQL workflows, you can handle \ncomplex data operations and tailor your data processing pipelines to meet specific requirements or \ndomain-specific needs.\nApplying a function\nPySpark also supports various UDFs and APIs to allow users to execute native Python functions.", "mimetype": "text/plain", "start_char_idx": 232043, "end_char_idx": 232922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bc7111c-8802-4ece-b0a5-039dd241f8f9": {"__data__": {"id_": "3bc7111c-8802-4ece-b0a5-039dd241f8f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91d7b649-b55d-4ad0-b106-e58cf62417dc", "node_type": "1", "metadata": {}, "hash": "2169718964a763b0c77d1300e1ed0a4c30083e529a5927190946ebe36464994e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4173dc84-ff0d-43f6-a4b3-d70560f276e5", "node_type": "1", "metadata": {}, "hash": "f03585054045614303ec19e9f5bc69ceaaa7602d6c6fc4a61912cd945d817cc3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "UDFs provide a powerful mechanism to \nextend Spark SQL\u2019s functionality by allowing us to apply custom transformations and computations to \nDataFrames or SQL queries. By incorporating UDFs into your Spark SQL workflows, you can handle \ncomplex data operations and tailor your data processing pipelines to meet specific requirements or \ndomain-specific needs.\nApplying a function\nPySpark also supports various UDFs and APIs to allow users to execute native Python functions. For \ninstance, the following example allows users to directly use the APIs in a pandas series within the \nPython native function:\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n# Simply plus one by using pandas Series.", "mimetype": "text/plain", "start_char_idx": 232450, "end_char_idx": 233232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4173dc84-ff0d-43f6-a4b3-d70560f276e5": {"__data__": {"id_": "4173dc84-ff0d-43f6-a4b3-d70560f276e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bc7111c-8802-4ece-b0a5-039dd241f8f9", "node_type": "1", "metadata": {}, "hash": "31319b5e421e93cc221186a9c02f479056d170443b687f52515a8114193ab9de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "589571bb-4c5b-45c2-bad7-aec9148a56ad", "node_type": "1", "metadata": {}, "hash": "0b5b8a37f5c6c59dea580931a110607469ef4665df0098701e594f51e851aaf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For \ninstance, the following example allows users to directly use the APIs in a pandas series within the \nPython native function:\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n# Simply plus one by using pandas Series.\n\nSQL Queries in Spark\n\nreturn series + 1\nsalary_data_with_id.select(pandas_plus_one(salary_data_with_\nid.Salary)).show()\nThe output will be the following:\n+-----------------------+\n|pandas_plus_one(Salary)|\n+-----------------------+\n|3501|\n|4001|\n|3501|\n|3001|\n|3501|\n|3001|\n|3501|\n|2201|\n+-----------------------+\nIn this example, we start by defining a pandas UDF called pandas_plus_one using the @\npandas_udf() function.", "mimetype": "text/plain", "start_char_idx": 232923, "end_char_idx": 233656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "589571bb-4c5b-45c2-bad7-aec9148a56ad": {"__data__": {"id_": "589571bb-4c5b-45c2-bad7-aec9148a56ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4173dc84-ff0d-43f6-a4b3-d70560f276e5", "node_type": "1", "metadata": {}, "hash": "f03585054045614303ec19e9f5bc69ceaaa7602d6c6fc4a61912cd945d817cc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd", "node_type": "1", "metadata": {}, "hash": "d37db395d9547506682330e3736636ee4f239fd61c9610b4d12f611185ade1c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We define this function so that it adds 1 to a pandas series. We use the \nalready created DataFrame named salary_data_with_id and call the pandas UDF to apply \nthis function to the salary column of the DataFrame.\nFinally, we call the show() method in the same statement, to display the DataFrame with the \ntransformed column.\nIn addition, UDFs can be registered and invoked in SQL out of the box.", "mimetype": "text/plain", "start_char_idx": 233657, "end_char_idx": 234053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd": {"__data__": {"id_": "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "589571bb-4c5b-45c2-bad7-aec9148a56ad", "node_type": "1", "metadata": {}, "hash": "0b5b8a37f5c6c59dea580931a110607469ef4665df0098701e594f51e851aaf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fca65d61-c2a9-4c40-8a5c-ba79928e4f93", "node_type": "1", "metadata": {}, "hash": "32c6ff59608fa0efcab9e1f8125c27ce21cb07cec2d2eef2499ed941f21c4fb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We define this function so that it adds 1 to a pandas series. We use the \nalready created DataFrame named salary_data_with_id and call the pandas UDF to apply \nthis function to the salary column of the DataFrame.\nFinally, we call the show() method in the same statement, to display the DataFrame with the \ntransformed column.\nIn addition, UDFs can be registered and invoked in SQL out of the box. The following is an example \nof how we can achieve this:\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\nreturn s + 1\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(Salary) FROM employees\").show()\n\nAdvanced Spark SQL operations\n\nThe output will be the following:\n+---------------+\n|add_one(Salary)|\n+---------------+\n|3501|\n|4001|\n|3501|\n|3001|\n|3501|\n|3001|\n|3501|\n|2201|\n+---------------+\nIn this example, we start by defining a pandas UDF called add_one by using the @pandas_udf() \nfunction.", "mimetype": "text/plain", "start_char_idx": 233657, "end_char_idx": 234581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fca65d61-c2a9-4c40-8a5c-ba79928e4f93": {"__data__": {"id_": "fca65d61-c2a9-4c40-8a5c-ba79928e4f93", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd", "node_type": "1", "metadata": {}, "hash": "d37db395d9547506682330e3736636ee4f239fd61c9610b4d12f611185ade1c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bfa5aa8-9161-4839-9fa4-3576431c7366", "node_type": "1", "metadata": {}, "hash": "dbf42e5ae15cca411e2092bb9cfec74e0a256bfc86460ac0f4ffb8468ca2e850", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We define this function so that it adds 1 to a pandas series. We then register this UDF for \nuse in SQL functions. We use the already created employees table and call the pandas UDF to apply \nthis function to the salary column of the table.\nFinally, we call the show() method in the same statement to display the results.\nIn this section, we explored the powerful capabilities of UDFs and how we can use them in \ncalculating aggregations.\nIn the next section, we will explore pivot and unpivot functions.\nWorking with complex data types \u2013 pivot and unpivot\nPivot and unpivot operations are used to transform data from a row-based format to a column-\nbased format and vice versa. In Spark SQL, these operations can be performed using the pivot and \nunpivot functions.\nThe pivot function is used to transform rows into columns.", "mimetype": "text/plain", "start_char_idx": 234582, "end_char_idx": 235407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4bfa5aa8-9161-4839-9fa4-3576431c7366": {"__data__": {"id_": "4bfa5aa8-9161-4839-9fa4-3576431c7366", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fca65d61-c2a9-4c40-8a5c-ba79928e4f93", "node_type": "1", "metadata": {}, "hash": "32c6ff59608fa0efcab9e1f8125c27ce21cb07cec2d2eef2499ed941f21c4fb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8530befd-42df-404d-95fa-a53deda26645", "node_type": "1", "metadata": {}, "hash": "ac9f39f4a1d476857be1a7490ce78828825412202bff6ae4cf1c5cef5ee6fa4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next section, we will explore pivot and unpivot functions.\nWorking with complex data types \u2013 pivot and unpivot\nPivot and unpivot operations are used to transform data from a row-based format to a column-\nbased format and vice versa. In Spark SQL, these operations can be performed using the pivot and \nunpivot functions.\nThe pivot function is used to transform rows into columns. It takes three arguments: the column to \nuse as the new column headers, the column to use as the new row headers and the columns to use \nas the values in the new table. The resulting table will have one row for each unique value in the row \nheader column, and one column for each unique value in the column header column.\nThe unpivot function is used to transform columns into rows. It takes two arguments: the columns \nto use as the new row headers, and the column to use as the values in the new table.", "mimetype": "text/plain", "start_char_idx": 235021, "end_char_idx": 235912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8530befd-42df-404d-95fa-a53deda26645": {"__data__": {"id_": "8530befd-42df-404d-95fa-a53deda26645", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bfa5aa8-9161-4839-9fa4-3576431c7366", "node_type": "1", "metadata": {}, "hash": "dbf42e5ae15cca411e2092bb9cfec74e0a256bfc86460ac0f4ffb8468ca2e850", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be2c5a17-8d41-4779-b0fb-8e5347df6094", "node_type": "1", "metadata": {}, "hash": "951ea19c4ceb7d7c0b12dbd29e7db4da8a6a5e3b75e9833cfb79b068b9b3cf43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting table will have one row for each unique value in the row \nheader column, and one column for each unique value in the column header column.\nThe unpivot function is used to transform columns into rows. It takes two arguments: the columns \nto use as the new row headers, and the column to use as the values in the new table. The resulting \ntable will have one row for each unique combination of values in the row header columns and one \ncolumn for the values.\n\nSQL Queries in Spark\n\nSome use cases for pivot and unpivot operations include the following:\n\u2022\t Converting data from a wide format to a long format or vice versa\n\u2022\t Aggregating data by multiple dimensions\n\u2022\t Creating summary tables or reports\n\u2022\t Preparing data for visualization or analysis\nOverall, pivot and unpivot operations are useful tools for transforming data in Spark SQL.", "mimetype": "text/plain", "start_char_idx": 235577, "end_char_idx": 236430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be2c5a17-8d41-4779-b0fb-8e5347df6094": {"__data__": {"id_": "be2c5a17-8d41-4779-b0fb-8e5347df6094", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8530befd-42df-404d-95fa-a53deda26645", "node_type": "1", "metadata": {}, "hash": "ac9f39f4a1d476857be1a7490ce78828825412202bff6ae4cf1c5cef5ee6fa4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b46d2d11-6158-428e-b752-95fd6dc22e75", "node_type": "1", "metadata": {}, "hash": "ba047c24f4a31949a8ff28eef8489dc6aa2d3d3e80a11c2f536be6f9db02dfda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SQL Queries in Spark\n\nSome use cases for pivot and unpivot operations include the following:\n\u2022\t Converting data from a wide format to a long format or vice versa\n\u2022\t Aggregating data by multiple dimensions\n\u2022\t Creating summary tables or reports\n\u2022\t Preparing data for visualization or analysis\nOverall, pivot and unpivot operations are useful tools for transforming data in Spark SQL.\nSummary\nIn this chapter, we explored the process of transforming and analyzing data in Spark SQL. We learned \nhow to filter and manipulate loaded data, save the transformed data as a table, and execute SQL queries \nto extract meaningful insights. By following the Python code examples provided, you can apply these \ntechniques to your own datasets, unlocking the potential of Spark SQL for data analysis and exploration.\nAfter covering those topics, we explored the powerful capabilities of window functions in Spark SQL \nfor advanced analytics.", "mimetype": "text/plain", "start_char_idx": 236049, "end_char_idx": 236976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b46d2d11-6158-428e-b752-95fd6dc22e75": {"__data__": {"id_": "b46d2d11-6158-428e-b752-95fd6dc22e75", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be2c5a17-8d41-4779-b0fb-8e5347df6094", "node_type": "1", "metadata": {}, "hash": "951ea19c4ceb7d7c0b12dbd29e7db4da8a6a5e3b75e9833cfb79b068b9b3cf43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3889a560-080a-4373-bcb8-e2e7eb583a60", "node_type": "1", "metadata": {}, "hash": "e9ee62a54b1f9175344637369919f0b30e208e151df925e5fa2be794fd0be048", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We learned \nhow to filter and manipulate loaded data, save the transformed data as a table, and execute SQL queries \nto extract meaningful insights. By following the Python code examples provided, you can apply these \ntechniques to your own datasets, unlocking the potential of Spark SQL for data analysis and exploration.\nAfter covering those topics, we explored the powerful capabilities of window functions in Spark SQL \nfor advanced analytics. We discussed the syntax and usage of window functions, allowing us to perform \ncomplex calculations and aggregations within defined partitions and windows. By incorporating window \nfunctions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding \nof your data for advanced analytical operations.\nWe then discussed some ways to use UDFs in Spark and how they can be useful in complex aggregations \nover multiple rows and columns of a DataFrame.\nFinally, we covered some of the ways to use pivot and unpivot in Spark SQL.", "mimetype": "text/plain", "start_char_idx": 236529, "end_char_idx": 237530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3889a560-080a-4373-bcb8-e2e7eb583a60": {"__data__": {"id_": "3889a560-080a-4373-bcb8-e2e7eb583a60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b46d2d11-6158-428e-b752-95fd6dc22e75", "node_type": "1", "metadata": {}, "hash": "ba047c24f4a31949a8ff28eef8489dc6aa2d3d3e80a11c2f536be6f9db02dfda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08209b0f-2b60-446a-bead-b92bb5cc7dba", "node_type": "1", "metadata": {}, "hash": "109177f31092038538de1e16de9fc8615691a55f21b07c0eb7d90d111802d917", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By incorporating window \nfunctions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding \nof your data for advanced analytical operations.\nWe then discussed some ways to use UDFs in Spark and how they can be useful in complex aggregations \nover multiple rows and columns of a DataFrame.\nFinally, we covered some of the ways to use pivot and unpivot in Spark SQL.\nPart 4: Spark Applications\nIn this part, we will cover Spark\u2019s Structured Streaming, focusing on real-time data processing with \nconcepts such as event time processing, watermarking, triggers, and output modes. Practical examples \nwill illustrate building and deploying streaming applications using Structured Streaming. Additionally, \nwe will delve into Spark ML, Spark\u2019s machine learning library, exploring supervised and unsupervised \ntechniques, model building, evaluation, and hyperparameter tuning across various algorithms.", "mimetype": "text/plain", "start_char_idx": 237133, "end_char_idx": 238061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08209b0f-2b60-446a-bead-b92bb5cc7dba": {"__data__": {"id_": "08209b0f-2b60-446a-bead-b92bb5cc7dba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3889a560-080a-4373-bcb8-e2e7eb583a60", "node_type": "1", "metadata": {}, "hash": "e9ee62a54b1f9175344637369919f0b30e208e151df925e5fa2be794fd0be048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0257d759-b104-4f4e-a2c4-4ce6d280b37d", "node_type": "1", "metadata": {}, "hash": "fc910d261f38039b96cad43d977928ef4548729108ed83aaa37d3e65442f54a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Practical examples \nwill illustrate building and deploying streaming applications using Structured Streaming. Additionally, \nwe will delve into Spark ML, Spark\u2019s machine learning library, exploring supervised and unsupervised \ntechniques, model building, evaluation, and hyperparameter tuning across various algorithms. Practical \nexamples will demonstrate Spark ML's application in real-world machine learning tasks, crucial in \ncontemporary data science. While not included in the Spark certification exam, understanding these \nconcepts is essential in modern data engineering.\nThis part has the following chapters:\n\u2022\t Chapter 7, Structured Streaming in Spark\n\u2022\t Chapter 8, Machine Learning with Spark ML\n\n\nStructured Streaming in Spark\nThe world of data processing has evolved rapidly as data volume and data velocity increase every \nday. With that, the need to analyze and derive insights from real-time data is becoming increasingly \ncrucial.", "mimetype": "text/plain", "start_char_idx": 237742, "end_char_idx": 238689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0257d759-b104-4f4e-a2c4-4ce6d280b37d": {"__data__": {"id_": "0257d759-b104-4f4e-a2c4-4ce6d280b37d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08209b0f-2b60-446a-bead-b92bb5cc7dba", "node_type": "1", "metadata": {}, "hash": "109177f31092038538de1e16de9fc8615691a55f21b07c0eb7d90d111802d917", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d21b5741-3f9a-48e2-8e90-714b3be77f1f", "node_type": "1", "metadata": {}, "hash": "664e87b728adc0192fbb6b8668301dc4b0d012739fce645291816911affe176e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This part has the following chapters:\n\u2022\t Chapter 7, Structured Streaming in Spark\n\u2022\t Chapter 8, Machine Learning with Spark ML\n\n\nStructured Streaming in Spark\nThe world of data processing has evolved rapidly as data volume and data velocity increase every \nday. With that, the need to analyze and derive insights from real-time data is becoming increasingly \ncrucial. Structured Streaming, a component of Apache Spark, has emerged as a powerful framework \nto process and analyze data streams in real time. This chapter delves into the realm of Structured \nStreaming, exploring its capabilities, features, and real-world applications.", "mimetype": "text/plain", "start_char_idx": 238322, "end_char_idx": 238955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d21b5741-3f9a-48e2-8e90-714b3be77f1f": {"__data__": {"id_": "d21b5741-3f9a-48e2-8e90-714b3be77f1f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0257d759-b104-4f4e-a2c4-4ce6d280b37d", "node_type": "1", "metadata": {}, "hash": "fc910d261f38039b96cad43d977928ef4548729108ed83aaa37d3e65442f54a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7dcbfea-d2ff-46b4-bf27-c28c877383e0", "node_type": "1", "metadata": {}, "hash": "5b2f7f004855bf7160eda28524f8aca255a2ee4fa9b0fa0520fc426c3b850cc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With that, the need to analyze and derive insights from real-time data is becoming increasingly \ncrucial. Structured Streaming, a component of Apache Spark, has emerged as a powerful framework \nto process and analyze data streams in real time. This chapter delves into the realm of Structured \nStreaming, exploring its capabilities, features, and real-world applications.\nIn this chapter, we will cover the following topics:\n\u2022\t Real-time data processing\n\u2022\t The fundamentals of streaming\n\u2022\t Streaming architectures\n\u2022\t Spark Streaming\n\u2022\t Structured Streaming\n\u2022\t Streaming sources and sinks\n\u2022\t Advanced topics in Structured Streaming\n\u2022\t Joins in Structured Streaming\nBy the end of this chapter, you will understand Spark Streaming and the power of real-time data insights.\nWe will start by looking at what real-time data processing means.\nReal-time data processing\nReal-time data processing has become increasingly critical in today\u2019s fast-paced and data-driven \nworld.", "mimetype": "text/plain", "start_char_idx": 238584, "end_char_idx": 239550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7dcbfea-d2ff-46b4-bf27-c28c877383e0": {"__data__": {"id_": "c7dcbfea-d2ff-46b4-bf27-c28c877383e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d21b5741-3f9a-48e2-8e90-714b3be77f1f", "node_type": "1", "metadata": {}, "hash": "664e87b728adc0192fbb6b8668301dc4b0d012739fce645291816911affe176e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "204c05cb-54d6-4537-a277-abe74093ca04", "node_type": "1", "metadata": {}, "hash": "e0efa08ed99de2f8c3271f223c80c34f3934792bdf3833602952f6f84f834522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will start by looking at what real-time data processing means.\nReal-time data processing\nReal-time data processing has become increasingly critical in today\u2019s fast-paced and data-driven \nworld. Organizations need to analyze and derive insights from data as it arrives, enabling them to \nmake timely decisions and take immediate action. Spark Streaming, a powerful component of Apache \nSpark, addresses this need by providing a scalable and fault-tolerant framework to process real-time \ndata streams.\n\nStructured Streaming in Spark\n\nReal-time data processing has gained immense importance in various industries, ranging from finance \nand e-commerce to the Internet of Things (IoT) and social media. Traditional batch processing \napproaches, while suitable for many scenarios, fall short when immediate insights and actions are \nrequired. Real-time data processing fills this gap by enabling the analysis and processing of data as it \narrives, allowing organizations to make timely decisions and respond quickly to changing conditions.", "mimetype": "text/plain", "start_char_idx": 239354, "end_char_idx": 240391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "204c05cb-54d6-4537-a277-abe74093ca04": {"__data__": {"id_": "204c05cb-54d6-4537-a277-abe74093ca04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7dcbfea-d2ff-46b4-bf27-c28c877383e0", "node_type": "1", "metadata": {}, "hash": "5b2f7f004855bf7160eda28524f8aca255a2ee4fa9b0fa0520fc426c3b850cc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109", "node_type": "1", "metadata": {}, "hash": "95f29480e8776156569f1dd7a1ab65e1a7484fe605f2bf64516a16516d5fe991", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Traditional batch processing \napproaches, while suitable for many scenarios, fall short when immediate insights and actions are \nrequired. Real-time data processing fills this gap by enabling the analysis and processing of data as it \narrives, allowing organizations to make timely decisions and respond quickly to changing conditions.\nReal-time data processing involves the continuous ingestion, processing, and analysis of streaming \ndata. Unlike batch processing, which operates on static datasets, real-time data processing systems \nhandle data that is generated and updated in real time. This data can be sourced from various channels, \nincluding sensors, logs, social media feeds, and financial transactions.\nThe key characteristics of real-time data processing are as follows:\n\u2022\t Low latency: Real-time data processing aims to minimize the time delay between data generation \nand processing. It requires fast and efficient processing capabilities to provide near-instantaneous \ninsights and responses.", "mimetype": "text/plain", "start_char_idx": 240056, "end_char_idx": 241064, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109": {"__data__": {"id_": "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "204c05cb-54d6-4537-a277-abe74093ca04", "node_type": "1", "metadata": {}, "hash": "e0efa08ed99de2f8c3271f223c80c34f3934792bdf3833602952f6f84f834522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d19bacc-0a1b-4583-82d4-d3296c92f966", "node_type": "1", "metadata": {}, "hash": "beea223f937afb8ad7cae98e83dbee129b3827a117e90a172f0b58fdddf7dcb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This data can be sourced from various channels, \nincluding sensors, logs, social media feeds, and financial transactions.\nThe key characteristics of real-time data processing are as follows:\n\u2022\t Low latency: Real-time data processing aims to minimize the time delay between data generation \nand processing. It requires fast and efficient processing capabilities to provide near-instantaneous \ninsights and responses.\n\u2022\t Scalability: Real-time data processing systems must be able to handle high-volume and high-\nvelocity data streams. The ability to scale horizontally and distribute processing across multiple \nnodes is essential to accommodate the increasing data load.\n\u2022\t Fault tolerance: Given the continuous nature of streaming data, real-time processing systems \nneed to be resilient to failures. They should have mechanisms in place to recover from failures \nand ensure uninterrupted processing.\n\u2022\t A streaming data model: Real-time data processing systems operate on streaming data, which is \nan unbounded sequence of events or records.", "mimetype": "text/plain", "start_char_idx": 240649, "end_char_idx": 241692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d19bacc-0a1b-4583-82d4-d3296c92f966": {"__data__": {"id_": "9d19bacc-0a1b-4583-82d4-d3296c92f966", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109", "node_type": "1", "metadata": {}, "hash": "95f29480e8776156569f1dd7a1ab65e1a7484fe605f2bf64516a16516d5fe991", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4826f2d-db11-476d-8a71-cb5a201400b9", "node_type": "1", "metadata": {}, "hash": "ff2dbdf0d06724dab2351d23b83bf0e53ce00b936fe198241619f623fb21a1c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Fault tolerance: Given the continuous nature of streaming data, real-time processing systems \nneed to be resilient to failures. They should have mechanisms in place to recover from failures \nand ensure uninterrupted processing.\n\u2022\t A streaming data model: Real-time data processing systems operate on streaming data, which is \nan unbounded sequence of events or records. Streaming data models are designed to handle the \ncontinuous flow of data and provide mechanisms for event-time and window-based computations.\nThese characteristics of real-time data processing lead to several advantages, including the following:\n\u2022\t A rapid response: Real-time processing enables organizations to respond quickly to changing \nconditions, events, or opportunities. It allows for timely actions, such as fraud detection, anomaly \ndetection, real-time monitoring, and alerting.\n\u2022\t Personalization: Real-time processing enables personalized experiences by analyzing and \nacting on user behavior in real time.", "mimetype": "text/plain", "start_char_idx": 241320, "end_char_idx": 242314, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4826f2d-db11-476d-8a71-cb5a201400b9": {"__data__": {"id_": "e4826f2d-db11-476d-8a71-cb5a201400b9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d19bacc-0a1b-4583-82d4-d3296c92f966", "node_type": "1", "metadata": {}, "hash": "beea223f937afb8ad7cae98e83dbee129b3827a117e90a172f0b58fdddf7dcb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16694feb-cd8d-4385-ba9b-139f317924ce", "node_type": "1", "metadata": {}, "hash": "34ce1809f1d2ead5dcb4efa178c2214b8865e0987a60b1d0acac4c59d30ae98c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These characteristics of real-time data processing lead to several advantages, including the following:\n\u2022\t A rapid response: Real-time processing enables organizations to respond quickly to changing \nconditions, events, or opportunities. It allows for timely actions, such as fraud detection, anomaly \ndetection, real-time monitoring, and alerting.\n\u2022\t Personalization: Real-time processing enables personalized experiences by analyzing and \nacting on user behavior in real time. It powers real-time recommendations, dynamic pricing, \ntargeted advertising, and content personalization.\n\u2022\t Operational efficiency: Real-time processing provides insights into operational processes, \nallowing organizations to optimize their operations, identify bottlenecks, and improve efficiency \nin real time. It facilitates predictive maintenance, supply chain optimization, and real-time \nresource allocation.\n\u2022\t Situational awareness: Real-time data processing helps organizations gain situational awareness \nby continuously analyzing and aggregating data from various sources.", "mimetype": "text/plain", "start_char_idx": 241836, "end_char_idx": 242899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16694feb-cd8d-4385-ba9b-139f317924ce": {"__data__": {"id_": "16694feb-cd8d-4385-ba9b-139f317924ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4826f2d-db11-476d-8a71-cb5a201400b9", "node_type": "1", "metadata": {}, "hash": "ff2dbdf0d06724dab2351d23b83bf0e53ce00b936fe198241619f623fb21a1c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1044b065-3193-47d5-85e8-3bc604b0912d", "node_type": "1", "metadata": {}, "hash": "2c02609097e8cf0e014549d99a9cdb5f4c0a0eeb8849f3d443453d1395db542a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Operational efficiency: Real-time processing provides insights into operational processes, \nallowing organizations to optimize their operations, identify bottlenecks, and improve efficiency \nin real time. It facilitates predictive maintenance, supply chain optimization, and real-time \nresource allocation.\n\u2022\t Situational awareness: Real-time data processing helps organizations gain situational awareness \nby continuously analyzing and aggregating data from various sources. It enables real-time \nanalytics, monitoring, and decision making in domains such as cybersecurity, financial markets, \nand emergency response systems.\n\nWhat is streaming?\n\nIn summary, real-time streaming involves the continuous transmission and processing of data, enabling \nimmediate insights and rapid decision-making. It has a wide range of applications across different \nindustries and utilizes various technologies to facilitate efficient and reliable streaming.\nIn the next section, we will explore the basics of streaming and understand how streaming is useful \nfor real-time operations.\nWhat is streaming?", "mimetype": "text/plain", "start_char_idx": 242421, "end_char_idx": 243513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1044b065-3193-47d5-85e8-3bc604b0912d": {"__data__": {"id_": "1044b065-3193-47d5-85e8-3bc604b0912d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16694feb-cd8d-4385-ba9b-139f317924ce", "node_type": "1", "metadata": {}, "hash": "34ce1809f1d2ead5dcb4efa178c2214b8865e0987a60b1d0acac4c59d30ae98c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3143592d-66cc-413e-8863-96948df4d304", "node_type": "1", "metadata": {}, "hash": "f3607f08f9aba5dbf5e525900526a987624a403eebcbaabd98a1ab8b4fa359ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What is streaming?\n\nIn summary, real-time streaming involves the continuous transmission and processing of data, enabling \nimmediate insights and rapid decision-making. It has a wide range of applications across different \nindustries and utilizes various technologies to facilitate efficient and reliable streaming.\nIn the next section, we will explore the basics of streaming and understand how streaming is useful \nfor real-time operations.\nWhat is streaming?\nStreaming refers to the continuous and real-time processing of data as it is generated or received. \nUnlike batch processing, where data is processed in chunks or batches at fixed intervals, streaming \nenables the processing of data continuously and incrementally. It allows applications to ingest, process, \nand analyze data in real time, enabling timely decision-making and immediate responses to events.\nDifferent types of streaming architectures are available to handle streaming data. We will look at \nthem next.\nStreaming architectures\nStreaming architectures are designed to handle the continuous and high-velocity nature of streaming \ndata.", "mimetype": "text/plain", "start_char_idx": 243052, "end_char_idx": 244162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3143592d-66cc-413e-8863-96948df4d304": {"__data__": {"id_": "3143592d-66cc-413e-8863-96948df4d304", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1044b065-3193-47d5-85e8-3bc604b0912d", "node_type": "1", "metadata": {}, "hash": "2c02609097e8cf0e014549d99a9cdb5f4c0a0eeb8849f3d443453d1395db542a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a", "node_type": "1", "metadata": {}, "hash": "b003781d3b12e3ca59ade0fe5b5be1523173f55e024abb524cb4ca9b51b500d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It allows applications to ingest, process, \nand analyze data in real time, enabling timely decision-making and immediate responses to events.\nDifferent types of streaming architectures are available to handle streaming data. We will look at \nthem next.\nStreaming architectures\nStreaming architectures are designed to handle the continuous and high-velocity nature of streaming \ndata. They typically consist of three key components:\n\u2022\t Streaming sources: These are the origins of the streaming data, such as IoT devices, sensors, \nlogs, social media feeds, or messaging systems. Streaming sources continuously produce and \nemit data in real time.\n\u2022\t A streaming processing engine: The streaming processing engine is responsible for ingesting, \nprocessing, and analyzing streaming data. It provides the necessary infrastructure and \ncomputational capabilities to handle the continuous and incremental nature of streaming data.\n\u2022\t Streaming sinks: Streaming sinks are destinations where the processed data is stored, visualized, \nor acted upon.", "mimetype": "text/plain", "start_char_idx": 243779, "end_char_idx": 244820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a": {"__data__": {"id_": "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3143592d-66cc-413e-8863-96948df4d304", "node_type": "1", "metadata": {}, "hash": "f3607f08f9aba5dbf5e525900526a987624a403eebcbaabd98a1ab8b4fa359ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536", "node_type": "1", "metadata": {}, "hash": "c581103fc0f57ffd57c08e93e89b39a17bc94f2fbf026fe19bb2490eef3d52a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Streaming sources continuously produce and \nemit data in real time.\n\u2022\t A streaming processing engine: The streaming processing engine is responsible for ingesting, \nprocessing, and analyzing streaming data. It provides the necessary infrastructure and \ncomputational capabilities to handle the continuous and incremental nature of streaming data.\n\u2022\t Streaming sinks: Streaming sinks are destinations where the processed data is stored, visualized, \nor acted upon. They can be databases, data warehouses, dashboards, or external systems that \nconsume the processed data.\nThere are various streaming architectures, including the following:\n\u2022\t Event-driven architecture: In event-driven architecture, events are generated by sources and \nthen captured and processed by the engine, leading to immediate reactions and triggering actions \nor updates in real time. This framework facilitates real-time event processing, supports the \ndevelopment of event-driven microservices, and contributes to the creation of reactive systems.", "mimetype": "text/plain", "start_char_idx": 244357, "end_char_idx": 245379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536": {"__data__": {"id_": "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a", "node_type": "1", "metadata": {}, "hash": "b003781d3b12e3ca59ade0fe5b5be1523173f55e024abb524cb4ca9b51b500d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403", "node_type": "1", "metadata": {}, "hash": "8609a5008c8fcb25a55bd0b27ea95aa9d976873b90495da86d09fd70c73c0d96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are various streaming architectures, including the following:\n\u2022\t Event-driven architecture: In event-driven architecture, events are generated by sources and \nthen captured and processed by the engine, leading to immediate reactions and triggering actions \nor updates in real time. This framework facilitates real-time event processing, supports the \ndevelopment of event-driven microservices, and contributes to the creation of reactive systems.\nEvent-driven architecture\u2019s advantages lie in its ability to provide responsiveness, scalability, \nand flexibility. This allows for prompt reactions to events as they unfold, fostering agility in \nsystem responses.\n\nStructured Streaming in Spark\n\n\u2022\t Lambda architecture: The lambda architecture seamlessly integrates batch and stream processing \nto effectively manage both historical and real-time data. This involves parallel processing of \ndata streams to enable real-time analysis, coupled with offline batch processing for in-depth \nand comprehensive analytics.\nThis approach is particularly well-suited for applications that require a balance of real-time \ninsights and thorough historical analysis.", "mimetype": "text/plain", "start_char_idx": 244927, "end_char_idx": 246084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403": {"__data__": {"id_": "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536", "node_type": "1", "metadata": {}, "hash": "c581103fc0f57ffd57c08e93e89b39a17bc94f2fbf026fe19bb2490eef3d52a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84275586-19f7-4104-953b-b90e13b18534", "node_type": "1", "metadata": {}, "hash": "ad4da5868e2c98cbc8626e03068c93dfb727cc92d25b1a96239ea978c30117c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming in Spark\n\n\u2022\t Lambda architecture: The lambda architecture seamlessly integrates batch and stream processing \nto effectively manage both historical and real-time data. This involves parallel processing of \ndata streams to enable real-time analysis, coupled with offline batch processing for in-depth \nand comprehensive analytics.\nThis approach is particularly well-suited for applications that require a balance of real-time \ninsights and thorough historical analysis. The lambda architecture\u2019s strengths lie in its provision \nof fault tolerance, scalability, and capacity to handle substantial data volumes. This is achieved \nby harnessing the combined power of both the batch and stream processing techniques.\n\u2022\t Unified streaming architecture: Unified streaming architectures, such as Apache Spark\u2019s \nStructured Streaming, aim to provide a unified API and processing model for both batch and \nstream processing. They simplify the development and deployment of real-time applications by \nabstracting away the complexities of managing separate batch and stream processing systems.", "mimetype": "text/plain", "start_char_idx": 245596, "end_char_idx": 246697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84275586-19f7-4104-953b-b90e13b18534": {"__data__": {"id_": "84275586-19f7-4104-953b-b90e13b18534", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403", "node_type": "1", "metadata": {}, "hash": "8609a5008c8fcb25a55bd0b27ea95aa9d976873b90495da86d09fd70c73c0d96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1a3cb02-42df-474d-897f-d333d0e9c6c3", "node_type": "1", "metadata": {}, "hash": "3466bfbf29e50394783e351812aac043430c49de53b2687cd6b4886027a0c554", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is achieved \nby harnessing the combined power of both the batch and stream processing techniques.\n\u2022\t Unified streaming architecture: Unified streaming architectures, such as Apache Spark\u2019s \nStructured Streaming, aim to provide a unified API and processing model for both batch and \nstream processing. They simplify the development and deployment of real-time applications by \nabstracting away the complexities of managing separate batch and stream processing systems.\nThis architecture abstracts complexities by offering a simplified approach to developing and \ndeploying real-time applications. This is ideal for scenarios where simplicity and ease of development \nare crucial, allowing developers to focus more on business logic than intricate technicalities.\nThe advantages are that it simplifies development, reduces operational overhead, and ensures \nconsistency across batch and stream processing.\nThese architectures cater to different needs based on the specific requirements of a given application.", "mimetype": "text/plain", "start_char_idx": 246225, "end_char_idx": 247237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1a3cb02-42df-474d-897f-d333d0e9c6c3": {"__data__": {"id_": "e1a3cb02-42df-474d-897f-d333d0e9c6c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84275586-19f7-4104-953b-b90e13b18534", "node_type": "1", "metadata": {}, "hash": "ad4da5868e2c98cbc8626e03068c93dfb727cc92d25b1a96239ea978c30117c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aff1d461-2af5-4425-ba68-ac4064a4c753", "node_type": "1", "metadata": {}, "hash": "d56462918c1bc20d2bd235f639f8bf9694b10bd2541d99fa58739977db1b7fde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This architecture abstracts complexities by offering a simplified approach to developing and \ndeploying real-time applications. This is ideal for scenarios where simplicity and ease of development \nare crucial, allowing developers to focus more on business logic than intricate technicalities.\nThe advantages are that it simplifies development, reduces operational overhead, and ensures \nconsistency across batch and stream processing.\nThese architectures cater to different needs based on the specific requirements of a given application. \nEvent-driven is ideal for real-time reactions, lambda for a balance between real-time and historical \ndata, and unified streaming for a streamlined, unified approach to both batch and stream processing. \nEach approach has its strengths and trade-offs, making them suitable for various scenarios based on \nthe specific needs of a system.\nIn the following sections, we will delve into the specifics of Structured Streaming, its key concepts, and \nhow it compares to Spark Streaming.", "mimetype": "text/plain", "start_char_idx": 246698, "end_char_idx": 247719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aff1d461-2af5-4425-ba68-ac4064a4c753": {"__data__": {"id_": "aff1d461-2af5-4425-ba68-ac4064a4c753", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1a3cb02-42df-474d-897f-d333d0e9c6c3", "node_type": "1", "metadata": {}, "hash": "3466bfbf29e50394783e351812aac043430c49de53b2687cd6b4886027a0c554", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7", "node_type": "1", "metadata": {}, "hash": "1c9ce7fcde6c354c216a2a6b78f4e7ab884062db98bc836d3e98aa5c9b769b14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each approach has its strengths and trade-offs, making them suitable for various scenarios based on \nthe specific needs of a system.\nIn the following sections, we will delve into the specifics of Structured Streaming, its key concepts, and \nhow it compares to Spark Streaming. We will also explore stateless and stateful streaming, streaming \nsources, and sinks, providing code examples and practical illustrations to enhance understanding.\nIntroducing Spark Streaming\nAs you\u2019ve seen so far, Spark Streaming is a powerful real-time data processing framework built on \nApache Spark. It extends the capabilities of the Spark engine to support high-throughput, fault-tolerant, \nand scalable stream processing. Spark Streaming enables developers to process real-time data streams \nusing the same programming model as batch processing, making it easy to transition from batch to \nstreaming workloads.", "mimetype": "text/plain", "start_char_idx": 247443, "end_char_idx": 248338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7": {"__data__": {"id_": "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aff1d461-2af5-4425-ba68-ac4064a4c753", "node_type": "1", "metadata": {}, "hash": "d56462918c1bc20d2bd235f639f8bf9694b10bd2541d99fa58739977db1b7fde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "113fbf70-a4fb-4251-b7a8-1ad3a836d069", "node_type": "1", "metadata": {}, "hash": "f4cacbe6308f7084385cf6c4a7c3fee56b1a048d7df8a9ca577f42e0540e625a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introducing Spark Streaming\nAs you\u2019ve seen so far, Spark Streaming is a powerful real-time data processing framework built on \nApache Spark. It extends the capabilities of the Spark engine to support high-throughput, fault-tolerant, \nand scalable stream processing. Spark Streaming enables developers to process real-time data streams \nusing the same programming model as batch processing, making it easy to transition from batch to \nstreaming workloads.\nAt its core, Spark Streaming divides the real-time data stream into small batches or micro-batches, \nwhich are then processed using Spark\u2019s distributed computing capabilities. Each micro-batch is \ntreated as a Resilient Distributed Dataset (RDD), Spark\u2019s fundamental abstraction for distributed \ndata processing. This approach allows developers to leverage Spark\u2019s extensive ecosystem of libraries, \nsuch as Spark SQL, MLlib, and GraphX, for real-time analytics and machine learning tasks.", "mimetype": "text/plain", "start_char_idx": 247884, "end_char_idx": 248828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "113fbf70-a4fb-4251-b7a8-1ad3a836d069": {"__data__": {"id_": "113fbf70-a4fb-4251-b7a8-1ad3a836d069", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7", "node_type": "1", "metadata": {}, "hash": "1c9ce7fcde6c354c216a2a6b78f4e7ab884062db98bc836d3e98aa5c9b769b14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "776336c2-5f9b-481e-b1be-d9d8ba4222cf", "node_type": "1", "metadata": {}, "hash": "36a7afe0398ff464371ea1878033c77e5e1beb8a06408273664fd20eaa26e603", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each micro-batch is \ntreated as a Resilient Distributed Dataset (RDD), Spark\u2019s fundamental abstraction for distributed \ndata processing. This approach allows developers to leverage Spark\u2019s extensive ecosystem of libraries, \nsuch as Spark SQL, MLlib, and GraphX, for real-time analytics and machine learning tasks.\n\nIntroducing Spark Streaming\n\nExploring the architecture of Spark Streaming\nSpark Streaming follows a master-worker architecture, where the driver program serves as the master \nand the worker nodes process the data. The high-level architecture consists of the following components:\n\u2022\t The driver program: The driver program runs the main application and manages the overall \nexecution of the Spark Streaming application. It divides the data stream into batches, schedules \ntasks on the worker nodes, and coordinates the processing.\n\u2022\t Receivers: Receivers are responsible for connecting to the streaming data sources and receiving \nthe data.", "mimetype": "text/plain", "start_char_idx": 248515, "end_char_idx": 249470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "776336c2-5f9b-481e-b1be-d9d8ba4222cf": {"__data__": {"id_": "776336c2-5f9b-481e-b1be-d9d8ba4222cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "113fbf70-a4fb-4251-b7a8-1ad3a836d069", "node_type": "1", "metadata": {}, "hash": "f4cacbe6308f7084385cf6c4a7c3fee56b1a048d7df8a9ca577f42e0540e625a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dccf6ee4-ac1c-40ce-b989-bcb736570a77", "node_type": "1", "metadata": {}, "hash": "6ccb73f22f9390f6296d198ed12b8a45166219210fc3362f90542c98f1b0b05f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The high-level architecture consists of the following components:\n\u2022\t The driver program: The driver program runs the main application and manages the overall \nexecution of the Spark Streaming application. It divides the data stream into batches, schedules \ntasks on the worker nodes, and coordinates the processing.\n\u2022\t Receivers: Receivers are responsible for connecting to the streaming data sources and receiving \nthe data. They run on worker nodes and pull the data from sources such as Kafka, Flume, or \nTCP sockets. The received data is then stored in the memory of the worker nodes.\n\u2022\t Discretized Stream (DStream): DStream is the basic abstraction in Spark Streaming. It \nrepresents a continuous stream of data divided into small, discrete RDDs. DStream provides a \nhigh-level API to perform transformations and actions on the streaming data.\n\u2022\t Transformations and actions: Spark Streaming supports a wide range of transformations and \nactions, similar to batch processing.", "mimetype": "text/plain", "start_char_idx": 249045, "end_char_idx": 250026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dccf6ee4-ac1c-40ce-b989-bcb736570a77": {"__data__": {"id_": "dccf6ee4-ac1c-40ce-b989-bcb736570a77", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "776336c2-5f9b-481e-b1be-d9d8ba4222cf", "node_type": "1", "metadata": {}, "hash": "36a7afe0398ff464371ea1878033c77e5e1beb8a06408273664fd20eaa26e603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0efd2111-1824-46e8-95fa-3e20e1d70250", "node_type": "1", "metadata": {}, "hash": "61df5048eb7fd0fd2f02164dc7791f61df5aa30f79b6f2550e74544c294cf2cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Discretized Stream (DStream): DStream is the basic abstraction in Spark Streaming. It \nrepresents a continuous stream of data divided into small, discrete RDDs. DStream provides a \nhigh-level API to perform transformations and actions on the streaming data.\n\u2022\t Transformations and actions: Spark Streaming supports a wide range of transformations and \nactions, similar to batch processing. Transformations, such as map, filter, and reduceByKey, \nare applied to each RDD in the DStream. Actions, such as count, saveAsTextFiles, \nand foreachRDD, trigger the execution of the streaming computation and produce results.\n\u2022\t Output operations: Output operations allow the processed data to be written to external \nsystems or storage. Spark Streaming supports various output operations, such as writing to \nfiles, databases, or sending to dashboards for visualization.", "mimetype": "text/plain", "start_char_idx": 249634, "end_char_idx": 250498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0efd2111-1824-46e8-95fa-3e20e1d70250": {"__data__": {"id_": "0efd2111-1824-46e8-95fa-3e20e1d70250", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dccf6ee4-ac1c-40ce-b989-bcb736570a77", "node_type": "1", "metadata": {}, "hash": "6ccb73f22f9390f6296d198ed12b8a45166219210fc3362f90542c98f1b0b05f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977", "node_type": "1", "metadata": {}, "hash": "cceaaa7819e7101c241571dd97039f36dfc9e49ffd1a22a336a059c8c9f3988c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Actions, such as count, saveAsTextFiles, \nand foreachRDD, trigger the execution of the streaming computation and produce results.\n\u2022\t Output operations: Output operations allow the processed data to be written to external \nsystems or storage. Spark Streaming supports various output operations, such as writing to \nfiles, databases, or sending to dashboards for visualization.\nKey concepts\nTo effectively use Spark Streaming, it is important to understand some key concepts:\n\u2022\t DStreams: As mentioned earlier, DStreams represent the continuous stream of data in Spark \nStreaming. They are a sequence of RDDs, where each RDD contains data from a specific time \ninterval. DStreams support various transformations and actions, enabling complex computations \non the stream.\n\u2022\t Window operations: Window operations allow you to apply transformations on a sliding \nwindow of data in the stream.", "mimetype": "text/plain", "start_char_idx": 250123, "end_char_idx": 251010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977": {"__data__": {"id_": "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0efd2111-1824-46e8-95fa-3e20e1d70250", "node_type": "1", "metadata": {}, "hash": "61df5048eb7fd0fd2f02164dc7791f61df5aa30f79b6f2550e74544c294cf2cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40a81e57-e9c3-4560-b0bf-a6f10222a132", "node_type": "1", "metadata": {}, "hash": "f21a51adc4795c58c4783250525e3bb02dc75a69af0b71fc4c71ef6606b44f71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They are a sequence of RDDs, where each RDD contains data from a specific time \ninterval. DStreams support various transformations and actions, enabling complex computations \non the stream.\n\u2022\t Window operations: Window operations allow you to apply transformations on a sliding \nwindow of data in the stream. It enables computations over a fixed window size or based on \ntime durations, enabling tasks such as windowed aggregations or time-based joins.\n\u2022\t Stateful operations: Spark Streaming gives you the ability to maintain stateful information \nacross batches. It enables operations that require maintaining and updating state, such as \ncumulative counts.\n\u2022\t Checkpointing: Checkpointing is a crucial mechanism in Spark Streaming to ensure fault \ntolerance and recovery. It periodically saves the metadata about the streaming application, \nincluding the configuration, DStream operations, and the processed data. It enables the recovery \nof the application if there are failures.", "mimetype": "text/plain", "start_char_idx": 250702, "end_char_idx": 251685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40a81e57-e9c3-4560-b0bf-a6f10222a132": {"__data__": {"id_": "40a81e57-e9c3-4560-b0bf-a6f10222a132", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977", "node_type": "1", "metadata": {}, "hash": "cceaaa7819e7101c241571dd97039f36dfc9e49ffd1a22a336a059c8c9f3988c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "706a5828-df7e-476c-987f-670acd804d7e", "node_type": "1", "metadata": {}, "hash": "22905e48a0e4857d895ac56f554a44649923fa8ef9e9652e8acf9deb13919aef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It enables operations that require maintaining and updating state, such as \ncumulative counts.\n\u2022\t Checkpointing: Checkpointing is a crucial mechanism in Spark Streaming to ensure fault \ntolerance and recovery. It periodically saves the metadata about the streaming application, \nincluding the configuration, DStream operations, and the processed data. It enables the recovery \nof the application if there are failures.\n\nStructured Streaming in Spark\n\nAdvantages\nNow, we will see the different advantages of using Spark Streaming in real-time operations:\n\u2022\t A unified processing model: One of the significant advantages of Spark Streaming is its \nintegration with the larger Spark ecosystem. It leverages the same programming model as batch \nprocessing, allowing users to seamlessly transition between batch and real-time processing. \nThis unified processing model simplifies development and reduces the learning curve for users \nfamiliar with Spark.", "mimetype": "text/plain", "start_char_idx": 251267, "end_char_idx": 252216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "706a5828-df7e-476c-987f-670acd804d7e": {"__data__": {"id_": "706a5828-df7e-476c-987f-670acd804d7e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40a81e57-e9c3-4560-b0bf-a6f10222a132", "node_type": "1", "metadata": {}, "hash": "f21a51adc4795c58c4783250525e3bb02dc75a69af0b71fc4c71ef6606b44f71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1821963a-b804-4d05-b64c-be8ba740d1ed", "node_type": "1", "metadata": {}, "hash": "e1d48f4ed4b52c7e0cbb5bd78f81ab5c2f2a90a382cdcec8eaee82f9315871bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It leverages the same programming model as batch \nprocessing, allowing users to seamlessly transition between batch and real-time processing. \nThis unified processing model simplifies development and reduces the learning curve for users \nfamiliar with Spark.\n\u2022\t High-level abstractions: Spark Streaming provides high-level abstractions such as DStreams \nto represent streaming data. DStreams are designed to handle continuous data streams and \nenable easy integration with existing Spark APIs, libraries, and data sources. These abstractions \nprovide a familiar and expressive programming interface to process real-time data.\n\u2022\t Fault tolerance and scalability: Spark Streaming offers fault-tolerant processing by leveraging \nSpark\u2019s RDD abstraction. It automatically recovers from failures by recomputing lost data, \nensuring the processing pipeline remains resilient and robust. Additionally, Spark Streaming \ncan scale horizontally by distributing a workload across a cluster of machines, allowing it to \nhandle large-scale data streams effectively.", "mimetype": "text/plain", "start_char_idx": 251958, "end_char_idx": 253010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1821963a-b804-4d05-b64c-be8ba740d1ed": {"__data__": {"id_": "1821963a-b804-4d05-b64c-be8ba740d1ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "706a5828-df7e-476c-987f-670acd804d7e", "node_type": "1", "metadata": {}, "hash": "22905e48a0e4857d895ac56f554a44649923fa8ef9e9652e8acf9deb13919aef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfaadb94-b123-4d12-97db-974bcc660886", "node_type": "1", "metadata": {}, "hash": "6dc1f361fcfa5c31a56f21edcd07154d6bcb8d0b96b585dcaa932631e482269f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Fault tolerance and scalability: Spark Streaming offers fault-tolerant processing by leveraging \nSpark\u2019s RDD abstraction. It automatically recovers from failures by recomputing lost data, \nensuring the processing pipeline remains resilient and robust. Additionally, Spark Streaming \ncan scale horizontally by distributing a workload across a cluster of machines, allowing it to \nhandle large-scale data streams effectively.\n\u2022\t Windowed computations: Spark Streaming supports windowed computations, which enable \ntime-based analysis over sliding or tumbling windows of data. Window operations provide \nflexibility in performing aggregations, time-series analysis, and window-level transformations. \nThis capability is particularly useful when analyzing streaming data based on temporal \ncharacteristics or patterns.\n\u2022\t A wide range of data sources: Spark Streaming seamlessly integrates with various data sources, \nincluding Kafka, Flume, Hadoop Distributed File System (HDFS), and Amazon S3.", "mimetype": "text/plain", "start_char_idx": 252584, "end_char_idx": 253578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfaadb94-b123-4d12-97db-974bcc660886": {"__data__": {"id_": "cfaadb94-b123-4d12-97db-974bcc660886", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1821963a-b804-4d05-b64c-be8ba740d1ed", "node_type": "1", "metadata": {}, "hash": "e1d48f4ed4b52c7e0cbb5bd78f81ab5c2f2a90a382cdcec8eaee82f9315871bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "496520a6-fe70-4fd8-8976-b29afd3d09c4", "node_type": "1", "metadata": {}, "hash": "0f422971f2ea5f31d34524d7e55296804862aacad8b3779e66ab31bb1eed361f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Window operations provide \nflexibility in performing aggregations, time-series analysis, and window-level transformations. \nThis capability is particularly useful when analyzing streaming data based on temporal \ncharacteristics or patterns.\n\u2022\t A wide range of data sources: Spark Streaming seamlessly integrates with various data sources, \nincluding Kafka, Flume, Hadoop Distributed File System (HDFS), and Amazon S3. This broad \nrange of data sources allows users to ingest data from multiple streams and integrate it with \nexisting data pipelines. Spark Streaming also supports custom data sources, enabling integration \nwith proprietary or specialized streaming platforms.\nWhile Spark Streaming offers powerful real-time data processing capabilities, there are certain \nchallenges to consider.\nChallenges\nBuilding streaming architectures for applications comes with its set of challenges, such as the following:\n\u2022\t End-to-end latency: Latency is introduced when Spark Streaming processes data in micro-\nbatches.", "mimetype": "text/plain", "start_char_idx": 253161, "end_char_idx": 254175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "496520a6-fe70-4fd8-8976-b29afd3d09c4": {"__data__": {"id_": "496520a6-fe70-4fd8-8976-b29afd3d09c4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfaadb94-b123-4d12-97db-974bcc660886", "node_type": "1", "metadata": {}, "hash": "6dc1f361fcfa5c31a56f21edcd07154d6bcb8d0b96b585dcaa932631e482269f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37563649-99cb-41a7-aabd-3cc56c744fc4", "node_type": "1", "metadata": {}, "hash": "768b0c1c470f40cfffd0af21bb7bb6d286c1d06917bc3fe2135aefdf78d569b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark Streaming also supports custom data sources, enabling integration \nwith proprietary or specialized streaming platforms.\nWhile Spark Streaming offers powerful real-time data processing capabilities, there are certain \nchallenges to consider.\nChallenges\nBuilding streaming architectures for applications comes with its set of challenges, such as the following:\n\u2022\t End-to-end latency: Latency is introduced when Spark Streaming processes data in micro-\nbatches. The end-to-end latency can vary based on factors such as the batch interval, the data \nsource, and the complexity of the computations.\n\u2022\t Fault tolerance: Spark Streaming provides fault tolerance through RDD lineage and checkpointing. \nHowever, failures in receivers or the driver program can still disrupt the stream processing. \n\nIntroducing Structured Streaming\n\nHandling and recovering from failures is an important consideration to ensure the reliability \nof Spark Streaming applications.\n\u2022\t Scalability: Scaling Spark Streaming applications to handle large volumes of data and meet high \nthroughput requirements can be a challenge.", "mimetype": "text/plain", "start_char_idx": 253711, "end_char_idx": 254813, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37563649-99cb-41a7-aabd-3cc56c744fc4": {"__data__": {"id_": "37563649-99cb-41a7-aabd-3cc56c744fc4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "496520a6-fe70-4fd8-8976-b29afd3d09c4", "node_type": "1", "metadata": {}, "hash": "0f422971f2ea5f31d34524d7e55296804862aacad8b3779e66ab31bb1eed361f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9187751-7cf6-4291-9668-99baabcb567c", "node_type": "1", "metadata": {}, "hash": "c470ec7851adf10c24ba76fe7732e1c3b18d166667754ac430eeccd7a684f88c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Fault tolerance: Spark Streaming provides fault tolerance through RDD lineage and checkpointing. \nHowever, failures in receivers or the driver program can still disrupt the stream processing. \n\nIntroducing Structured Streaming\n\nHandling and recovering from failures is an important consideration to ensure the reliability \nof Spark Streaming applications.\n\u2022\t Scalability: Scaling Spark Streaming applications to handle large volumes of data and meet high \nthroughput requirements can be a challenge. Proper resource allocation, tuning, and cluster \nmanagement are crucial to achieve scalability.\n\u2022\t Data ordering: Spark Streaming processes data in parallel across multiple worker nodes, which \ncan affect the order of events. Ensuring the correctness of the order of events becomes important \nin certain use cases, and developers need to consider this when designing their applications.\nIn summary, Spark Streaming brings the power of Apache Spark to real-time data processing.", "mimetype": "text/plain", "start_char_idx": 254311, "end_char_idx": 255291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9187751-7cf6-4291-9668-99baabcb567c": {"__data__": {"id_": "e9187751-7cf6-4291-9668-99baabcb567c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37563649-99cb-41a7-aabd-3cc56c744fc4", "node_type": "1", "metadata": {}, "hash": "768b0c1c470f40cfffd0af21bb7bb6d286c1d06917bc3fe2135aefdf78d569b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c22e25a4-be23-467a-866d-aa26ace2571c", "node_type": "1", "metadata": {}, "hash": "3ebd912bbd4e56cd519625b2a11ba23a2707d023df973c25393e40ac054dc020", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proper resource allocation, tuning, and cluster \nmanagement are crucial to achieve scalability.\n\u2022\t Data ordering: Spark Streaming processes data in parallel across multiple worker nodes, which \ncan affect the order of events. Ensuring the correctness of the order of events becomes important \nin certain use cases, and developers need to consider this when designing their applications.\nIn summary, Spark Streaming brings the power of Apache Spark to real-time data processing. Its \nintegration with the Spark ecosystem, high-level abstractions, fault tolerance, scalability, and support \nfor windowed computations make it a compelling choice for processing streaming data. By harnessing \nthe advantages of Spark Streaming, organizations can unlock valuable insights and make informed \ndecisions in real time.\nIn the next section, we will explore Structured Streaming, a newer and more expressive streaming \nAPI in Apache Spark that overcomes some of the limitations and challenges of Spark Streaming.", "mimetype": "text/plain", "start_char_idx": 254814, "end_char_idx": 255815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c22e25a4-be23-467a-866d-aa26ace2571c": {"__data__": {"id_": "c22e25a4-be23-467a-866d-aa26ace2571c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9187751-7cf6-4291-9668-99baabcb567c", "node_type": "1", "metadata": {}, "hash": "c470ec7851adf10c24ba76fe7732e1c3b18d166667754ac430eeccd7a684f88c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3243c420-1d1f-49df-b4e6-fbcb6db504c5", "node_type": "1", "metadata": {}, "hash": "ebeaf48069c4dbbde78b79a16da27e92d1590ee987720fff9f8713f9a483c916", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By harnessing \nthe advantages of Spark Streaming, organizations can unlock valuable insights and make informed \ndecisions in real time.\nIn the next section, we will explore Structured Streaming, a newer and more expressive streaming \nAPI in Apache Spark that overcomes some of the limitations and challenges of Spark Streaming. We \nwill discuss its core concepts, the differences from Spark Streaming, and its benefits for real-time \ndata processing.\nIntroducing Structured Streaming\nStructured Streaming is a revolutionary addition to Apache Spark that brings a new paradigm for \nreal-time data processing. It introduces a high-level API that seamlessly integrates batch and streaming \nprocessing, providing a unified programming model. Structured Streaming treats streaming data as an \nunbounded table or DataFrame, enabling developers to express complex computations using familiar \nSQL-like queries and transformations.\nUnlike the micro-batch processing model of Spark Streaming, Structured Streaming follows a \ncontinuous processing model.", "mimetype": "text/plain", "start_char_idx": 255488, "end_char_idx": 256532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3243c420-1d1f-49df-b4e6-fbcb6db504c5": {"__data__": {"id_": "3243c420-1d1f-49df-b4e6-fbcb6db504c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c22e25a4-be23-467a-866d-aa26ace2571c", "node_type": "1", "metadata": {}, "hash": "3ebd912bbd4e56cd519625b2a11ba23a2707d023df973c25393e40ac054dc020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba263390-366f-4e75-917f-fd6004ef7724", "node_type": "1", "metadata": {}, "hash": "57c2788c7ea55cea13dbcd9ae92fda35b99ff295826075f7783e152d8acc344d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It introduces a high-level API that seamlessly integrates batch and streaming \nprocessing, providing a unified programming model. Structured Streaming treats streaming data as an \nunbounded table or DataFrame, enabling developers to express complex computations using familiar \nSQL-like queries and transformations.\nUnlike the micro-batch processing model of Spark Streaming, Structured Streaming follows a \ncontinuous processing model. It processes data incrementally as it arrives, providing low-latency \nand near-real-time results. This shift toward continuous processing opens up new possibilities for \ninteractive analytics, dynamic visualizations, and real-time decision-making.\nKey features and advantages\nStructured Streaming offers several key features and advantages over traditional stream \nprocessing frameworks:\n\u2022\t An expressive API: Structured Streaming provides a declarative API that allows developers to \nexpress complex streaming computations using SQL queries, DataFrame operations, and Spark \nSQL functions. This enables developers with SQL or DataFrame expertise to easily transition \nto real-time data processing.", "mimetype": "text/plain", "start_char_idx": 256096, "end_char_idx": 257231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba263390-366f-4e75-917f-fd6004ef7724": {"__data__": {"id_": "ba263390-366f-4e75-917f-fd6004ef7724", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3243c420-1d1f-49df-b4e6-fbcb6db504c5", "node_type": "1", "metadata": {}, "hash": "ebeaf48069c4dbbde78b79a16da27e92d1590ee987720fff9f8713f9a483c916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9efbc85-e15b-4451-aaba-5b629f5c340b", "node_type": "1", "metadata": {}, "hash": "2b632c761cc999e6a5c37289d460da7961335d83c15094cb04e24a66a010364d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Key features and advantages\nStructured Streaming offers several key features and advantages over traditional stream \nprocessing frameworks:\n\u2022\t An expressive API: Structured Streaming provides a declarative API that allows developers to \nexpress complex streaming computations using SQL queries, DataFrame operations, and Spark \nSQL functions. This enables developers with SQL or DataFrame expertise to easily transition \nto real-time data processing.\n\nStructured Streaming in Spark\n\n\u2022\t Fault tolerance and exactly-once semantics: Structured Streaming guarantees end-to-end \nfault tolerance and exactly-once semantics by maintaining the necessary metadata and state \ninformation. It handles failures gracefully and ensures that data is processed exactly once, even \nin the presence of failures or retries.\n\u2022\t Scalability: Structured Streaming leverages the scalability of the Spark engine, enabling \nhorizontal scaling by adding more worker nodes to the cluster. It can handle high-throughput \ndata streams and scale seamlessly as the data volume increases.", "mimetype": "text/plain", "start_char_idx": 256781, "end_char_idx": 257837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9efbc85-e15b-4451-aaba-5b629f5c340b": {"__data__": {"id_": "c9efbc85-e15b-4451-aaba-5b629f5c340b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba263390-366f-4e75-917f-fd6004ef7724", "node_type": "1", "metadata": {}, "hash": "57c2788c7ea55cea13dbcd9ae92fda35b99ff295826075f7783e152d8acc344d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff507ead-b1a4-4569-8992-e94ede22decb", "node_type": "1", "metadata": {}, "hash": "f184222268de2648d0b50bd8e6f6460c9a663faa8750a5cef44f72288876b71d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It handles failures gracefully and ensures that data is processed exactly once, even \nin the presence of failures or retries.\n\u2022\t Scalability: Structured Streaming leverages the scalability of the Spark engine, enabling \nhorizontal scaling by adding more worker nodes to the cluster. It can handle high-throughput \ndata streams and scale seamlessly as the data volume increases.\n\u2022\t Unified batch and streaming: With Structured Streaming, developers can use the same API \nand programming model for both batch and streaming processing. This unification simplifies \nthe development and maintenance of applications, as there is no need to manage separate batch \nand stream processing code bases.\n\u2022\t Ecosystem integration: Structured Streaming seamlessly integrates with the broader Spark \necosystem, enabling the use of libraries such as Spark SQL, MLlib, and GraphX for real-time \nanalytics, machine learning, and graph processing on streaming data.", "mimetype": "text/plain", "start_char_idx": 257460, "end_char_idx": 258405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff507ead-b1a4-4569-8992-e94ede22decb": {"__data__": {"id_": "ff507ead-b1a4-4569-8992-e94ede22decb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9efbc85-e15b-4451-aaba-5b629f5c340b", "node_type": "1", "metadata": {}, "hash": "2b632c761cc999e6a5c37289d460da7961335d83c15094cb04e24a66a010364d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48d21dad-6d4b-4448-b6d9-6426a7535ce6", "node_type": "1", "metadata": {}, "hash": "21d2b9272a4d67901d7a93fa6a52ee088507ae995f9a7d7b3fb79b1fa628f900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This unification simplifies \nthe development and maintenance of applications, as there is no need to manage separate batch \nand stream processing code bases.\n\u2022\t Ecosystem integration: Structured Streaming seamlessly integrates with the broader Spark \necosystem, enabling the use of libraries such as Spark SQL, MLlib, and GraphX for real-time \nanalytics, machine learning, and graph processing on streaming data.\nNow, let\u2019s take a look at some of the differences between Structured Streaming and Spark Streaming.\nStructured Streaming versus Spark Streaming\nStructured Streaming differs from Spark Streaming in several fundamental ways:\n\u2022\t The processing model: Spark Streaming processes data in micro-batches, where each batch is \ntreated as a discrete RDD. In contrast, Structured Streaming processes data incrementally in a \ncontinuous manner, treating the stream as an unbounded table or DataFrame.\n\u2022\t The API and query language: Spark Streaming primarily offers a low-level API based on RDD \ntransformations and actions.", "mimetype": "text/plain", "start_char_idx": 257993, "end_char_idx": 259017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48d21dad-6d4b-4448-b6d9-6426a7535ce6": {"__data__": {"id_": "48d21dad-6d4b-4448-b6d9-6426a7535ce6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff507ead-b1a4-4569-8992-e94ede22decb", "node_type": "1", "metadata": {}, "hash": "f184222268de2648d0b50bd8e6f6460c9a663faa8750a5cef44f72288876b71d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f", "node_type": "1", "metadata": {}, "hash": "d72d876e508e912593d4502279a9f9cbf1aa557330844e8fbcf8f92670ab4b9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast, Structured Streaming processes data incrementally in a \ncontinuous manner, treating the stream as an unbounded table or DataFrame.\n\u2022\t The API and query language: Spark Streaming primarily offers a low-level API based on RDD \ntransformations and actions. Structured Streaming, on the other hand, provides a higher-level \nAPI with SQL-like queries, DataFrame operations, and Spark SQL functions. This makes it \neasier to express complex computations and leverage the power of SQL for real-time analytics.\n\u2022\t Fault tolerance: Both Spark Streaming and Structured Streaming provide fault tolerance. \nHowever, Structured Streaming\u2019s fault tolerance is achieved by maintaining the necessary metadata \nand state information, whereas Spark Streaming relies on RDD lineage and checkpointing for \nfault recovery.\n\u2022\t Data processing guarantees: Spark Streaming provides at-least-once processing guarantees \nby default, where some duplicates may be processed if there are failures.", "mimetype": "text/plain", "start_char_idx": 258751, "end_char_idx": 259732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f": {"__data__": {"id_": "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48d21dad-6d4b-4448-b6d9-6426a7535ce6", "node_type": "1", "metadata": {}, "hash": "21d2b9272a4d67901d7a93fa6a52ee088507ae995f9a7d7b3fb79b1fa628f900", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70ccc0a2-3007-47b6-9917-0b0a3c5397f8", "node_type": "1", "metadata": {}, "hash": "6b0bbcb7fe9fc34a57f9081eae9d714dcf8b1f5ac260aa60cfebee786e1584e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Fault tolerance: Both Spark Streaming and Structured Streaming provide fault tolerance. \nHowever, Structured Streaming\u2019s fault tolerance is achieved by maintaining the necessary metadata \nand state information, whereas Spark Streaming relies on RDD lineage and checkpointing for \nfault recovery.\n\u2022\t Data processing guarantees: Spark Streaming provides at-least-once processing guarantees \nby default, where some duplicates may be processed if there are failures. Structured Streaming, \non the other hand, provides exactly-once processing semantics, ensuring that each event is \nprocessed exactly once, even in the presence of failures or retries.\n\nStreaming fundamentals\n\nLimitations and considerations\nWhile Structured Streaming offers significant advantages, there are certain limitations and considerations \nto keep in mind:\n\u2022\t Event time handling: Proper handling of event time, such as timestamp extraction, watermarking, \nand late data handling, is essential in Structured Streaming. Care should be taken to ensure the \ncorrect processing and handling of out-of-order events.", "mimetype": "text/plain", "start_char_idx": 259267, "end_char_idx": 260351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70ccc0a2-3007-47b6-9917-0b0a3c5397f8": {"__data__": {"id_": "70ccc0a2-3007-47b6-9917-0b0a3c5397f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f", "node_type": "1", "metadata": {}, "hash": "d72d876e508e912593d4502279a9f9cbf1aa557330844e8fbcf8f92670ab4b9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e59321b5-64ce-4cd2-8554-da9157d59aa5", "node_type": "1", "metadata": {}, "hash": "3efd28553506f90169ba8cd1630aa88e72521cb8c7b3864e4f651725e1ea6dd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Streaming fundamentals\n\nLimitations and considerations\nWhile Structured Streaming offers significant advantages, there are certain limitations and considerations \nto keep in mind:\n\u2022\t Event time handling: Proper handling of event time, such as timestamp extraction, watermarking, \nand late data handling, is essential in Structured Streaming. Care should be taken to ensure the \ncorrect processing and handling of out-of-order events.\n\u2022\t State management: Structured Streaming allows you to maintain stateful information \nacross batches, which can introduce challenges related to state management and scalability. \nMonitoring memory usage and configuring appropriate state retention policies are crucial for \noptimal performance.\n\u2022\t Ecosystem compatibility: While Structured Streaming integrates well with the Spark ecosystem, \ncertain libraries and features might not be fully compatible with real-time streaming use cases. \nIt is important to evaluate the compatibility of specific libraries and functionalities before using \nthem in a Structured Streaming application.", "mimetype": "text/plain", "start_char_idx": 259918, "end_char_idx": 260988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e59321b5-64ce-4cd2-8554-da9157d59aa5": {"__data__": {"id_": "e59321b5-64ce-4cd2-8554-da9157d59aa5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70ccc0a2-3007-47b6-9917-0b0a3c5397f8", "node_type": "1", "metadata": {}, "hash": "6b0bbcb7fe9fc34a57f9081eae9d714dcf8b1f5ac260aa60cfebee786e1584e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad658f22-79a2-4a69-9c2d-aaccdd525120", "node_type": "1", "metadata": {}, "hash": "ef4406eebcb0df52a430af1708f3780dbe466e9c9a9af2a326af4b44ba51380e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Monitoring memory usage and configuring appropriate state retention policies are crucial for \noptimal performance.\n\u2022\t Ecosystem compatibility: While Structured Streaming integrates well with the Spark ecosystem, \ncertain libraries and features might not be fully compatible with real-time streaming use cases. \nIt is important to evaluate the compatibility of specific libraries and functionalities before using \nthem in a Structured Streaming application.\n\u2022\t Performance considerations: Structured Streaming\u2019s continuous processing model introduces \ndifferent performance considerations compared to micro-batch processing. Factors such as \nthe event rate, processing time, and resource allocation need to be carefully monitored and \noptimized for efficient real-time data processing.\nIn the next section, we will delve deeper into the concepts of stateless and stateful streaming, exploring \ntheir differences and use cases in the context of Structured Streaming.\nStreaming fundamentals\nLet\u2019s start by looking at some of the fundamental concepts in streaming that will help us get familiarized \nwith different paradigms in streaming.", "mimetype": "text/plain", "start_char_idx": 260532, "end_char_idx": 261666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad658f22-79a2-4a69-9c2d-aaccdd525120": {"__data__": {"id_": "ad658f22-79a2-4a69-9c2d-aaccdd525120", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e59321b5-64ce-4cd2-8554-da9157d59aa5", "node_type": "1", "metadata": {}, "hash": "3efd28553506f90169ba8cd1630aa88e72521cb8c7b3864e4f651725e1ea6dd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71277d93-6365-40c7-bda9-5e28d6de9d03", "node_type": "1", "metadata": {}, "hash": "7221fd8d7109378243c22d9a2161f74a5fea4de631e53f282081bcc65dc500b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next section, we will delve deeper into the concepts of stateless and stateful streaming, exploring \ntheir differences and use cases in the context of Structured Streaming.\nStreaming fundamentals\nLet\u2019s start by looking at some of the fundamental concepts in streaming that will help us get familiarized \nwith different paradigms in streaming.\nStateless streaming \u2013 processing one event at a time\nStateless streaming refers to the processing of each event in isolation, without considering any context or \nhistory from previous events. In this approach, each event is treated independently, and the processing \nlogic does not rely on any accumulated state or information from past events.\nStateless streaming is well-suited for scenarios where each event can be processed independently, \nand the output is solely determined by the content of the event itself. This approach is often used for \nsimple filtering, transformation, or enrichment operations that do not require you to maintain any \ncontextual information across events.", "mimetype": "text/plain", "start_char_idx": 261317, "end_char_idx": 262353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71277d93-6365-40c7-bda9-5e28d6de9d03": {"__data__": {"id_": "71277d93-6365-40c7-bda9-5e28d6de9d03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad658f22-79a2-4a69-9c2d-aaccdd525120", "node_type": "1", "metadata": {}, "hash": "ef4406eebcb0df52a430af1708f3780dbe466e9c9a9af2a326af4b44ba51380e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f488999e-c3c3-4817-b2aa-5cd7ae5854d3", "node_type": "1", "metadata": {}, "hash": "210d142d32d6df3ee2fb102e5b9b2dae9775239cca2f35606cbd0d0f793ece5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stateless streaming is well-suited for scenarios where each event can be processed independently, \nand the output is solely determined by the content of the event itself. This approach is often used for \nsimple filtering, transformation, or enrichment operations that do not require you to maintain any \ncontextual information across events.\n\nStructured Streaming in Spark\n\nStateful streaming \u2013 maintaining stateful information\nStateful streaming involves maintaining and utilizing contextual information or state across multiple \nevents during processing. The processing logic considers the history of events and uses accumulated \ninformation to make decisions or perform computations. Stateful streaming enables more sophisticated \nanalysis and complex computations that rely on context or accumulated knowledge.\nStateful streaming requires you to maintain and update state information as new events arrive. \nThe state can be as simple as a running count or more complex, involving aggregations, windowed \ncomputations, or maintaining session information.", "mimetype": "text/plain", "start_char_idx": 262012, "end_char_idx": 263069, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f488999e-c3c3-4817-b2aa-5cd7ae5854d3": {"__data__": {"id_": "f488999e-c3c3-4817-b2aa-5cd7ae5854d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71277d93-6365-40c7-bda9-5e28d6de9d03", "node_type": "1", "metadata": {}, "hash": "7221fd8d7109378243c22d9a2161f74a5fea4de631e53f282081bcc65dc500b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da", "node_type": "1", "metadata": {}, "hash": "309ae2e66d73feb243b39cc9c1b8b5c208e37e6338a8649af76c05cd73d389f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The processing logic considers the history of events and uses accumulated \ninformation to make decisions or perform computations. Stateful streaming enables more sophisticated \nanalysis and complex computations that rely on context or accumulated knowledge.\nStateful streaming requires you to maintain and update state information as new events arrive. \nThe state can be as simple as a running count or more complex, involving aggregations, windowed \ncomputations, or maintaining session information. Proper management of state is essential to ensure \ncorrectness, scalability, and fault tolerance in stateful streaming applications.\nLet\u2019s understand the differences between stateless and stateful streaming.\nThe differences between stateless and stateful streaming\nThe main differences between stateless and stateful streaming can be summarized as follows:\n\u2022\t Stateless streaming processes events independently, while stateful streaming maintains and \nuses accumulated state information across events.", "mimetype": "text/plain", "start_char_idx": 262569, "end_char_idx": 263571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da": {"__data__": {"id_": "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f488999e-c3c3-4817-b2aa-5cd7ae5854d3", "node_type": "1", "metadata": {}, "hash": "210d142d32d6df3ee2fb102e5b9b2dae9775239cca2f35606cbd0d0f793ece5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41de8e14-b71e-4776-8fa1-46d827c36c27", "node_type": "1", "metadata": {}, "hash": "6ab847af0ee2bf21b56a1f86519c1068e101be0e76b2445a58da4fab8fec306f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proper management of state is essential to ensure \ncorrectness, scalability, and fault tolerance in stateful streaming applications.\nLet\u2019s understand the differences between stateless and stateful streaming.\nThe differences between stateless and stateful streaming\nThe main differences between stateless and stateful streaming can be summarized as follows:\n\u2022\t Stateless streaming processes events independently, while stateful streaming maintains and \nuses accumulated state information across events.\n\u2022\t Stateless streaming is suitable for simple operations that don\u2019t rely on past events, while stateful \nstreaming enables complex computations that require context or accumulated knowledge.\n\u2022\t Stateless streaming is generally simpler to implement and reason about, while stateful streaming \nintroduces additional challenges in managing state, fault tolerance, and scalability.\n\u2022\t Stateless streaming is often used for real-time filtering, transformation, or basic aggregations, while \nstateful streaming is necessary for windowed computations, sessionization, and stateful joins.", "mimetype": "text/plain", "start_char_idx": 263070, "end_char_idx": 264152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41de8e14-b71e-4776-8fa1-46d827c36c27": {"__data__": {"id_": "41de8e14-b71e-4776-8fa1-46d827c36c27", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da", "node_type": "1", "metadata": {}, "hash": "309ae2e66d73feb243b39cc9c1b8b5c208e37e6338a8649af76c05cd73d389f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b602795-25d9-4792-93ed-eb9598d2226b", "node_type": "1", "metadata": {}, "hash": "b47cdd7a96319637ffff00a3b586c77766650a1de9e91d35db023bb65126cd26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Stateless streaming is generally simpler to implement and reason about, while stateful streaming \nintroduces additional challenges in managing state, fault tolerance, and scalability.\n\u2022\t Stateless streaming is often used for real-time filtering, transformation, or basic aggregations, while \nstateful streaming is necessary for windowed computations, sessionization, and stateful joins.\nUnderstanding the distinction between stateless and stateful streaming is crucial when designing real-\ntime data processing systems, as it helps determine the appropriate processing model and requirements \nfor a given use case.\nNow, let\u2019s take a look at some of the fundamentals of Structured Streaming.\nStructured Streaming concepts\nTo understand Structured Streaming, it\u2019s important for us to understand the different operations that take \nplace in a near-real-time scenario when data arrives. We will understand them in the following section.", "mimetype": "text/plain", "start_char_idx": 263763, "end_char_idx": 264698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b602795-25d9-4792-93ed-eb9598d2226b": {"__data__": {"id_": "9b602795-25d9-4792-93ed-eb9598d2226b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41de8e14-b71e-4776-8fa1-46d827c36c27", "node_type": "1", "metadata": {}, "hash": "6ab847af0ee2bf21b56a1f86519c1068e101be0e76b2445a58da4fab8fec306f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6", "node_type": "1", "metadata": {}, "hash": "b4880f9ac0bba1b457bb3a4f6332caf266d7f74be7f98b2bcee8bae0ae63f2ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, let\u2019s take a look at some of the fundamentals of Structured Streaming.\nStructured Streaming concepts\nTo understand Structured Streaming, it\u2019s important for us to understand the different operations that take \nplace in a near-real-time scenario when data arrives. We will understand them in the following section.\nEvent time and processing time\nIn Structured Streaming, there are two important notions of time \u2013 event time and processing time:\n\u2022\t Event time: Event time refers to the time when an event occurred or was generated. It is typically \nembedded within the data itself, representing the timestamp or a field indicating when the event \n\nStructured Streaming concepts\n\noccurred in the real world. Event time is crucial for analyzing data based on its temporal order \nor performing window-based computations.\n\u2022\t Processing time: Processing time, on the other hand, refers to the time when an event is \nprocessed by the streaming application.", "mimetype": "text/plain", "start_char_idx": 264381, "end_char_idx": 265333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6": {"__data__": {"id_": "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b602795-25d9-4792-93ed-eb9598d2226b", "node_type": "1", "metadata": {}, "hash": "b47cdd7a96319637ffff00a3b586c77766650a1de9e91d35db023bb65126cd26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6513f047-6a0e-45cc-aa50-aa6de5575b12", "node_type": "1", "metadata": {}, "hash": "9d5e736967d62a1b08bb8ede5dff07fc527c40e0a589f341ba2c3017a6f2a99e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is typically \nembedded within the data itself, representing the timestamp or a field indicating when the event \n\nStructured Streaming concepts\n\noccurred in the real world. Event time is crucial for analyzing data based on its temporal order \nor performing window-based computations.\n\u2022\t Processing time: Processing time, on the other hand, refers to the time when an event is \nprocessed by the streaming application. It is determined by the system clock or the time at which \nthe event is ingested by the processing engine. Processing time is useful for tasks that require \nlow latency or an immediate response but may not accurately reflect the actual event order.\nBased on these different time concepts, we can determine which one works best for a given use case. \nIt\u2019s important to understand the difference between the two. Based on that, the strategy for data \nprocessing can be determined.", "mimetype": "text/plain", "start_char_idx": 264915, "end_char_idx": 265812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6513f047-6a0e-45cc-aa50-aa6de5575b12": {"__data__": {"id_": "6513f047-6a0e-45cc-aa50-aa6de5575b12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6", "node_type": "1", "metadata": {}, "hash": "b4880f9ac0bba1b457bb3a4f6332caf266d7f74be7f98b2bcee8bae0ae63f2ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ff53ebc-4cf2-405f-9567-bdbefb7593cc", "node_type": "1", "metadata": {}, "hash": "bfec59d2b9df57fb6aef2e6ab4ffa8f3ae8a71fcc3dd2ced44d9e2cc8716efc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Processing time is useful for tasks that require \nlow latency or an immediate response but may not accurately reflect the actual event order.\nBased on these different time concepts, we can determine which one works best for a given use case. \nIt\u2019s important to understand the difference between the two. Based on that, the strategy for data \nprocessing can be determined.\nWatermarking and late data handling\nNow, we will discuss how to handle data that doesn\u2019t arrive at the defined time in real-time applications. \nThere are different ways to handle that situation. Structured Streaming has a built-in mechanism to \nhandle this type of data. These mechanisms include:\n\u2022\t Watermarking: Watermarking is a mechanism in Structured Streaming used to deal with \nevent time and handle delayed or late-arriving data. A watermark is a threshold timestamp \nthat indicates the maximum event time seen by a system up to a certain point.", "mimetype": "text/plain", "start_char_idx": 265441, "end_char_idx": 266366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ff53ebc-4cf2-405f-9567-bdbefb7593cc": {"__data__": {"id_": "1ff53ebc-4cf2-405f-9567-bdbefb7593cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6513f047-6a0e-45cc-aa50-aa6de5575b12", "node_type": "1", "metadata": {}, "hash": "9d5e736967d62a1b08bb8ede5dff07fc527c40e0a589f341ba2c3017a6f2a99e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "231f8202-66da-42ad-b657-843c826b99e2", "node_type": "1", "metadata": {}, "hash": "52055d982543e18c9a95c0f7157c82ea23e55c47aaa89d54abde05e4f8be16cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are different ways to handle that situation. Structured Streaming has a built-in mechanism to \nhandle this type of data. These mechanisms include:\n\u2022\t Watermarking: Watermarking is a mechanism in Structured Streaming used to deal with \nevent time and handle delayed or late-arriving data. A watermark is a threshold timestamp \nthat indicates the maximum event time seen by a system up to a certain point. It allows the \nsystem to track the progress of event time and determine when it is safe to emit results for a \nspecific window.\n\u2022\t Late data handling: Late-arriving data refers to events that have timestamps beyond the \nwatermark threshold. Structured Streaming provides options to handle late data, such as \ndiscarding it, updating existing results, or storing it separately for further analysis.\nThese built-in mechanisms save users a lot of time and efficiently handle late-arriving data.", "mimetype": "text/plain", "start_char_idx": 265957, "end_char_idx": 266858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "231f8202-66da-42ad-b657-843c826b99e2": {"__data__": {"id_": "231f8202-66da-42ad-b657-843c826b99e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ff53ebc-4cf2-405f-9567-bdbefb7593cc", "node_type": "1", "metadata": {}, "hash": "bfec59d2b9df57fb6aef2e6ab4ffa8f3ae8a71fcc3dd2ced44d9e2cc8716efc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90799028-d669-4363-a648-78b55b29d8bf", "node_type": "1", "metadata": {}, "hash": "c4cf00939023e93d087ea55cf3ce1ae0a2e703fddbcdb53c7ae1c50616c23472", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Late data handling: Late-arriving data refers to events that have timestamps beyond the \nwatermark threshold. Structured Streaming provides options to handle late data, such as \ndiscarding it, updating existing results, or storing it separately for further analysis.\nThese built-in mechanisms save users a lot of time and efficiently handle late-arriving data.\nNext, we will see, once the data arrives, how we start the operations on it in streaming.\nTriggers and output modes\nTriggers determine when a streaming application should emit results or trigger the execution of the \ncomputation. Structured Streaming supports different types of triggers:\n\u2022\t Event time triggers: Event time triggers operate based on the arrival of new events or when \na watermark advances beyond a certain threshold. They enable more accurate and efficient \nprocessing, based on event time semantics.", "mimetype": "text/plain", "start_char_idx": 266495, "end_char_idx": 267376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90799028-d669-4363-a648-78b55b29d8bf": {"__data__": {"id_": "90799028-d669-4363-a648-78b55b29d8bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "231f8202-66da-42ad-b657-843c826b99e2", "node_type": "1", "metadata": {}, "hash": "52055d982543e18c9a95c0f7157c82ea23e55c47aaa89d54abde05e4f8be16cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "902ec6b2-376e-4c52-adcd-f2333c2cc3d0", "node_type": "1", "metadata": {}, "hash": "42e3a20ac343b71e50ce6a1f983f4cb9b1be8aab445d0d0d9f0d91064468e5cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Triggers and output modes\nTriggers determine when a streaming application should emit results or trigger the execution of the \ncomputation. Structured Streaming supports different types of triggers:\n\u2022\t Event time triggers: Event time triggers operate based on the arrival of new events or when \na watermark advances beyond a certain threshold. They enable more accurate and efficient \nprocessing, based on event time semantics.\n\u2022\t Processing time triggers: These triggers operate based on processing time, allowing you to \nspecify time intervals or durations at which the computation should be executed.\n\nStructured Streaming in Spark\n\nStructured Streaming also offers different output modes. The output modes determine how data is \nupdated in the sink. A sink is where we would write the output after the streaming operation:\n\u2022\t Complete mode: In this mode, the entire updated result, including all the rows in the output, \nis written to the sink.", "mimetype": "text/plain", "start_char_idx": 266949, "end_char_idx": 267897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "902ec6b2-376e-4c52-adcd-f2333c2cc3d0": {"__data__": {"id_": "902ec6b2-376e-4c52-adcd-f2333c2cc3d0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90799028-d669-4363-a648-78b55b29d8bf", "node_type": "1", "metadata": {}, "hash": "c4cf00939023e93d087ea55cf3ce1ae0a2e703fddbcdb53c7ae1c50616c23472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ab02bc8-feec-4598-ba88-eea311e8f611", "node_type": "1", "metadata": {}, "hash": "53defd76fd86e8d1eaca8a8e55fa1bfaf71e43998ba9544154d05e7d64648c36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming in Spark\n\nStructured Streaming also offers different output modes. The output modes determine how data is \nupdated in the sink. A sink is where we would write the output after the streaming operation:\n\u2022\t Complete mode: In this mode, the entire updated result, including all the rows in the output, \nis written to the sink. This mode provides the most comprehensive view of data but can be \nmemory-intensive for large result sets.\n\u2022\t Append mode: In append mode, only the new rows appended to the result table since the \nlast trigger are written to the sink. This mode is suitable for cases where the result is an \nappend-only stream.\n\u2022\t Update mode: Update mode only writes the changed rows to the sink, preserving the existing \nrows that haven\u2019t changed since the last trigger. This mode is useful for cases where the result \ntable is updated incrementally.", "mimetype": "text/plain", "start_char_idx": 267554, "end_char_idx": 268433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ab02bc8-feec-4598-ba88-eea311e8f611": {"__data__": {"id_": "3ab02bc8-feec-4598-ba88-eea311e8f611", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "902ec6b2-376e-4c52-adcd-f2333c2cc3d0", "node_type": "1", "metadata": {}, "hash": "42e3a20ac343b71e50ce6a1f983f4cb9b1be8aab445d0d0d9f0d91064468e5cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69be9a93-8601-459a-8edd-e6a1e32c64c8", "node_type": "1", "metadata": {}, "hash": "39ff1c6a9935aa46ac3012d8a49702ac77f0d298710b9e1f91e7e6ae31f23c64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This mode is suitable for cases where the result is an \nappend-only stream.\n\u2022\t Update mode: Update mode only writes the changed rows to the sink, preserving the existing \nrows that haven\u2019t changed since the last trigger. This mode is useful for cases where the result \ntable is updated incrementally.\nNow, let\u2019s take a look at the different types of aggregate operations we can do on streaming data.\nWindowing operations\nWindowing operations in Structured Streaming allow you to group and aggregate data over specific \ntime windows. Windows for these operations can be defined based on either event time or processing \ntime, and they provide a way to perform computations over a subset of events within a given time range.\nThe common types of windowing operations include the following:\n\u2022\t Tumbling windows: Tumbling windows divide a stream into non-overlapping fixed-size windows. \nEach event falls into exactly one window, and computations are performed independently for \neach window.", "mimetype": "text/plain", "start_char_idx": 268133, "end_char_idx": 269120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69be9a93-8601-459a-8edd-e6a1e32c64c8": {"__data__": {"id_": "69be9a93-8601-459a-8edd-e6a1e32c64c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ab02bc8-feec-4598-ba88-eea311e8f611", "node_type": "1", "metadata": {}, "hash": "53defd76fd86e8d1eaca8a8e55fa1bfaf71e43998ba9544154d05e7d64648c36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc6106f0-3725-4577-93a3-340b050513dd", "node_type": "1", "metadata": {}, "hash": "2f520db2bc635ad94cd28ef644af31a67d382560b5c286f835789465f58b17f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Windows for these operations can be defined based on either event time or processing \ntime, and they provide a way to perform computations over a subset of events within a given time range.\nThe common types of windowing operations include the following:\n\u2022\t Tumbling windows: Tumbling windows divide a stream into non-overlapping fixed-size windows. \nEach event falls into exactly one window, and computations are performed independently for \neach window.\n\u2022\t Sliding windows: Sliding windows create overlapping windows that slide or move over a stream \nat regular intervals. Each event can contribute to multiple windows, and computations can be \nperformed on the overlapping parts.\n\u2022\t Session windows: Session windows group events that are close in time or belong to the same \nsession, based on a specified session timeout. A session is defined as a series of events within \na certain time threshold of each other.\nThe next operation that we frequently use in streaming is the join operation.", "mimetype": "text/plain", "start_char_idx": 268666, "end_char_idx": 269658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc6106f0-3725-4577-93a3-340b050513dd": {"__data__": {"id_": "dc6106f0-3725-4577-93a3-340b050513dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69be9a93-8601-459a-8edd-e6a1e32c64c8", "node_type": "1", "metadata": {}, "hash": "39ff1c6a9935aa46ac3012d8a49702ac77f0d298710b9e1f91e7e6ae31f23c64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be10f36-de61-469b-996c-091c339025d5", "node_type": "1", "metadata": {}, "hash": "fd443aa738e46855919f79d99adc8ab147d9178f3961d88fad27f3552e5adfa1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each event can contribute to multiple windows, and computations can be \nperformed on the overlapping parts.\n\u2022\t Session windows: Session windows group events that are close in time or belong to the same \nsession, based on a specified session timeout. A session is defined as a series of events within \na certain time threshold of each other.\nThe next operation that we frequently use in streaming is the join operation. Now, we will see how \nwe can use joins with streaming data.\nJoins and aggregations\nStructured Streaming supports joins and aggregations on streaming data, enabling complex analytics \nand data transformations:\n\u2022\t Joins: Streaming joins allow you to combine two or more streams or a stream with static/\nreference data, based on a common key or condition. The join operation can be performed \n\nStreaming sources and sinks\n\nusing event time or processing time, and it supports different join types such as inner join, \nouter join, and left/right join.", "mimetype": "text/plain", "start_char_idx": 269240, "end_char_idx": 270206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8be10f36-de61-469b-996c-091c339025d5": {"__data__": {"id_": "8be10f36-de61-469b-996c-091c339025d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc6106f0-3725-4577-93a3-340b050513dd", "node_type": "1", "metadata": {}, "hash": "2f520db2bc635ad94cd28ef644af31a67d382560b5c286f835789465f58b17f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb1f9fc6-8635-43b1-b74c-13af19cd13e2", "node_type": "1", "metadata": {}, "hash": "422f79218c72a51d2a7ec5bf503e672de1e008fae02ca69374ef5774487bf589", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The join operation can be performed \n\nStreaming sources and sinks\n\nusing event time or processing time, and it supports different join types such as inner join, \nouter join, and left/right join.\n\u2022\t Aggregations: Streaming aggregations enable you to calculate summary statistics or perform \naggregations on streaming data. Aggregations can be performed on individual streams or in \ncombination with joins. Common operations include count, sum, average, min, and max.\nStructured Streaming\u2019s flexible and expressive API for handling event time, triggers, output modes, \nwindowing operations, joins, and aggregations allows developers to perform comprehensive real-time \nanalytics and computations on streaming data. By understanding these concepts, developers can build \nsophisticated streaming applications with ease and precision.\nIn the next section, we will explore how to read and write data with streaming sources and sinks.", "mimetype": "text/plain", "start_char_idx": 270012, "end_char_idx": 270939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb1f9fc6-8635-43b1-b74c-13af19cd13e2": {"__data__": {"id_": "eb1f9fc6-8635-43b1-b74c-13af19cd13e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8be10f36-de61-469b-996c-091c339025d5", "node_type": "1", "metadata": {}, "hash": "fd443aa738e46855919f79d99adc8ab147d9178f3961d88fad27f3552e5adfa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6800bda3-1851-4c38-89ae-6fe1689386de", "node_type": "1", "metadata": {}, "hash": "679840a611b81a254006059b8a89d619bfa08196215c173c8bb7e55885cc1722", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming\u2019s flexible and expressive API for handling event time, triggers, output modes, \nwindowing operations, joins, and aggregations allows developers to perform comprehensive real-time \nanalytics and computations on streaming data. By understanding these concepts, developers can build \nsophisticated streaming applications with ease and precision.\nIn the next section, we will explore how to read and write data with streaming sources and sinks.\nStreaming sources and sinks\nStreaming sources and sinks are essential components in a streaming system that enable the ingestion \nof data from external systems and the output of processed data to external destinations. They form \nthe connectors between the streaming application and the data sources or sinks.\nStreaming sources retrieve data from various input systems, such as message queues, filesystems, \ndatabases, or external APIs, and make it available for processing in a streaming application.", "mimetype": "text/plain", "start_char_idx": 270478, "end_char_idx": 271441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6800bda3-1851-4c38-89ae-6fe1689386de": {"__data__": {"id_": "6800bda3-1851-4c38-89ae-6fe1689386de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb1f9fc6-8635-43b1-b74c-13af19cd13e2", "node_type": "1", "metadata": {}, "hash": "422f79218c72a51d2a7ec5bf503e672de1e008fae02ca69374ef5774487bf589", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd56ff93-dc0b-4f49-8a36-6736740e4025", "node_type": "1", "metadata": {}, "hash": "aa333dac89aef4be9f3b663b642ef809cb927128bc4f02351199e8f5a2ce2e49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Streaming sources and sinks\nStreaming sources and sinks are essential components in a streaming system that enable the ingestion \nof data from external systems and the output of processed data to external destinations. They form \nthe connectors between the streaming application and the data sources or sinks.\nStreaming sources retrieve data from various input systems, such as message queues, filesystems, \ndatabases, or external APIs, and make it available for processing in a streaming application. On the \nother hand, streaming sinks receive processed data from the application and write it to external storage, \ndatabases, filesystems, or other systems for further analysis or consumption.\nThere are different types of streaming sources and sinks. We will explore some of them next.\nBuilt-in streaming sources\nStructured Streaming provides built-in support for a variety of streaming sources, making it easy to \nintegrate with popular data systems.", "mimetype": "text/plain", "start_char_idx": 270940, "end_char_idx": 271893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd56ff93-dc0b-4f49-8a36-6736740e4025": {"__data__": {"id_": "dd56ff93-dc0b-4f49-8a36-6736740e4025", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6800bda3-1851-4c38-89ae-6fe1689386de", "node_type": "1", "metadata": {}, "hash": "679840a611b81a254006059b8a89d619bfa08196215c173c8bb7e55885cc1722", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0aebecb-ab78-4711-ac52-2b5630666c9f", "node_type": "1", "metadata": {}, "hash": "b08d8cc5a2937f37517a7439fb7500d82f4047c283e94cb91f2a37a718c50030", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the \nother hand, streaming sinks receive processed data from the application and write it to external storage, \ndatabases, filesystems, or other systems for further analysis or consumption.\nThere are different types of streaming sources and sinks. We will explore some of them next.\nBuilt-in streaming sources\nStructured Streaming provides built-in support for a variety of streaming sources, making it easy to \nintegrate with popular data systems. Some of the commonly used built-in streaming sources include \nthe following:\n\u2022\t The file source: The file source allows you to read data from files in a directory or a file stream \nfrom a file-based system, such as HDFS or Amazon S3.\n\u2022\t KafkaSource: KafkaSource enables the consumption of data from Apache Kafka, a distributed \nstreaming platform. It provides fault-tolerant, scalable, and high-throughput ingestion of \ndata streams.", "mimetype": "text/plain", "start_char_idx": 271442, "end_char_idx": 272327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0aebecb-ab78-4711-ac52-2b5630666c9f": {"__data__": {"id_": "a0aebecb-ab78-4711-ac52-2b5630666c9f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd56ff93-dc0b-4f49-8a36-6736740e4025", "node_type": "1", "metadata": {}, "hash": "aa333dac89aef4be9f3b663b642ef809cb927128bc4f02351199e8f5a2ce2e49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b323cdcc-4d57-498c-82c6-e50f3a92fea0", "node_type": "1", "metadata": {}, "hash": "c9129a7fe28b8cf93468bbc9956ff3e2a48071c1018672a89af14633f64bc9df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t KafkaSource: KafkaSource enables the consumption of data from Apache Kafka, a distributed \nstreaming platform. It provides fault-tolerant, scalable, and high-throughput ingestion of \ndata streams.\n\u2022\t The socket source: The socket source allows a streaming application to read data from a \nTransmission Control Protocol (TCP) socket. It is useful for scenarios where data is sent \nthrough network connections, such as log streaming or data sent by external systems.\n\nStructured Streaming in Spark\n\n\u2022\t The Structured Streaming source: The Structured Streaming source allows developers to \ndefine their own streaming sources by extending the built-in source interfaces. It provides the \nflexibility to integrate with custom or proprietary data sources.\nCustom streaming sources\nIn addition to the built-in streaming sources, Structured Streaming allows developers to create \ncustom streaming sources to ingest data from any system that can be accessed programmatically.", "mimetype": "text/plain", "start_char_idx": 272128, "end_char_idx": 273097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b323cdcc-4d57-498c-82c6-e50f3a92fea0": {"__data__": {"id_": "b323cdcc-4d57-498c-82c6-e50f3a92fea0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0aebecb-ab78-4711-ac52-2b5630666c9f", "node_type": "1", "metadata": {}, "hash": "b08d8cc5a2937f37517a7439fb7500d82f4047c283e94cb91f2a37a718c50030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec9dd294-7f95-489e-8524-054b012640cd", "node_type": "1", "metadata": {}, "hash": "2a38ec6f035a68a13d580aca4b2d1354f0acf50d4b229122f1bcb939694d1c90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming in Spark\n\n\u2022\t The Structured Streaming source: The Structured Streaming source allows developers to \ndefine their own streaming sources by extending the built-in source interfaces. It provides the \nflexibility to integrate with custom or proprietary data sources.\nCustom streaming sources\nIn addition to the built-in streaming sources, Structured Streaming allows developers to create \ncustom streaming sources to ingest data from any system that can be accessed programmatically. \nCustom streaming sources can be implemented by extending the Source interface provided by the \nStructured Streaming API.\nWhen implementing a custom streaming source, developers need to consider aspects such as data \ningestion, event-time management, fault tolerance, and scalability. They must define how data is \nfetched, how it is partitioned and distributed among workers, and how to handle late-arriving data \nand schema evolution.\nSimilar to different streaming sources, we have streaming sinks as well. Let\u2019s explore them next.", "mimetype": "text/plain", "start_char_idx": 272597, "end_char_idx": 273632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec9dd294-7f95-489e-8524-054b012640cd": {"__data__": {"id_": "ec9dd294-7f95-489e-8524-054b012640cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b323cdcc-4d57-498c-82c6-e50f3a92fea0", "node_type": "1", "metadata": {}, "hash": "c9129a7fe28b8cf93468bbc9956ff3e2a48071c1018672a89af14633f64bc9df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88df90ff-1ed7-4b48-a970-0b6f8a6dd027", "node_type": "1", "metadata": {}, "hash": "d1c65dd70ada6e140efcbfc0d155d235a0b1d3e70a0f2dd348c1bbb714fe8afb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When implementing a custom streaming source, developers need to consider aspects such as data \ningestion, event-time management, fault tolerance, and scalability. They must define how data is \nfetched, how it is partitioned and distributed among workers, and how to handle late-arriving data \nand schema evolution.\nSimilar to different streaming sources, we have streaming sinks as well. Let\u2019s explore them next.\nBuilt-in streaming sinks\nStructured Streaming provides built-in support for various streaming sinks, enabling the output of \nprocessed data to different systems. Some of the commonly used built-in streaming sinks include \nthe following:\n\u2022\t The console sink: The console sink writes the output data to the console or standard output. It \nis useful for debugging and quick prototyping but not suitable for production use.\n\u2022\t The file sink: The file sink writes the output data to files in a directory or a file-based system \nsuch as HDFS or Amazon S3.", "mimetype": "text/plain", "start_char_idx": 273220, "end_char_idx": 274182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88df90ff-1ed7-4b48-a970-0b6f8a6dd027": {"__data__": {"id_": "88df90ff-1ed7-4b48-a970-0b6f8a6dd027", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec9dd294-7f95-489e-8524-054b012640cd", "node_type": "1", "metadata": {}, "hash": "2a38ec6f035a68a13d580aca4b2d1354f0acf50d4b229122f1bcb939694d1c90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c4cf198-8c52-47a3-9011-a82b9d8db5a9", "node_type": "1", "metadata": {}, "hash": "81b0e251bcb88a53507505df8b56aa216e47cb8afd7a2909c1cb77228a587a44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some of the commonly used built-in streaming sinks include \nthe following:\n\u2022\t The console sink: The console sink writes the output data to the console or standard output. It \nis useful for debugging and quick prototyping but not suitable for production use.\n\u2022\t The file sink: The file sink writes the output data to files in a directory or a file-based system \nsuch as HDFS or Amazon S3. It allows data to be stored and consumed later for batch processing \nor archival purposes.\n\u2022\t The Kafka sink: The Kafka sink enables you to write data to Apache Kafka topics. It provides \nfault-tolerant, scalable, and high-throughput output to Kafka for consumption by other systems.\n\u2022\t The foreach sink: The foreach sink allows developers to define their own custom sink logic \nby implementing the ForeachWriter interface. It provides the flexibility to write data to \nexternal systems or perform custom operations on the output data.", "mimetype": "text/plain", "start_char_idx": 273795, "end_char_idx": 274718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c4cf198-8c52-47a3-9011-a82b9d8db5a9": {"__data__": {"id_": "7c4cf198-8c52-47a3-9011-a82b9d8db5a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88df90ff-1ed7-4b48-a970-0b6f8a6dd027", "node_type": "1", "metadata": {}, "hash": "d1c65dd70ada6e140efcbfc0d155d235a0b1d3e70a0f2dd348c1bbb714fe8afb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd2c935b-3880-44f9-84ea-49280825005f", "node_type": "1", "metadata": {}, "hash": "7661a5781961cdd713ea2859ce5d618d3915e37a0383f9e232775dd843284a51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It provides \nfault-tolerant, scalable, and high-throughput output to Kafka for consumption by other systems.\n\u2022\t The foreach sink: The foreach sink allows developers to define their own custom sink logic \nby implementing the ForeachWriter interface. It provides the flexibility to write data to \nexternal systems or perform custom operations on the output data.\nCustom streaming sinks\nSimilar to custom streaming sources, developers can implement custom streaming sinks in Structured \nStreaming by extending the Sink interface. There are instances when you would need to write the \ndata back to a system that might not support streaming. It could be a database or a file-based storage \nsystem. Custom streaming sinks enable integration with external systems or databases that are not \nsupported by the built-in sinks.\n\nAdvanced techniques in Structured Streaming\n\nWhen implementing a custom streaming sink, developers need to define how output data is written \nor processed by the external system.", "mimetype": "text/plain", "start_char_idx": 274358, "end_char_idx": 275354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd2c935b-3880-44f9-84ea-49280825005f": {"__data__": {"id_": "dd2c935b-3880-44f9-84ea-49280825005f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c4cf198-8c52-47a3-9011-a82b9d8db5a9", "node_type": "1", "metadata": {}, "hash": "81b0e251bcb88a53507505df8b56aa216e47cb8afd7a2909c1cb77228a587a44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "539f36a1-045e-4dc8-976d-af4d371158a6", "node_type": "1", "metadata": {}, "hash": "541e6ba0b72701f969fb4e12500047cbcb906aac1d90bca3e0ec260848953eda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are instances when you would need to write the \ndata back to a system that might not support streaming. It could be a database or a file-based storage \nsystem. Custom streaming sinks enable integration with external systems or databases that are not \nsupported by the built-in sinks.\n\nAdvanced techniques in Structured Streaming\n\nWhen implementing a custom streaming sink, developers need to define how output data is written \nor processed by the external system. This may involve establishing connections, handling batching \nor buffering, and ensuring fault tolerance and exactly-once semantics.\nIn the next section, we will talk about advanced techniques in Structured Streaming.\nAdvanced techniques in Structured Streaming\nThere are certain built-in capabilities of Structured Streaming that makes it the default choice for even \nsome batch operations. Instead of architecting things yourself, Structured Streaming handles these \nproperties for you. Some of them are as follows.", "mimetype": "text/plain", "start_char_idx": 274885, "end_char_idx": 275872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "539f36a1-045e-4dc8-976d-af4d371158a6": {"__data__": {"id_": "539f36a1-045e-4dc8-976d-af4d371158a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd2c935b-3880-44f9-84ea-49280825005f", "node_type": "1", "metadata": {}, "hash": "7661a5781961cdd713ea2859ce5d618d3915e37a0383f9e232775dd843284a51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d19ecea-21d6-4fec-9603-a2051bdb8078", "node_type": "1", "metadata": {}, "hash": "d87411f145cbefa400898c185a93f931fba34854c76fbc28f66e84738de75508", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the next section, we will talk about advanced techniques in Structured Streaming.\nAdvanced techniques in Structured Streaming\nThere are certain built-in capabilities of Structured Streaming that makes it the default choice for even \nsome batch operations. Instead of architecting things yourself, Structured Streaming handles these \nproperties for you. Some of them are as follows.\nHandling fault tolerance\nFault tolerance is crucial in streaming systems to ensure data integrity and reliability. Structured \nStreaming provides built-in fault tolerance mechanisms to handle failures in both streaming sources \nand sinks:\n\u2022\t Source fault tolerance: Structured Streaming ensures end-to-end fault tolerance in sources, by \ntracking the progress of event time using watermarks and checkpointing the metadata related \nto the stream. If there are failures, the system can recover and resume processing from the last \nconsistent state.\n\u2022\t Sink fault tolerance: Fault tolerance in sinks depends on the guarantees provided by the specific \nsink implementation.", "mimetype": "text/plain", "start_char_idx": 275488, "end_char_idx": 276542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d19ecea-21d6-4fec-9603-a2051bdb8078": {"__data__": {"id_": "5d19ecea-21d6-4fec-9603-a2051bdb8078", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "539f36a1-045e-4dc8-976d-af4d371158a6", "node_type": "1", "metadata": {}, "hash": "541e6ba0b72701f969fb4e12500047cbcb906aac1d90bca3e0ec260848953eda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed", "node_type": "1", "metadata": {}, "hash": "b3e60720461c8fa5efdd72bacf0c93364e3513877e57e64bc6bcfa268be2b0d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If there are failures, the system can recover and resume processing from the last \nconsistent state.\n\u2022\t Sink fault tolerance: Fault tolerance in sinks depends on the guarantees provided by the specific \nsink implementation. Some sinks may inherently provide exactly-once semantics, while others \nmay rely on idempotent writes or deduplication techniques to achieve at-least-once semantics. \nSink implementations should be carefully chosen to ensure data consistency and reliability.\nDevelopers should consider the fault tolerance characteristics of the streaming sources and sinks they \nuse and configure appropriate checkpointing intervals, retention policies, and recovery mechanisms \nto ensure the reliability of their streaming applications.\nStructured Streaming has built-in support for schema evolution as well. Let\u2019s explore that in the \nnext section.\nHandling schema evolution\nStructured Streaming provides support for handling schema evolution in streaming data sources. \nSchema evolution refers to changes in the structure or schema of incoming data over time.", "mimetype": "text/plain", "start_char_idx": 276319, "end_char_idx": 277389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed": {"__data__": {"id_": "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d19ecea-21d6-4fec-9603-a2051bdb8078", "node_type": "1", "metadata": {}, "hash": "d87411f145cbefa400898c185a93f931fba34854c76fbc28f66e84738de75508", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eff11646-636b-4f05-a193-ca69282c17ef", "node_type": "1", "metadata": {}, "hash": "5f0dbb800b0a8e4b07f2b56f52f9d58a6cc4141a81c5d3bc2c1f9bbda84409eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming has built-in support for schema evolution as well. Let\u2019s explore that in the \nnext section.\nHandling schema evolution\nStructured Streaming provides support for handling schema evolution in streaming data sources. \nSchema evolution refers to changes in the structure or schema of incoming data over time.\nStructured Streaming can handle schema evolution by applying the concept of schema inference \nor schema merging. When reading from streaming sources, the initial schema is inferred from the \nincoming data. As the data evolves, subsequent DataFrames are merged with the initial schema, \naccommodating any new or changed fields.", "mimetype": "text/plain", "start_char_idx": 277065, "end_char_idx": 277716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eff11646-636b-4f05-a193-ca69282c17ef": {"__data__": {"id_": "eff11646-636b-4f05-a193-ca69282c17ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed", "node_type": "1", "metadata": {}, "hash": "b3e60720461c8fa5efdd72bacf0c93364e3513877e57e64bc6bcfa268be2b0d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea148f05-65b9-412c-97ff-da12acd35136", "node_type": "1", "metadata": {}, "hash": "3984e9beae567a6abfda1d4842033d0051ed30740a1d003e06c5c5b04a725f24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Schema evolution refers to changes in the structure or schema of incoming data over time.\nStructured Streaming can handle schema evolution by applying the concept of schema inference \nor schema merging. When reading from streaming sources, the initial schema is inferred from the \nincoming data. As the data evolves, subsequent DataFrames are merged with the initial schema, \naccommodating any new or changed fields.\n\nStructured Streaming in Spark\n\nThe following code snippet demonstrates handling schema evolution in Structured Streaming:\nstream = spark.readStream \\\n.format(\"csv\") \\\n.option(\"header\", \"true\") \\\n.schema(initialSchema) \\\n.load(\"data/input\")\nmergedStream = stream \\\n.selectExpr(\"col1\", \"col2\", \"new_col AS col3\")\nIn this example, the initial schema is provided explicitly using the schema method. As new data \narrives with additional fields, such as new_col, it can be selected and merged into the stream using \nthe selectExpr method.", "mimetype": "text/plain", "start_char_idx": 277300, "end_char_idx": 278250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea148f05-65b9-412c-97ff-da12acd35136": {"__data__": {"id_": "ea148f05-65b9-412c-97ff-da12acd35136", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eff11646-636b-4f05-a193-ca69282c17ef", "node_type": "1", "metadata": {}, "hash": "5f0dbb800b0a8e4b07f2b56f52f9d58a6cc4141a81c5d3bc2c1f9bbda84409eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8777d75b-6951-453a-8947-8fcff76542bb", "node_type": "1", "metadata": {}, "hash": "4238f4b3ef751f9815109e2b78d4bb9318411f7c7d0e259bd88ce25d0c83f53a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As new data \narrives with additional fields, such as new_col, it can be selected and merged into the stream using \nthe selectExpr method.\nHandling schema evolution is crucial to ensure compatibility and flexibility in streaming applications \nwhere the data schema may change or evolve over time.\nDifferent joins in Structured Streaming\nOne of the key features of Structured Streaming is its ability to join different types of data streams \ntogether in one sink.\nStream-stream joins\nStream-stream joins, also known as stream-stream co-grouping or stream-stream correlation, \ninvolve joining two or more streaming data sources based on a common key or condition. In this \ntype of join, each incoming event from the streams is matched with events from other streams that \nshare the same key or satisfy the specified condition.\nStream-stream joins enable real-time data correlation and enrichment, making it possible to combine \nmultiple streams of data to gain deeper insights and perform complex analytics.", "mimetype": "text/plain", "start_char_idx": 278113, "end_char_idx": 279117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8777d75b-6951-453a-8947-8fcff76542bb": {"__data__": {"id_": "8777d75b-6951-453a-8947-8fcff76542bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea148f05-65b9-412c-97ff-da12acd35136", "node_type": "1", "metadata": {}, "hash": "3984e9beae567a6abfda1d4842033d0051ed30740a1d003e06c5c5b04a725f24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a5a8151-c060-41a6-a81f-438158a969cd", "node_type": "1", "metadata": {}, "hash": "9b2287e3370530964c78d6854ec956f16cc2a75ee0fb2db67f9e4b120c4cfbec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this \ntype of join, each incoming event from the streams is matched with events from other streams that \nshare the same key or satisfy the specified condition.\nStream-stream joins enable real-time data correlation and enrichment, making it possible to combine \nmultiple streams of data to gain deeper insights and perform complex analytics. However, stream-\nstream joins present unique challenges compared to batch or stream-static joins, due to the unbounded \nnature of streaming data and potential event-time skew.\nOne common approach to stream-stream joins is the use of windowing operations. By defining \noverlapping or tumbling windows on the streams, events within the same window can be joined based \non their keys. Careful consideration of window size, watermarking, and event time characteristics is \nnecessary to ensure accurate and meaningful joins.", "mimetype": "text/plain", "start_char_idx": 278774, "end_char_idx": 279637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a5a8151-c060-41a6-a81f-438158a969cd": {"__data__": {"id_": "6a5a8151-c060-41a6-a81f-438158a969cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8777d75b-6951-453a-8947-8fcff76542bb", "node_type": "1", "metadata": {}, "hash": "4238f4b3ef751f9815109e2b78d4bb9318411f7c7d0e259bd88ce25d0c83f53a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f09da6d-5228-4569-9cf4-e0466584b90e", "node_type": "1", "metadata": {}, "hash": "71f54726046bd555ae6707b9c6222ef50d0e74858cdb0a26159af20bcda7b7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One common approach to stream-stream joins is the use of windowing operations. By defining \noverlapping or tumbling windows on the streams, events within the same window can be joined based \non their keys. Careful consideration of window size, watermarking, and event time characteristics is \nnecessary to ensure accurate and meaningful joins.\nHere\u2019s an example of a stream-stream join using Structured Streaming:\nstream1 = spark.readStream.format(\"kafka\")\n.option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n.option(\"subscribe\", \"topic1\") .load()\n\nDifferent joins in Structured Streaming\n\nstream2 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.\nservers\",\"localhost:9092\") .option(\"subscribe\", \"topic2\") .load()\n joinedStream =stream1.join(stream2, \"common_key\")\nIn this example, two Kafka streams, stream1 and stream2, are read from different topics.", "mimetype": "text/plain", "start_char_idx": 279294, "end_char_idx": 280159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f09da6d-5228-4569-9cf4-e0466584b90e": {"__data__": {"id_": "8f09da6d-5228-4569-9cf4-e0466584b90e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a5a8151-c060-41a6-a81f-438158a969cd", "node_type": "1", "metadata": {}, "hash": "9b2287e3370530964c78d6854ec956f16cc2a75ee0fb2db67f9e4b120c4cfbec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f70dde24-41af-487b-aa84-f3e731a78bd6", "node_type": "1", "metadata": {}, "hash": "42bbe4d4edadcecc227c3ddd399235b257ccc893a2b5f592239135b0426ffc97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "servers\",\"localhost:9092\") .option(\"subscribe\", \"topic2\") .load()\n joinedStream =stream1.join(stream2, \"common_key\")\nIn this example, two Kafka streams, stream1 and stream2, are read from different topics. The \njoin method is then applied to perform the join operation, based on the common_key field shared \nby both streams.\nStream-static joins\nStream-static joins, also known as stream-batch joins, involve joining a streaming data source to a \nstatic or reference dataset. The static dataset typically represents reference data, such as configuration \ndata or dimension tables, that remains constant over time.\nStream-static joins are useful for enriching streaming data with additional information or attributes \nfrom the static dataset. For example, you might join a stream of user activity events with a static user \nprofile table to enrich each event with user-related details.", "mimetype": "text/plain", "start_char_idx": 279954, "end_char_idx": 280837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f70dde24-41af-487b-aa84-f3e731a78bd6": {"__data__": {"id_": "f70dde24-41af-487b-aa84-f3e731a78bd6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f09da6d-5228-4569-9cf4-e0466584b90e", "node_type": "1", "metadata": {}, "hash": "71f54726046bd555ae6707b9c6222ef50d0e74858cdb0a26159af20bcda7b7cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5", "node_type": "1", "metadata": {}, "hash": "79f58ab16a74f1218232f9b4910c3d87700a1b4765979e36cab672cfbf343f69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The static dataset typically represents reference data, such as configuration \ndata or dimension tables, that remains constant over time.\nStream-static joins are useful for enriching streaming data with additional information or attributes \nfrom the static dataset. For example, you might join a stream of user activity events with a static user \nprofile table to enrich each event with user-related details.\nTo perform a stream-static join in Structured Streaming, you can load the static dataset as a static \nDataFrame and then use the join method to perform the join with the streaming DataFrame. Since \nthe static dataset does not change, the join operation can be performed using the default \u201cright outer \njoin\u201d mode.", "mimetype": "text/plain", "start_char_idx": 280429, "end_char_idx": 281151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5": {"__data__": {"id_": "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f70dde24-41af-487b-aa84-f3e731a78bd6", "node_type": "1", "metadata": {}, "hash": "42bbe4d4edadcecc227c3ddd399235b257ccc893a2b5f592239135b0426ffc97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35d17a88-58e9-4ca6-b300-752255957f9f", "node_type": "1", "metadata": {}, "hash": "153a11418d6be5f3a5b633e5f6e689663590f4f718c6eaafc8c3e96045b4769c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To perform a stream-static join in Structured Streaming, you can load the static dataset as a static \nDataFrame and then use the join method to perform the join with the streaming DataFrame. Since \nthe static dataset does not change, the join operation can be performed using the default \u201cright outer \njoin\u201d mode.\nHere\u2019s an example of a stream-static join in Structured Streaming:\nstream =spark.readStream.format(\"kafka\")\n.option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n.option(\"subscribe\", \"topic\") .load()\nstaticData = spark.read.format(\"csv\") .option(\"header\", \"true\") \n.load(\"data/static_data.csv\")\n enrichedStream = stream.join(staticData,\"common_key\")\nIn this example, the streaming data is read from a Kafka source, and the static dataset is loaded from \na CSV file. The join method is then used to perform the stream-static join based on the \u201ccommon_\nkey\u201d field.", "mimetype": "text/plain", "start_char_idx": 280838, "end_char_idx": 281714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35d17a88-58e9-4ca6-b300-752255957f9f": {"__data__": {"id_": "35d17a88-58e9-4ca6-b300-752255957f9f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5", "node_type": "1", "metadata": {}, "hash": "79f58ab16a74f1218232f9b4910c3d87700a1b4765979e36cab672cfbf343f69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e63f8e2-d303-48b0-bfca-38a585648486", "node_type": "1", "metadata": {}, "hash": "2df677306ca79182f688c6b6f06b582efb43da07b3d830d92ec8b1f471083dc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The join method is then used to perform the stream-static join based on the \u201ccommon_\nkey\u201d field.\nBoth stream-stream and stream-static joins provide powerful capabilities for real-time data analysis \nand enrichment. When using these join operations, it is essential to carefully manage event time \ncharacteristics, windowing options, and data consistency to ensure accurate and reliable results. \nAdditionally, performance considerations should be considered to handle large volumes of data and \nmeet low-latency requirements in real-time streaming applications.\n\nStructured Streaming in Spark\n\nFinal thoughts and future developments\nStructured Streaming has emerged as a powerful framework for real-time data processing in Apache \nSpark. Its unified programming model, fault tolerance, and seamless integration with the Spark \necosystem make it an attractive choice for building scalable and robust streaming applications.\nAs Structured Streaming continues to evolve, there are several areas that hold promise for future \ndevelopments.", "mimetype": "text/plain", "start_char_idx": 281618, "end_char_idx": 282653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e63f8e2-d303-48b0-bfca-38a585648486": {"__data__": {"id_": "2e63f8e2-d303-48b0-bfca-38a585648486", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35d17a88-58e9-4ca6-b300-752255957f9f", "node_type": "1", "metadata": {}, "hash": "153a11418d6be5f3a5b633e5f6e689663590f4f718c6eaafc8c3e96045b4769c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd3bd027-5004-4674-be09-1c9a43db5dac", "node_type": "1", "metadata": {}, "hash": "8cec2cf72b531035e7c6177b3320365c878ec3d9bc5b6e50aaca5bfe28168847", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured Streaming in Spark\n\nFinal thoughts and future developments\nStructured Streaming has emerged as a powerful framework for real-time data processing in Apache \nSpark. Its unified programming model, fault tolerance, and seamless integration with the Spark \necosystem make it an attractive choice for building scalable and robust streaming applications.\nAs Structured Streaming continues to evolve, there are several areas that hold promise for future \ndevelopments. These include the following:\n\u2022\t Enhanced support for streaming sources and sinks: Providing more built-in connectors for \npopular streaming systems and databases, as well as improving the integration and compatibility \nwith custom sources and sinks.\n\u2022\t Advanced event time handling: Introducing more advanced features for event time handling, \nincluding support for event-time skew detection and handling, event deduplication, and \nwatermark optimizations.\n\u2022\t Performance optimization: Continuously improving the performance of Structured Streaming, \nespecially in scenarios with high data volumes and complex computations.", "mimetype": "text/plain", "start_char_idx": 282181, "end_char_idx": 283277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd3bd027-5004-4674-be09-1c9a43db5dac": {"__data__": {"id_": "cd3bd027-5004-4674-be09-1c9a43db5dac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e63f8e2-d303-48b0-bfca-38a585648486", "node_type": "1", "metadata": {}, "hash": "2df677306ca79182f688c6b6f06b582efb43da07b3d830d92ec8b1f471083dc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca121c4e-3189-4cb6-bd61-1a6333682543", "node_type": "1", "metadata": {}, "hash": "bc63f9d7caef605ffba33ac19c7d9bbad575f75dee4ee2854ad461fa2b8da7f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Advanced event time handling: Introducing more advanced features for event time handling, \nincluding support for event-time skew detection and handling, event deduplication, and \nwatermark optimizations.\n\u2022\t Performance optimization: Continuously improving the performance of Structured Streaming, \nespecially in scenarios with high data volumes and complex computations. This could involve \noptimizations in memory management, query planning, and query optimization techniques.\n\u2022\t Integration with AI and machine learning: Further integrating Structured Streaming with \nAI and machine learning libraries in Spark, such as MLlib and TensorFlow, to enable real-time \nmachine learning and predictive analytics on streaming data.\n\u2022\t Seamless integration with streaming data warehouses: Providing better integration with \nstreaming data warehouses or data lakes, such as Apache Iceberg or Delta Lake, to enable \nscalable and efficient storage and the querying of streaming data.\nIn conclusion, Structured Streaming offers a modern and expressive approach to real-time data \nprocessing in Apache Spark.", "mimetype": "text/plain", "start_char_idx": 282904, "end_char_idx": 284003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca121c4e-3189-4cb6-bd61-1a6333682543": {"__data__": {"id_": "ca121c4e-3189-4cb6-bd61-1a6333682543", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd3bd027-5004-4674-be09-1c9a43db5dac", "node_type": "1", "metadata": {}, "hash": "8cec2cf72b531035e7c6177b3320365c878ec3d9bc5b6e50aaca5bfe28168847", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54e1a5c8-4834-4780-9a19-88880306b6f9", "node_type": "1", "metadata": {}, "hash": "ac7021cca2790064c6ad7a068f44a391ed96f036369323ec638b5719e6781104", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Seamless integration with streaming data warehouses: Providing better integration with \nstreaming data warehouses or data lakes, such as Apache Iceberg or Delta Lake, to enable \nscalable and efficient storage and the querying of streaming data.\nIn conclusion, Structured Streaming offers a modern and expressive approach to real-time data \nprocessing in Apache Spark. Its ease of use, scalability, fault tolerance, and integration with the Spark \necosystem make it a valuable tool for building robust and scalable streaming applications. By leveraging \nthe concepts and techniques covered in this chapter, developers can unlock the full potential of real-\ntime data processing with Structured Streaming.\nSummary\nThroughout this chapter, we have explored the fundamental concepts and advanced techniques in \nStructured Streaming.\nWe started by understanding the fundamentals of Structured Streaming, its advantages, and the core \nconcepts that underpin its operation. Then, we talked about Spark Streaming and what it has to offer.", "mimetype": "text/plain", "start_char_idx": 283633, "end_char_idx": 284666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54e1a5c8-4834-4780-9a19-88880306b6f9": {"__data__": {"id_": "54e1a5c8-4834-4780-9a19-88880306b6f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca121c4e-3189-4cb6-bd61-1a6333682543", "node_type": "1", "metadata": {}, "hash": "bc63f9d7caef605ffba33ac19c7d9bbad575f75dee4ee2854ad461fa2b8da7f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7", "node_type": "1", "metadata": {}, "hash": "a39f79c28078a946124294eaf0b1e3afd162fd3bb90e5e90a546ab4e661c515c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By leveraging \nthe concepts and techniques covered in this chapter, developers can unlock the full potential of real-\ntime data processing with Structured Streaming.\nSummary\nThroughout this chapter, we have explored the fundamental concepts and advanced techniques in \nStructured Streaming.\nWe started by understanding the fundamentals of Structured Streaming, its advantages, and the core \nconcepts that underpin its operation. Then, we talked about Spark Streaming and what it has to offer.\n\nSummary\n\nAfter that, we dived into the core functionalities of Structured Streaming. Then, we further delved \ninto advanced topics, such as windowed operations in Structured Streaming. We explored sliding and \ntumbling windows, which enable us to perform aggregations and computations over a specified time \nwindow, allowing for time-based analysis of the streaming data.", "mimetype": "text/plain", "start_char_idx": 284174, "end_char_idx": 285039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7": {"__data__": {"id_": "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54e1a5c8-4834-4780-9a19-88880306b6f9", "node_type": "1", "metadata": {}, "hash": "ac7021cca2790064c6ad7a068f44a391ed96f036369323ec638b5719e6781104", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7f2ee58-0602-49fd-8efd-e75054637752", "node_type": "1", "metadata": {}, "hash": "f79ef63320a4ef6e65e9b7e39b7d74a84657ece1e1aa7da7f251f76a03db15bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, we talked about Spark Streaming and what it has to offer.\n\nSummary\n\nAfter that, we dived into the core functionalities of Structured Streaming. Then, we further delved \ninto advanced topics, such as windowed operations in Structured Streaming. We explored sliding and \ntumbling windows, which enable us to perform aggregations and computations over a specified time \nwindow, allowing for time-based analysis of the streaming data. Additionally, we explored stateful \nstreaming processing, which involves maintaining and updating state in streaming applications and \nintegrating external libraries and APIs to enhance the capabilities of Structured Streaming.\nFinally, we explored emerging trends in real-time data processing and concluded the chapter by \nsummarizing the key takeaways and insights gained.\nIn the next chapter, we will look into machine learning techniques and how to use Spark with \nmachine learning.\n\n\nMachine Learning \nwith Spark ML\nMachine learning has gained popularity in recent times.", "mimetype": "text/plain", "start_char_idx": 284603, "end_char_idx": 285616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7f2ee58-0602-49fd-8efd-e75054637752": {"__data__": {"id_": "b7f2ee58-0602-49fd-8efd-e75054637752", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7", "node_type": "1", "metadata": {}, "hash": "a39f79c28078a946124294eaf0b1e3afd162fd3bb90e5e90a546ab4e661c515c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415", "node_type": "1", "metadata": {}, "hash": "d114f4cf8a649c6ca368e0040657063cb3c0c3db1c714f47321ff56464f152ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we explored emerging trends in real-time data processing and concluded the chapter by \nsummarizing the key takeaways and insights gained.\nIn the next chapter, we will look into machine learning techniques and how to use Spark with \nmachine learning.\n\n\nMachine Learning \nwith Spark ML\nMachine learning has gained popularity in recent times. In this chapter, we will do a comprehensive \nexploration of Spark Machine Learning (ML), a powerful framework for scalable ML on Apache Spark. \nWe will delve into the foundational concepts of ML and how Spark ML leverages these principles to \nenable efficient and scalable data-driven insights.\nWe will cover the following topics:\n\u2022\t Key concepts in ML\n\u2022\t Different types of ML\n\u2022\t ML with Spark\n\u2022\t Considering the ML life cycle with the help of a real-world example\n\u2022\t Different case studies for ML\n\u2022\t Future trends in Spark ML and distributed ML\nML encompasses diverse methodologies tailored to different data scenarios.", "mimetype": "text/plain", "start_char_idx": 285268, "end_char_idx": 286238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415": {"__data__": {"id_": "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7f2ee58-0602-49fd-8efd-e75054637752", "node_type": "1", "metadata": {}, "hash": "f79ef63320a4ef6e65e9b7e39b7d74a84657ece1e1aa7da7f251f76a03db15bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58", "node_type": "1", "metadata": {}, "hash": "d0b53646bc067c3b8b18dde4b923f0c20280e8b576542fba29bcc1f4f58d35a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will cover the following topics:\n\u2022\t Key concepts in ML\n\u2022\t Different types of ML\n\u2022\t ML with Spark\n\u2022\t Considering the ML life cycle with the help of a real-world example\n\u2022\t Different case studies for ML\n\u2022\t Future trends in Spark ML and distributed ML\nML encompasses diverse methodologies tailored to different data scenarios. We will start by learning \nabout different key concepts in ML.\nIntroduction to ML\nML is a field of study that focuses on the development of algorithms and models that enable computer \nsystems to learn and make predictions or decisions without being explicitly programmed. It is a subset \nof artificial intelligence (AI) that aims to provide systems with the ability to automatically learn and \nimprove from data and experience.", "mimetype": "text/plain", "start_char_idx": 285912, "end_char_idx": 286666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58": {"__data__": {"id_": "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415", "node_type": "1", "metadata": {}, "hash": "d114f4cf8a649c6ca368e0040657063cb3c0c3db1c714f47321ff56464f152ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd", "node_type": "1", "metadata": {}, "hash": "eb250a3c3e6ff956a13cb36a5e6063f81ee704060f7b85cc3c503cd3e51c444b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will start by learning \nabout different key concepts in ML.\nIntroduction to ML\nML is a field of study that focuses on the development of algorithms and models that enable computer \nsystems to learn and make predictions or decisions without being explicitly programmed. It is a subset \nof artificial intelligence (AI) that aims to provide systems with the ability to automatically learn and \nimprove from data and experience.\n\nMachine Learning with Spark ML\n\nIn today\u2019s world, where vast amounts of data are being generated at an unprecedented rate, ML plays a \ncritical role in extracting meaningful insights, making accurate predictions, and automating decision-\nmaking processes. As data grows, machines can learn the patterns better, thus making it even easier \nto gain insights from this data. It finds applications in various domains, including finance, healthcare, \nmarketing, image and speech recognition, recommendation systems, and many more.", "mimetype": "text/plain", "start_char_idx": 286239, "end_char_idx": 287193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd": {"__data__": {"id_": "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58", "node_type": "1", "metadata": {}, "hash": "d0b53646bc067c3b8b18dde4b923f0c20280e8b576542fba29bcc1f4f58d35a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b57fb07-0350-4055-a7fa-240752a182d6", "node_type": "1", "metadata": {}, "hash": "1a6a87fd8f16a6e6614da06e75f9de04ef9e1d57b116cd7f141a5a14ce953500", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As data grows, machines can learn the patterns better, thus making it even easier \nto gain insights from this data. It finds applications in various domains, including finance, healthcare, \nmarketing, image and speech recognition, recommendation systems, and many more.\nThe key concepts of ML\nTo understand ML, it is important to grasp the fundamental concepts that underpin its methodology.\nData\nData is the foundation of any ML process. It can be structured, semi-structured, or unstructured and \nencompasses various types, such as numerical, categorical, text, images, and more. ML algorithms \nrequire high-quality, relevant, and representative data to learn patterns and make accurate predictions. \nWhen dealing with ML problems, it is imperative to have data that can answer the question that \nwe\u2019re trying to solve. The quality of data used in any analysis or model-building process significantly \nimpacts the outcomes and decisions derived from it.", "mimetype": "text/plain", "start_char_idx": 286924, "end_char_idx": 287879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b57fb07-0350-4055-a7fa-240752a182d6": {"__data__": {"id_": "8b57fb07-0350-4055-a7fa-240752a182d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd", "node_type": "1", "metadata": {}, "hash": "eb250a3c3e6ff956a13cb36a5e6063f81ee704060f7b85cc3c503cd3e51c444b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f9e688-d85a-4691-96f1-5d1b875e76b4", "node_type": "1", "metadata": {}, "hash": "d35ec4150cf7fb4210507f6a25899974d87a52fefc634bcbb8ec951e9b9c6245", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ML algorithms \nrequire high-quality, relevant, and representative data to learn patterns and make accurate predictions. \nWhen dealing with ML problems, it is imperative to have data that can answer the question that \nwe\u2019re trying to solve. The quality of data used in any analysis or model-building process significantly \nimpacts the outcomes and decisions derived from it. Bad or poor-quality data can lead to inaccurate, \nunreliable, or misleading results, ultimately affecting the overall performance and credibility of any \nanalysis or model.\nAn ML model trained on bad data is likely to make inaccurate predictions or classifications. For \ninstance, a model trained on incomplete or biased data might incorrectly identify loyal customers as \npotential churners or vice versa.\nDecision-makers relying on flawed or biased analysis derived from bad data might implement strategies \nbased on inaccurate insights. For instance, marketing campaigns targeting the wrong customer \nsegments due to flawed churn predictions can lead to wasted resources and missed opportunities.", "mimetype": "text/plain", "start_char_idx": 287506, "end_char_idx": 288579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16f9e688-d85a-4691-96f1-5d1b875e76b4": {"__data__": {"id_": "16f9e688-d85a-4691-96f1-5d1b875e76b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b57fb07-0350-4055-a7fa-240752a182d6", "node_type": "1", "metadata": {}, "hash": "1a6a87fd8f16a6e6614da06e75f9de04ef9e1d57b116cd7f141a5a14ce953500", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ecd8fee-c08e-4d27-8246-aad6f761d987", "node_type": "1", "metadata": {}, "hash": "f71e1cfa7294d842a11919d23ad5a12caa5178444d1ada1a6059ac6966a8722c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An ML model trained on bad data is likely to make inaccurate predictions or classifications. For \ninstance, a model trained on incomplete or biased data might incorrectly identify loyal customers as \npotential churners or vice versa.\nDecision-makers relying on flawed or biased analysis derived from bad data might implement strategies \nbased on inaccurate insights. For instance, marketing campaigns targeting the wrong customer \nsegments due to flawed churn predictions can lead to wasted resources and missed opportunities.\nTherefore, we need to make sure that the data we\u2019re using for ML problems is representative of the \npopulation that we want to build the models for. The other thing to note is that data might have some \ninherent biases in it. It is our responsibility to look for them and be aware of them when using this \ndata to build ML models.\nFeatures\nFeatures are the measurable properties or characteristics of the data that the ML algorithm uses to make \npredictions or decisions.", "mimetype": "text/plain", "start_char_idx": 288053, "end_char_idx": 289051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ecd8fee-c08e-4d27-8246-aad6f761d987": {"__data__": {"id_": "4ecd8fee-c08e-4d27-8246-aad6f761d987", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16f9e688-d85a-4691-96f1-5d1b875e76b4", "node_type": "1", "metadata": {}, "hash": "d35ec4150cf7fb4210507f6a25899974d87a52fefc634bcbb8ec951e9b9c6245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83a8bf1c-3073-45ec-8558-8ffd210cd308", "node_type": "1", "metadata": {}, "hash": "caafbfc2afe5cb3bf7a8feb5b44243c06a43100b72df1f8ef0e07a75de61d28a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The other thing to note is that data might have some \ninherent biases in it. It is our responsibility to look for them and be aware of them when using this \ndata to build ML models.\nFeatures\nFeatures are the measurable properties or characteristics of the data that the ML algorithm uses to make \npredictions or decisions. They are the variables or attributes that capture the relevant information from \nthe data. Out of the vast amounts of data that are present, we want to understand which features of this \ndata would be useful for solving a particular problem. Relevant features would generate better models.\nFeature engineering, the process of selecting, extracting, and transforming features, plays a crucial \nrole in improving the performance of ML models.\n\nIntroduction to ML\n\nLabels and targets\nLabels or targets are the desired outputs or outcomes that the ML model aims to predict or classify.", "mimetype": "text/plain", "start_char_idx": 288729, "end_char_idx": 289633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83a8bf1c-3073-45ec-8558-8ffd210cd308": {"__data__": {"id_": "83a8bf1c-3073-45ec-8558-8ffd210cd308", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ecd8fee-c08e-4d27-8246-aad6f761d987", "node_type": "1", "metadata": {}, "hash": "f71e1cfa7294d842a11919d23ad5a12caa5178444d1ada1a6059ac6966a8722c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14", "node_type": "1", "metadata": {}, "hash": "9f13b5d508b0514c12f55ce70d7b1e239a209dd716062afe8047da3dd391e293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Relevant features would generate better models.\nFeature engineering, the process of selecting, extracting, and transforming features, plays a crucial \nrole in improving the performance of ML models.\n\nIntroduction to ML\n\nLabels and targets\nLabels or targets are the desired outputs or outcomes that the ML model aims to predict or classify. \nIn supervised learning, where the model learns from labeled data, the labels represent the correct \nanswers or class labels associated with the input data. In unsupervised learning, the model identifies \npatterns or clusters in the data without any explicit labels.\nTraining and testing\nIn ML, models are trained using a subset of the available data, which is called the training set. The \ntraining process involves feeding the input data and corresponding labels to the model, which learns \nfrom this data to make predictions. Once the model is trained, its performance is evaluated using a \nseparate subset of data called the testing set.", "mimetype": "text/plain", "start_char_idx": 289294, "end_char_idx": 290275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14": {"__data__": {"id_": "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83a8bf1c-3073-45ec-8558-8ffd210cd308", "node_type": "1", "metadata": {}, "hash": "caafbfc2afe5cb3bf7a8feb5b44243c06a43100b72df1f8ef0e07a75de61d28a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccaa9034-6d31-4c57-9751-64c8355f1f01", "node_type": "1", "metadata": {}, "hash": "42b18287de868aef53f62e455fcc49fda2d4506cff4838f61ba122141541e188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training and testing\nIn ML, models are trained using a subset of the available data, which is called the training set. The \ntraining process involves feeding the input data and corresponding labels to the model, which learns \nfrom this data to make predictions. Once the model is trained, its performance is evaluated using a \nseparate subset of data called the testing set. This evaluation helps assess the model\u2019s ability to generalize \nand make accurate predictions on unseen data.\nAlgorithms and models\nML algorithms are mathematical or statistical procedures that learn patterns and relationships in \nthe data and make predictions or decisions. They can be categorized into various types, including \nregression, classification, clustering, dimensionality reduction, and reinforcement learning. These \nalgorithms, when trained on data, generate models that capture the learned patterns and can be used \nto make predictions on new, unseen data.\nDiscussing different ML algorithms in depth is beyond the scope of this book.", "mimetype": "text/plain", "start_char_idx": 289901, "end_char_idx": 290926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccaa9034-6d31-4c57-9751-64c8355f1f01": {"__data__": {"id_": "ccaa9034-6d31-4c57-9751-64c8355f1f01", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14", "node_type": "1", "metadata": {}, "hash": "9f13b5d508b0514c12f55ce70d7b1e239a209dd716062afe8047da3dd391e293", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad", "node_type": "1", "metadata": {}, "hash": "555298e4330cec1b7f5873d02f3e5831381b944e1dfbf6413eebe1f38e7056af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They can be categorized into various types, including \nregression, classification, clustering, dimensionality reduction, and reinforcement learning. These \nalgorithms, when trained on data, generate models that capture the learned patterns and can be used \nto make predictions on new, unseen data.\nDiscussing different ML algorithms in depth is beyond the scope of this book. We will talk about \ndifferent types of ML problems in the next section.\nTypes of ML\nML problems can be broadly categorized into two distinct categories. In this section, we will explore \nboth of them.\nSupervised learning\nSupervised learning is a type of ML where the algorithm learns from labeled training data to make \npredictions or decisions. In supervised learning, the training data consists of input features and \ncorresponding output labels or target values. The goal is to learn a mapping function that can accurately \npredict the output for new, unseen inputs.", "mimetype": "text/plain", "start_char_idx": 290551, "end_char_idx": 291496, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad": {"__data__": {"id_": "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccaa9034-6d31-4c57-9751-64c8355f1f01", "node_type": "1", "metadata": {}, "hash": "42b18287de868aef53f62e455fcc49fda2d4506cff4838f61ba122141541e188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61fdc6f7-900d-49ea-83f6-c19c89253afa", "node_type": "1", "metadata": {}, "hash": "639a5208a62abf6fbf23c34875107c427a30b9e078968b1ee15362d5767d2d90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we will explore \nboth of them.\nSupervised learning\nSupervised learning is a type of ML where the algorithm learns from labeled training data to make \npredictions or decisions. In supervised learning, the training data consists of input features and \ncorresponding output labels or target values. The goal is to learn a mapping function that can accurately \npredict the output for new, unseen inputs.\nThe process of supervised learning involves the following steps:\n1.\t\nData preparation: The first step is to collect and preprocess the training data. This includes \ncleaning the data, handling missing values, and transforming the data into a suitable format \nfor the learning algorithm. The data should be split into features (input variables) and labels \n(output variables).\n\nMachine Learning with Spark ML\n\n2.\t\nModel training: Once the data has been prepared, the supervised learning algorithm is trained \non the labeled training data.", "mimetype": "text/plain", "start_char_idx": 291080, "end_char_idx": 292034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61fdc6f7-900d-49ea-83f6-c19c89253afa": {"__data__": {"id_": "61fdc6f7-900d-49ea-83f6-c19c89253afa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad", "node_type": "1", "metadata": {}, "hash": "555298e4330cec1b7f5873d02f3e5831381b944e1dfbf6413eebe1f38e7056af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d556006-0617-495c-b540-7fb518542a6b", "node_type": "1", "metadata": {}, "hash": "0a79be61354f3f3835c8523ef76bb9b5eba82a60e23dc984e4776d1d808f1c8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This includes \ncleaning the data, handling missing values, and transforming the data into a suitable format \nfor the learning algorithm. The data should be split into features (input variables) and labels \n(output variables).\n\nMachine Learning with Spark ML\n\n2.\t\nModel training: Once the data has been prepared, the supervised learning algorithm is trained \non the labeled training data. The algorithm learns the patterns and relationships between the \ninput features and the corresponding output labels. The goal is to find a model that can generalize \nwell to unseen data and make accurate predictions.\n3.\t\nModel evaluation: After training the model, it needs to be evaluated to assess its performance. \nThis is done using a separate set of data called the testing or validation set. The model\u2019s predictions \nare compared with the actual labels in the testing set, and various evaluation metrics such as \naccuracy, precision, recall, or mean squared error are calculated.\n4.", "mimetype": "text/plain", "start_char_idx": 291647, "end_char_idx": 292623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d556006-0617-495c-b540-7fb518542a6b": {"__data__": {"id_": "9d556006-0617-495c-b540-7fb518542a6b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61fdc6f7-900d-49ea-83f6-c19c89253afa", "node_type": "1", "metadata": {}, "hash": "639a5208a62abf6fbf23c34875107c427a30b9e078968b1ee15362d5767d2d90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6", "node_type": "1", "metadata": {}, "hash": "824094064465d0689664bae333a4f415bde8fca4eb7c0d78d4b2a3101b90e01f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.\t\nModel evaluation: After training the model, it needs to be evaluated to assess its performance. \nThis is done using a separate set of data called the testing or validation set. The model\u2019s predictions \nare compared with the actual labels in the testing set, and various evaluation metrics such as \naccuracy, precision, recall, or mean squared error are calculated.\n4.\t\nModel deployment and prediction: Once the model is trained and evaluated, it can be deployed \nto make predictions on new, unseen data. The trained model takes the input features of the new \ndata and produces predictions or decisions based on what it has learned during the training phase.\nExamples of supervised learning algorithms include linear regression, logistic regression, support \nvector machines (SVM), decision trees, random forests, gradient boosting, and neural networks. \nAgain, going in-depth on these algorithms is beyond the scope of this book.", "mimetype": "text/plain", "start_char_idx": 292252, "end_char_idx": 293185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6": {"__data__": {"id_": "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d556006-0617-495c-b540-7fb518542a6b", "node_type": "1", "metadata": {}, "hash": "0a79be61354f3f3835c8523ef76bb9b5eba82a60e23dc984e4776d1d808f1c8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60fc0fb3-cc6b-40dc-b835-f62bdd50414b", "node_type": "1", "metadata": {}, "hash": "ee406ad4c630c33aa8f4ccc9d45fe0ec6315818234243a944087d2e3d6e0d195", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The trained model takes the input features of the new \ndata and produces predictions or decisions based on what it has learned during the training phase.\nExamples of supervised learning algorithms include linear regression, logistic regression, support \nvector machines (SVM), decision trees, random forests, gradient boosting, and neural networks. \nAgain, going in-depth on these algorithms is beyond the scope of this book. You can read more \nabout them here: https://spark.apache.org/docs/latest/ml-classification-\nregression.html.\nUnsupervised Learning\nUnsupervised learning is a type of ML where the algorithm learns patterns and relationships in the \ndata without any labeled output. In unsupervised learning, the training data consists only of input \nfeatures, and the goal is to discover hidden patterns, structures, or clusters within the data.\nThe process of unsupervised learning involves the following steps:\n1.", "mimetype": "text/plain", "start_char_idx": 292760, "end_char_idx": 293683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60fc0fb3-cc6b-40dc-b835-f62bdd50414b": {"__data__": {"id_": "60fc0fb3-cc6b-40dc-b835-f62bdd50414b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6", "node_type": "1", "metadata": {}, "hash": "824094064465d0689664bae333a4f415bde8fca4eb7c0d78d4b2a3101b90e01f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c794b653-5beb-423d-b311-fd822b2117e9", "node_type": "1", "metadata": {}, "hash": "d77a96776245e30a4ddaddc9f302206b4e01b09b513030e3c951c58fd1ad6fa4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unsupervised Learning\nUnsupervised learning is a type of ML where the algorithm learns patterns and relationships in the \ndata without any labeled output. In unsupervised learning, the training data consists only of input \nfeatures, and the goal is to discover hidden patterns, structures, or clusters within the data.\nThe process of unsupervised learning involves the following steps:\n1.\t\nData preparation: Similar to supervised learning, the first step is to collect and preprocess the \ndata. However, in unsupervised learning, there are no labeled output values or target variables. \nThe data is transformed and prepared in a way that it\u2019s suitable for the specific unsupervised \nlearning algorithm.\n2.\t\nModel training: In unsupervised learning, the algorithm is trained on the input features \nwithout any specific target variable. The algorithm explores the data and identifies patterns or \nclusters based on statistical properties or similarity measures.", "mimetype": "text/plain", "start_char_idx": 293295, "end_char_idx": 294254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c794b653-5beb-423d-b311-fd822b2117e9": {"__data__": {"id_": "c794b653-5beb-423d-b311-fd822b2117e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60fc0fb3-cc6b-40dc-b835-f62bdd50414b", "node_type": "1", "metadata": {}, "hash": "ee406ad4c630c33aa8f4ccc9d45fe0ec6315818234243a944087d2e3d6e0d195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "165f9ba9-22f6-4923-8d69-29e47af3a02b", "node_type": "1", "metadata": {}, "hash": "5c56904e543cfe3dd4e28c9b061aec9a7e2117f21c8d89197dd6743a426b6e9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in unsupervised learning, there are no labeled output values or target variables. \nThe data is transformed and prepared in a way that it\u2019s suitable for the specific unsupervised \nlearning algorithm.\n2.\t\nModel training: In unsupervised learning, the algorithm is trained on the input features \nwithout any specific target variable. The algorithm explores the data and identifies patterns or \nclusters based on statistical properties or similarity measures. The goal is to extract meaningful \ninformation from the data without any predefined labels.\n3.\t\nModel evaluation (optional): Unlike supervised learning, unsupervised learning does not \nhave a direct evaluation metric based on known labels. Evaluation in unsupervised learning is \noften subjective and depends on the specific task or problem domain. It is also a more manual \nprocess than it is in supervised learning.", "mimetype": "text/plain", "start_char_idx": 293790, "end_char_idx": 294672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "165f9ba9-22f6-4923-8d69-29e47af3a02b": {"__data__": {"id_": "165f9ba9-22f6-4923-8d69-29e47af3a02b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c794b653-5beb-423d-b311-fd822b2117e9", "node_type": "1", "metadata": {}, "hash": "d77a96776245e30a4ddaddc9f302206b4e01b09b513030e3c951c58fd1ad6fa4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5965ad36-a7d6-460c-b76f-621eb3f6cf67", "node_type": "1", "metadata": {}, "hash": "de2320e00045ec130fb40fb3e230a68aa740a2af9c849f6eff62a385d1c5e19c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The goal is to extract meaningful \ninformation from the data without any predefined labels.\n3.\t\nModel evaluation (optional): Unlike supervised learning, unsupervised learning does not \nhave a direct evaluation metric based on known labels. Evaluation in unsupervised learning is \noften subjective and depends on the specific task or problem domain. It is also a more manual \nprocess than it is in supervised learning. Evaluation can involve visualizing the discovered \nclusters, assessing the quality of dimensionality reduction, or using domain knowledge to \nvalidate the results.\n\nIntroduction to ML\n\n4.\t\nPattern discovery and insights: The primary objective of unsupervised learning is to discover \nhidden patterns, structures, or clusters in the data. Unsupervised learning algorithms can reveal \ninsights about the data, identify anomalies or outliers, perform dimensionality reduction, or \ngenerate recommendations.", "mimetype": "text/plain", "start_char_idx": 294255, "end_char_idx": 295176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5965ad36-a7d6-460c-b76f-621eb3f6cf67": {"__data__": {"id_": "5965ad36-a7d6-460c-b76f-621eb3f6cf67", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "165f9ba9-22f6-4923-8d69-29e47af3a02b", "node_type": "1", "metadata": {}, "hash": "5c56904e543cfe3dd4e28c9b061aec9a7e2117f21c8d89197dd6743a426b6e9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd075574-8afe-417e-bb55-a7d0701dc1af", "node_type": "1", "metadata": {}, "hash": "8d73dca3cce14af4d8502d4067e1dc12c9c33de6219a5fb8a8178e8852c0960f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction to ML\n\n4.\t\nPattern discovery and insights: The primary objective of unsupervised learning is to discover \nhidden patterns, structures, or clusters in the data. Unsupervised learning algorithms can reveal \ninsights about the data, identify anomalies or outliers, perform dimensionality reduction, or \ngenerate recommendations.\nExamples of unsupervised learning algorithms include K-means clustering, hierarchical clustering, \nprincipal component analysis (PCA), association rule mining, and self-organizing maps (SOM).\nIn conclusion, supervised learning and unsupervised learning are two key types of ML. Supervised \nlearning relies on labeled data to learn patterns and make predictions, while unsupervised learning \ndiscovers patterns and structures in unlabeled data. Both types have their own set of algorithms and \ntechniques, as well as different choices. Discussing unsupervised learning in depth is beyond the \nscope of this book.", "mimetype": "text/plain", "start_char_idx": 294838, "end_char_idx": 295788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd075574-8afe-417e-bb55-a7d0701dc1af": {"__data__": {"id_": "fd075574-8afe-417e-bb55-a7d0701dc1af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5965ad36-a7d6-460c-b76f-621eb3f6cf67", "node_type": "1", "metadata": {}, "hash": "de2320e00045ec130fb40fb3e230a68aa740a2af9c849f6eff62a385d1c5e19c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db35fb43-d82a-43f6-9b8e-b40dfec5b99a", "node_type": "1", "metadata": {}, "hash": "69005b66d367748ee5e1c83dac7090e6c4b8921f6f589b8381aae1ef936d1124", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In conclusion, supervised learning and unsupervised learning are two key types of ML. Supervised \nlearning relies on labeled data to learn patterns and make predictions, while unsupervised learning \ndiscovers patterns and structures in unlabeled data. Both types have their own set of algorithms and \ntechniques, as well as different choices. Discussing unsupervised learning in depth is beyond the \nscope of this book.\nIn the next section, we will explore supervised ML, a cornerstone in the realm of AI and data science \nthat represents a powerful approach to building predictive models and making data-driven decisions.\nTypes of supervised learning\nAs we know, supervised learning is a branch of ML where algorithms learn patterns and relationships \nfrom labeled training data. It involves teaching or supervising the model by presenting input data along \nwith corresponding output labels, allowing the algorithm to learn the mapping between the input \nand output variables.", "mimetype": "text/plain", "start_char_idx": 295369, "end_char_idx": 296346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db35fb43-d82a-43f6-9b8e-b40dfec5b99a": {"__data__": {"id_": "db35fb43-d82a-43f6-9b8e-b40dfec5b99a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd075574-8afe-417e-bb55-a7d0701dc1af", "node_type": "1", "metadata": {}, "hash": "8d73dca3cce14af4d8502d4067e1dc12c9c33de6219a5fb8a8178e8852c0960f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27a48193-8aca-459c-8ba1-b3bc484eae80", "node_type": "1", "metadata": {}, "hash": "16c86e0fc9bc70fe8e31579a370470ea1a891483176b02b00268385141be92c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Types of supervised learning\nAs we know, supervised learning is a branch of ML where algorithms learn patterns and relationships \nfrom labeled training data. It involves teaching or supervising the model by presenting input data along \nwith corresponding output labels, allowing the algorithm to learn the mapping between the input \nand output variables. We\u2019ll explore three key types of supervised learning \u2013 classification, regression, \nand time series.\nClassification\nClassification is a type of ML task where the goal is to categorize or classify data into predefined classes \nor categories based on its features. The algorithm learns from labeled training data to build a model \nthat can predict the class label of new, unseen data instances.\nIn classification, the output is discrete and represents class labels. Some common algorithms that \nare used for classification tasks include logistic regression, decision trees, random forests, SVM, and \nnaive Bayes.", "mimetype": "text/plain", "start_char_idx": 295992, "end_char_idx": 296957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27a48193-8aca-459c-8ba1-b3bc484eae80": {"__data__": {"id_": "27a48193-8aca-459c-8ba1-b3bc484eae80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db35fb43-d82a-43f6-9b8e-b40dfec5b99a", "node_type": "1", "metadata": {}, "hash": "69005b66d367748ee5e1c83dac7090e6c4b8921f6f589b8381aae1ef936d1124", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "960464e9-fc8a-4bd3-bb58-e784d1e302af", "node_type": "1", "metadata": {}, "hash": "851a2a1b84d220b0cdefcb6fe0d5e81f644d32657999d56f0411f3586b99f703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The algorithm learns from labeled training data to build a model \nthat can predict the class label of new, unseen data instances.\nIn classification, the output is discrete and represents class labels. Some common algorithms that \nare used for classification tasks include logistic regression, decision trees, random forests, SVM, and \nnaive Bayes.\nFor example, consider a spam email classification task, where the goal is to predict whether an incoming \nemail is spam or not. The algorithm is trained on a dataset of labeled emails, where each email is \nassociated with a class label indicating whether it is spam or not. The trained model can then classify \nnew emails as spam or non-spam based on their features, such as the content, subject, or sender.\nRegression\nRegression is another type of ML task that focuses on predicting continuous or numerical values based \non input features.", "mimetype": "text/plain", "start_char_idx": 296610, "end_char_idx": 297498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "960464e9-fc8a-4bd3-bb58-e784d1e302af": {"__data__": {"id_": "960464e9-fc8a-4bd3-bb58-e784d1e302af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27a48193-8aca-459c-8ba1-b3bc484eae80", "node_type": "1", "metadata": {}, "hash": "16c86e0fc9bc70fe8e31579a370470ea1a891483176b02b00268385141be92c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa12d595-377e-4bbb-aa58-f2cab7e8a57f", "node_type": "1", "metadata": {}, "hash": "5af2933b157c713f5769a7b610c3447d2e55bda373f10fb08410e5bb1535c581", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The algorithm is trained on a dataset of labeled emails, where each email is \nassociated with a class label indicating whether it is spam or not. The trained model can then classify \nnew emails as spam or non-spam based on their features, such as the content, subject, or sender.\nRegression\nRegression is another type of ML task that focuses on predicting continuous or numerical values based \non input features. In regression, the algorithm learns from labeled training data to build a model that \ncan estimate or forecast the numerical value of a target variable given a set of input features.\n\nMachine Learning with Spark ML\n\nRegression models are used when the output is a continuous value, such as predicting house prices, \nstock market trends, or predicting the sales of a product based on historical data. Some commonly \nused regression algorithms include linear regression, decision trees, random forests, gradient boosting, \nand neural networks.", "mimetype": "text/plain", "start_char_idx": 297086, "end_char_idx": 298040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa12d595-377e-4bbb-aa58-f2cab7e8a57f": {"__data__": {"id_": "fa12d595-377e-4bbb-aa58-f2cab7e8a57f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "960464e9-fc8a-4bd3-bb58-e784d1e302af", "node_type": "1", "metadata": {}, "hash": "851a2a1b84d220b0cdefcb6fe0d5e81f644d32657999d56f0411f3586b99f703", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc6c5046-b198-454f-9620-a9a5d74f9fa2", "node_type": "1", "metadata": {}, "hash": "ff0f37005c0f4a19b04cd2439be5859bf0e686d37d0385f0c4c9300f4c2458e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine Learning with Spark ML\n\nRegression models are used when the output is a continuous value, such as predicting house prices, \nstock market trends, or predicting the sales of a product based on historical data. Some commonly \nused regression algorithms include linear regression, decision trees, random forests, gradient boosting, \nand neural networks.\nFor example, consider a case where you want to predict the price of a house based on its various features, \nsuch as the area, number of bedrooms, location, and so on. In this case, the algorithm is trained on a \ndataset of labeled house data, where each house is associated with its corresponding price. The trained \nregression model can then predict the price of a new house based on its features.\nTime series\nTime series analysis is a specialized area of ML that deals with data collected over time, where the \norder of observations is important.", "mimetype": "text/plain", "start_char_idx": 297683, "end_char_idx": 298589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc6c5046-b198-454f-9620-a9a5d74f9fa2": {"__data__": {"id_": "bc6c5046-b198-454f-9620-a9a5d74f9fa2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa12d595-377e-4bbb-aa58-f2cab7e8a57f", "node_type": "1", "metadata": {}, "hash": "5af2933b157c713f5769a7b610c3447d2e55bda373f10fb08410e5bb1535c581", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5910b5be-4267-4ea2-9135-65cb147c7c39", "node_type": "1", "metadata": {}, "hash": "affcf45c1565c27adfd64175bda55694401f926d7f578f7d32338ea5055b63a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, the algorithm is trained on a \ndataset of labeled house data, where each house is associated with its corresponding price. The trained \nregression model can then predict the price of a new house based on its features.\nTime series\nTime series analysis is a specialized area of ML that deals with data collected over time, where the \norder of observations is important. In time series analysis, the goal is to understand and forecast the \npatterns, trends, and dependencies within the data.\nTime series models are used to predict future values based on historical data points. They are widely \nused in fields such as finance, stock market prediction, weather forecasting, and demand forecasting. \nSome popular time series algorithms include Autoregressive Integrated Moving Average (ARIMA), \nexponential smoothing methods, and long short-term memory (LSTM) networks.\nFor example, suppose you have historical stock market data for a particular company, including the \ndate and the corresponding stock prices.", "mimetype": "text/plain", "start_char_idx": 298208, "end_char_idx": 299227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5910b5be-4267-4ea2-9135-65cb147c7c39": {"__data__": {"id_": "5910b5be-4267-4ea2-9135-65cb147c7c39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc6c5046-b198-454f-9620-a9a5d74f9fa2", "node_type": "1", "metadata": {}, "hash": "ff0f37005c0f4a19b04cd2439be5859bf0e686d37d0385f0c4c9300f4c2458e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373", "node_type": "1", "metadata": {}, "hash": "d61bdd5b72e0d6566305513333ecba4e02486ea2f2f70a05376b13d853bb96d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They are widely \nused in fields such as finance, stock market prediction, weather forecasting, and demand forecasting. \nSome popular time series algorithms include Autoregressive Integrated Moving Average (ARIMA), \nexponential smoothing methods, and long short-term memory (LSTM) networks.\nFor example, suppose you have historical stock market data for a particular company, including the \ndate and the corresponding stock prices. The time series algorithm can analyze the patterns and trends \nin the data and make predictions about future stock prices based on historical price fluctuations.\nIn conclusion, supervised learning encompasses various types, including classification, regression, and \ntime series analysis. Each type addresses specific learning tasks and requires different algorithms and \ntechniques. Understanding these types helps in choosing the appropriate algorithms and approaches \nfor specific data analysis and prediction tasks.\nNext, we will explore how to leverage Spark for ML tasks.\nML with Spark\nSpark provides a powerful and scalable platform for performing large-scale ML tasks.", "mimetype": "text/plain", "start_char_idx": 298797, "end_char_idx": 299904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373": {"__data__": {"id_": "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5910b5be-4267-4ea2-9135-65cb147c7c39", "node_type": "1", "metadata": {}, "hash": "affcf45c1565c27adfd64175bda55694401f926d7f578f7d32338ea5055b63a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cefbdf0f-869e-426e-ba24-39be83615381", "node_type": "1", "metadata": {}, "hash": "a361947d14392b5c0320975fb6019b16bc34182b941b3c5d9362b0900b92b135", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In conclusion, supervised learning encompasses various types, including classification, regression, and \ntime series analysis. Each type addresses specific learning tasks and requires different algorithms and \ntechniques. Understanding these types helps in choosing the appropriate algorithms and approaches \nfor specific data analysis and prediction tasks.\nNext, we will explore how to leverage Spark for ML tasks.\nML with Spark\nSpark provides a powerful and scalable platform for performing large-scale ML tasks. Spark\u2019s ML \nlibrary, also known as MLlib, offers a wide range of algorithms and tools for building and deploying \nML models.\nThe advantages of using Spark for ML include its distributed computing capabilities, efficient data \nprocessing, scalability, and integration with other Spark components, such as Spark SQL and Spark \nStreaming. Spark\u2019s MLlib supports both batch and streaming data processing, enabling the development \nof real-time ML applications.", "mimetype": "text/plain", "start_char_idx": 299390, "end_char_idx": 300361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cefbdf0f-869e-426e-ba24-39be83615381": {"__data__": {"id_": "cefbdf0f-869e-426e-ba24-39be83615381", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373", "node_type": "1", "metadata": {}, "hash": "d61bdd5b72e0d6566305513333ecba4e02486ea2f2f70a05376b13d853bb96d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "266183ce-a17a-448d-8aa1-7bffe8d254d5", "node_type": "1", "metadata": {}, "hash": "f979461ab29794c4f8659d99c1cb1695686e376345c1190fe1c9b1fe4aec76be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark\u2019s ML \nlibrary, also known as MLlib, offers a wide range of algorithms and tools for building and deploying \nML models.\nThe advantages of using Spark for ML include its distributed computing capabilities, efficient data \nprocessing, scalability, and integration with other Spark components, such as Spark SQL and Spark \nStreaming. Spark\u2019s MLlib supports both batch and streaming data processing, enabling the development \nof real-time ML applications.\n\nML with Spark\n\nML is a transformative field that enables computers to learn from data and make predictions or \ndecisions. By understanding the key concepts and leveraging tools such as Spark\u2019s MLlib, we can harness \nthe power of ML to gain insights, automate processes, and drive innovation across various domains.\nNow, let\u2019s take a look at the benefits of using Spark for ML tasks.", "mimetype": "text/plain", "start_char_idx": 299905, "end_char_idx": 300745, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "266183ce-a17a-448d-8aa1-7bffe8d254d5": {"__data__": {"id_": "266183ce-a17a-448d-8aa1-7bffe8d254d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cefbdf0f-869e-426e-ba24-39be83615381", "node_type": "1", "metadata": {}, "hash": "a361947d14392b5c0320975fb6019b16bc34182b941b3c5d9362b0900b92b135", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63e2ec22-3381-49f0-ac9e-fb540705c9fd", "node_type": "1", "metadata": {}, "hash": "ea76d49acaccea85624724fe44a2c1e78ffbfa1d8dc5f5a6e702c99458fd1cd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ML with Spark\n\nML is a transformative field that enables computers to learn from data and make predictions or \ndecisions. By understanding the key concepts and leveraging tools such as Spark\u2019s MLlib, we can harness \nthe power of ML to gain insights, automate processes, and drive innovation across various domains.\nNow, let\u2019s take a look at the benefits of using Spark for ML tasks.\nAdvantages of Apache Spark for large-scale ML\nBy leveraging Spark\u2019s distributed computing capabilities and rich ecosystem, data scientists and engineers \ncan effectively tackle complex ML challenges on massive datasets. It offers various advantages due to \nits distributed computing capabilities, some of which are as follows:\n\u2022\t Speed and performance: One of the key advantages of Apache Spark is its ability to handle \nlarge-scale data processing with exceptional speed. Spark leverages in-memory computing \nand optimized data processing techniques, such as data parallelism and task pipelining, to \naccelerate computations.", "mimetype": "text/plain", "start_char_idx": 300363, "end_char_idx": 301372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63e2ec22-3381-49f0-ac9e-fb540705c9fd": {"__data__": {"id_": "63e2ec22-3381-49f0-ac9e-fb540705c9fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "266183ce-a17a-448d-8aa1-7bffe8d254d5", "node_type": "1", "metadata": {}, "hash": "f979461ab29794c4f8659d99c1cb1695686e376345c1190fe1c9b1fe4aec76be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05acb078-a0cf-46da-8ce1-4f58448535d2", "node_type": "1", "metadata": {}, "hash": "2d72abe7d00962c26688197e47239063b0e81317a7538547ffccfff8685a378d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It offers various advantages due to \nits distributed computing capabilities, some of which are as follows:\n\u2022\t Speed and performance: One of the key advantages of Apache Spark is its ability to handle \nlarge-scale data processing with exceptional speed. Spark leverages in-memory computing \nand optimized data processing techniques, such as data parallelism and task pipelining, to \naccelerate computations. This makes it highly efficient for iterative algorithms often used in \nML, reducing the overall processing time significantly.\n\u2022\t Distributed computing: Spark\u2019s distributed computing model allows it to distribute data and \ncomputations across multiple nodes in a cluster, enabling parallel processing. This distributed \nnature enables Spark to scale horizontally, leveraging the computing power of multiple machines \nand processing data in parallel. This makes it well-suited for large-scale ML tasks that require \nprocessing massive volumes of data.\n\u2022\t Fault tolerance: Another advantage of Apache Spark is its built-in fault tolerance mechanism.", "mimetype": "text/plain", "start_char_idx": 300966, "end_char_idx": 302020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05acb078-a0cf-46da-8ce1-4f58448535d2": {"__data__": {"id_": "05acb078-a0cf-46da-8ce1-4f58448535d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63e2ec22-3381-49f0-ac9e-fb540705c9fd", "node_type": "1", "metadata": {}, "hash": "ea76d49acaccea85624724fe44a2c1e78ffbfa1d8dc5f5a6e702c99458fd1cd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92536d3a-746f-4ccd-8190-9b4a6dd95b18", "node_type": "1", "metadata": {}, "hash": "ffb7b7bd38664a130f319471726c32f2832f6a56aa3a8a8ce29efd57f6c20891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This distributed \nnature enables Spark to scale horizontally, leveraging the computing power of multiple machines \nand processing data in parallel. This makes it well-suited for large-scale ML tasks that require \nprocessing massive volumes of data.\n\u2022\t Fault tolerance: Another advantage of Apache Spark is its built-in fault tolerance mechanism. \nSpark automatically tracks the lineage of Resilient Distributed Datasets (RDDs), which are \nthe fundamental data abstraction in Spark, allowing it to recover from failures and rerun \nfailed tasks. This ensures the reliability and resilience of Spark applications, making it a robust \nplatform for handling large-scale ML workloads.\n\u2022\t Versatility and flexibility: Spark provides a wide range of APIs and libraries that facilitate \nvarious data processing and analytics tasks, including ML. Spark\u2019s MLlib library offers a rich \nset of distributed ML algorithms and utilities, making it easy to develop and deploy scalable \nML models.", "mimetype": "text/plain", "start_char_idx": 301675, "end_char_idx": 302654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92536d3a-746f-4ccd-8190-9b4a6dd95b18": {"__data__": {"id_": "92536d3a-746f-4ccd-8190-9b4a6dd95b18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05acb078-a0cf-46da-8ce1-4f58448535d2", "node_type": "1", "metadata": {}, "hash": "2d72abe7d00962c26688197e47239063b0e81317a7538547ffccfff8685a378d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "443dfd09-54ff-47da-82ae-501a641369a3", "node_type": "1", "metadata": {}, "hash": "28b9e5bfd04b53401f9c4d6cc6898680799e87b3cfa89b0a80b627550a1f699a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This ensures the reliability and resilience of Spark applications, making it a robust \nplatform for handling large-scale ML workloads.\n\u2022\t Versatility and flexibility: Spark provides a wide range of APIs and libraries that facilitate \nvarious data processing and analytics tasks, including ML. Spark\u2019s MLlib library offers a rich \nset of distributed ML algorithms and utilities, making it easy to develop and deploy scalable \nML models. Additionally, Spark integrates well with other popular data processing frameworks \nand tools, enabling seamless integration into existing data pipelines and ecosystems.\n\u2022\t Real-time and streaming capabilities: As we discussed in the previous chapter, Spark extends \nits capabilities beyond batch processing with its streaming component called Spark Streaming. \nThis is particularly valuable in scenarios where immediate insights or decisions are required \nbased on continuously arriving data, such as real-time fraud detection, sensor data analysis, \nor sentiment analysis on social media streams.", "mimetype": "text/plain", "start_char_idx": 302219, "end_char_idx": 303252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "443dfd09-54ff-47da-82ae-501a641369a3": {"__data__": {"id_": "443dfd09-54ff-47da-82ae-501a641369a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92536d3a-746f-4ccd-8190-9b4a6dd95b18", "node_type": "1", "metadata": {}, "hash": "ffb7b7bd38664a130f319471726c32f2832f6a56aa3a8a8ce29efd57f6c20891", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5d255df-d61d-4ff0-b190-6f4ffc52e94b", "node_type": "1", "metadata": {}, "hash": "f9859384758b5d3a223887c4b9186905559c484729ea1aad1103d7f5a50d1d05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Real-time and streaming capabilities: As we discussed in the previous chapter, Spark extends \nits capabilities beyond batch processing with its streaming component called Spark Streaming. \nThis is particularly valuable in scenarios where immediate insights or decisions are required \nbased on continuously arriving data, such as real-time fraud detection, sensor data analysis, \nor sentiment analysis on social media streams.\n\nMachine Learning with Spark ML\n\n\u2022\t Ecosystem and community support: Apache Spark has a vibrant and active community of \ndevelopers and contributors, ensuring continuous development, improvement, and support. \nSpark benefits from a rich ecosystem of tools and extensions, providing additional functionality \nand integration options. The community-driven nature of Spark ensures a wealth of resources, \ndocumentation, tutorials, and online forums for learning and troubleshooting.\nTherefore, Apache Spark offers significant advantages for large-scale ML tasks.", "mimetype": "text/plain", "start_char_idx": 302824, "end_char_idx": 303812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5d255df-d61d-4ff0-b190-6f4ffc52e94b": {"__data__": {"id_": "a5d255df-d61d-4ff0-b190-6f4ffc52e94b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "443dfd09-54ff-47da-82ae-501a641369a3", "node_type": "1", "metadata": {}, "hash": "28b9e5bfd04b53401f9c4d6cc6898680799e87b3cfa89b0a80b627550a1f699a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a", "node_type": "1", "metadata": {}, "hash": "43c49a03ac84919bb877b2577fce6a5d2e751c9dfda2748e566d7197b957aeae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark benefits from a rich ecosystem of tools and extensions, providing additional functionality \nand integration options. The community-driven nature of Spark ensures a wealth of resources, \ndocumentation, tutorials, and online forums for learning and troubleshooting.\nTherefore, Apache Spark offers significant advantages for large-scale ML tasks. Its speed, scalability, \nfault tolerance, versatility, and real-time capabilities make it a powerful framework for processing big \ndata and developing scalable ML models.\nNow let\u2019s take a look at different libraries that Spark provides to make use of ML capabilities in the \ndistributed framework.\nSpark MLlib versus Spark ML\nApache Spark provides two libraries for ML: Spark MLlib and Spark ML. Although they share a similar \nname, there are some key differences between the two libraries in terms of their design, APIs, and \nfunctionality. Let\u2019s compare Spark MLlib and Spark ML to understand their characteristics and use cases.", "mimetype": "text/plain", "start_char_idx": 303463, "end_char_idx": 304444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a": {"__data__": {"id_": "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5d255df-d61d-4ff0-b190-6f4ffc52e94b", "node_type": "1", "metadata": {}, "hash": "f9859384758b5d3a223887c4b9186905559c484729ea1aad1103d7f5a50d1d05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65faffbf-4c40-469a-8216-834a9ff4041d", "node_type": "1", "metadata": {}, "hash": "d12a355a4e3b21ee8c0e83cc0e48404973f48a7f1b610a31194a8b95941f78ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Spark MLlib versus Spark ML\nApache Spark provides two libraries for ML: Spark MLlib and Spark ML. Although they share a similar \nname, there are some key differences between the two libraries in terms of their design, APIs, and \nfunctionality. Let\u2019s compare Spark MLlib and Spark ML to understand their characteristics and use cases.\nSpark MLlib\nSpark MLlib is the original ML library in Apache Spark. It was introduced in earlier versions of Spark \nand provides a rich set of distributed ML algorithms and utilities. MLlib is built on top of the RDD \nAPI, which is the core data abstraction in Spark.\nSpark MLlib has a few key features that set it apart from other non-distributed ML libraries such as \nscikit-learn. Let\u2019s look at a few of them:\n\u2022\t RDD-based API: MLlib leverages the RDD abstraction for distributed data processing, making \nit suitable for batch processing and iterative algorithms.", "mimetype": "text/plain", "start_char_idx": 304111, "end_char_idx": 305011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65faffbf-4c40-469a-8216-834a9ff4041d": {"__data__": {"id_": "65faffbf-4c40-469a-8216-834a9ff4041d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a", "node_type": "1", "metadata": {}, "hash": "43c49a03ac84919bb877b2577fce6a5d2e751c9dfda2748e566d7197b957aeae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37c938d0-d3d3-42ed-9a20-73fd187a1c59", "node_type": "1", "metadata": {}, "hash": "ba5f64bfbe78760407770adbbdfb3e7aa8a2c673a17467f5bfa02fd3599c58a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MLlib is built on top of the RDD \nAPI, which is the core data abstraction in Spark.\nSpark MLlib has a few key features that set it apart from other non-distributed ML libraries such as \nscikit-learn. Let\u2019s look at a few of them:\n\u2022\t RDD-based API: MLlib leverages the RDD abstraction for distributed data processing, making \nit suitable for batch processing and iterative algorithms. The RDD API allows for efficient \ndistributed computing but can be low-level and complex for some use cases.\n\u2022\t Diverse algorithms: MLlib offers a wide range of distributed ML algorithms, including \nclassification, regression, clustering, collaborative filtering, dimensionality reduction, and \nmore. These algorithms are implemented to work with large-scale data and can handle various \ntasks in the ML pipeline.\n\u2022\t Feature engineering: MLlib provides utilities for feature extraction, transformation, and \nselection.", "mimetype": "text/plain", "start_char_idx": 304629, "end_char_idx": 305530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37c938d0-d3d3-42ed-9a20-73fd187a1c59": {"__data__": {"id_": "37c938d0-d3d3-42ed-9a20-73fd187a1c59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65faffbf-4c40-469a-8216-834a9ff4041d", "node_type": "1", "metadata": {}, "hash": "d12a355a4e3b21ee8c0e83cc0e48404973f48a7f1b610a31194a8b95941f78ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffd9d661-fd05-4848-b224-57e2a42d678c", "node_type": "1", "metadata": {}, "hash": "1d5cbe3533561637ce38a22f9f9360503d3fca61784367ba7129db1f42732096", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Diverse algorithms: MLlib offers a wide range of distributed ML algorithms, including \nclassification, regression, clustering, collaborative filtering, dimensionality reduction, and \nmore. These algorithms are implemented to work with large-scale data and can handle various \ntasks in the ML pipeline.\n\u2022\t Feature engineering: MLlib provides utilities for feature extraction, transformation, and \nselection. It includes methods for handling categorical and numerical features, text processing, \nand feature scaling.\n\u2022\t Model persistence: MLlib supports model persistence, allowing trained models to be saved to \ndisk and loaded later for deployment or further analysis.\nIn the next section, we will explore the Spark ML library. This is the newer library that also provides \nML capabilities.\n\nML with Spark\n\nSpark ML\nSpark ML, introduced in Spark 2.0, is the newer ML library in Apache Spark.", "mimetype": "text/plain", "start_char_idx": 305121, "end_char_idx": 306015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffd9d661-fd05-4848-b224-57e2a42d678c": {"__data__": {"id_": "ffd9d661-fd05-4848-b224-57e2a42d678c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37c938d0-d3d3-42ed-9a20-73fd187a1c59", "node_type": "1", "metadata": {}, "hash": "ba5f64bfbe78760407770adbbdfb3e7aa8a2c673a17467f5bfa02fd3599c58a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "693f011b-ec1e-4d38-86fc-99572ab4b862", "node_type": "1", "metadata": {}, "hash": "c22f3249b9aa858823db5b80a8834a2211e48beae1a99de8fbe95377e6dc5dcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Model persistence: MLlib supports model persistence, allowing trained models to be saved to \ndisk and loaded later for deployment or further analysis.\nIn the next section, we will explore the Spark ML library. This is the newer library that also provides \nML capabilities.\n\nML with Spark\n\nSpark ML\nSpark ML, introduced in Spark 2.0, is the newer ML library in Apache Spark. It is designed to be \nmore user-friendly, with a higher-level API and a focus on DataFrames, which are a structured and \noptimized distributed data collection introduced in Spark SQL.\nThe key features of Spark ML are as follows:\n\u2022\t DataFrame-based API: Spark ML leverages the DataFrame API, which provides a more \nintuitive and higher-level interface compared to the RDD API. DataFrames offer a structured \nand tabular data representation, making it easier to work with structured data and integrate \nwith Spark SQL.", "mimetype": "text/plain", "start_char_idx": 305639, "end_char_idx": 306532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "693f011b-ec1e-4d38-86fc-99572ab4b862": {"__data__": {"id_": "693f011b-ec1e-4d38-86fc-99572ab4b862", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffd9d661-fd05-4848-b224-57e2a42d678c", "node_type": "1", "metadata": {}, "hash": "1d5cbe3533561637ce38a22f9f9360503d3fca61784367ba7129db1f42732096", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5751895-b3d1-4cff-b0e1-c135cf785f19", "node_type": "1", "metadata": {}, "hash": "a595b50bc70546356274a45f908210dfe32704ba0627dea58ed713a0fd6bec35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The key features of Spark ML are as follows:\n\u2022\t DataFrame-based API: Spark ML leverages the DataFrame API, which provides a more \nintuitive and higher-level interface compared to the RDD API. DataFrames offer a structured \nand tabular data representation, making it easier to work with structured data and integrate \nwith Spark SQL.\n\u2022\t Pipelines: Spark ML introduces the concept of pipelines, which provides a higher-level abstraction \nfor constructing ML workflows. Pipelines enable the chaining of multiple data transformations \nand model training stages into a single pipeline, simplifying the development and deployment \nof complex ML pipelines.\n\u2022\t Integrated feature transformers: Spark ML includes a rich set of feature transformers, such \nas StringIndexer, OneHotEncoder, VectorAssembler, and more. These transformers seamlessly \nintegrate with DataFrames and simplify the feature engineering process.", "mimetype": "text/plain", "start_char_idx": 306200, "end_char_idx": 307108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5751895-b3d1-4cff-b0e1-c135cf785f19": {"__data__": {"id_": "b5751895-b3d1-4cff-b0e1-c135cf785f19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "693f011b-ec1e-4d38-86fc-99572ab4b862", "node_type": "1", "metadata": {}, "hash": "c22f3249b9aa858823db5b80a8834a2211e48beae1a99de8fbe95377e6dc5dcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d2b075-7766-4445-bb88-256551ecadac", "node_type": "1", "metadata": {}, "hash": "4f74d218f7d904b3b6af1da0acc0aac9144bd6a60a8404ce3be8e4d9dcb08b83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pipelines enable the chaining of multiple data transformations \nand model training stages into a single pipeline, simplifying the development and deployment \nof complex ML pipelines.\n\u2022\t Integrated feature transformers: Spark ML includes a rich set of feature transformers, such \nas StringIndexer, OneHotEncoder, VectorAssembler, and more. These transformers seamlessly \nintegrate with DataFrames and simplify the feature engineering process.\n\u2022\t Unified API: Spark ML unifies the APIs for different ML tasks, such as classification, regression, \nclustering, and recommendation. This provides a consistent and cohesive programming interface \nacross different algorithms and simplifies the learning curve.\nNow that we know the key features of both Spark MLlib and Spark ML, let\u2019s explore when to use each \nof them.", "mimetype": "text/plain", "start_char_idx": 306667, "end_char_idx": 307478, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0d2b075-7766-4445-bb88-256551ecadac": {"__data__": {"id_": "f0d2b075-7766-4445-bb88-256551ecadac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5751895-b3d1-4cff-b0e1-c135cf785f19", "node_type": "1", "metadata": {}, "hash": "a595b50bc70546356274a45f908210dfe32704ba0627dea58ed713a0fd6bec35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00a5bf42-d5a3-45b6-b151-87159f937a11", "node_type": "1", "metadata": {}, "hash": "64a98af1dbda5f49774610fd9f9a6e41136750c6e9b3c3354e84b45ec0c19f18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Unified API: Spark ML unifies the APIs for different ML tasks, such as classification, regression, \nclustering, and recommendation. This provides a consistent and cohesive programming interface \nacross different algorithms and simplifies the learning curve.\nNow that we know the key features of both Spark MLlib and Spark ML, let\u2019s explore when to use each \nof them.\nYou would benefit from using Spark MLlib in the following scenarios:\n\u2022\t You are working with older versions of Spark that do not support Spark ML\n\u2022\t You require low-level control and need to work directly with RDDs\n\u2022\t You need access to a specific algorithm or functionality that is not available in Spark ML\nYou should prefer to use Spark ML in the following scenarios:\n\u2022\t You are using Spark 2.0 or later versions\n\u2022\t You prefer a higher-level API and want to leverage DataFrames and Spark SQL capabilities\n\u2022\t You need to build end-to-end ML pipelines with integrated feature transformers and pipelines\nBoth Spark MLlib and Spark ML provide powerful ML capabilities in Apache Spark.", "mimetype": "text/plain", "start_char_idx": 307109, "end_char_idx": 308162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00a5bf42-d5a3-45b6-b151-87159f937a11": {"__data__": {"id_": "00a5bf42-d5a3-45b6-b151-87159f937a11", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d2b075-7766-4445-bb88-256551ecadac", "node_type": "1", "metadata": {}, "hash": "4f74d218f7d904b3b6af1da0acc0aac9144bd6a60a8404ce3be8e4d9dcb08b83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c230598-c4da-4bc7-9300-45b148ac5377", "node_type": "1", "metadata": {}, "hash": "319621d005208d7649725469e0582eee0be37a1eb121ea788a94e690991b5b05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we\u2019ve seen, \nSpark MLlib is the original library with a rich set of distributed algorithms, while Spark ML is a newer \nlibrary with a more user-friendly API and integration with DataFrames. The choice between the two \ndepends on your Spark version, preference for API style, and specific requirements of your ML tasks.\n\nMachine Learning with Spark ML\n\nML life cycle\nThe ML life cycle encompasses the end-to-end process of developing and deploying ML models. It \ninvolves several stages, each with its own set of tasks and considerations. Understanding the ML life \ncycle is crucial for building robust and successful ML solutions. In this section, we will explore the \nkey stages of the ML life cycle:\n1.\t\nProblem definition: The first stage of the ML life cycle is problem definition. It involves clearly \ndefining the problem you want to solve and understanding the goals and objectives of your \nML project.", "mimetype": "text/plain", "start_char_idx": 308163, "end_char_idx": 309075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c230598-c4da-4bc7-9300-45b148ac5377": {"__data__": {"id_": "8c230598-c4da-4bc7-9300-45b148ac5377", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00a5bf42-d5a3-45b6-b151-87159f937a11", "node_type": "1", "metadata": {}, "hash": "64a98af1dbda5f49774610fd9f9a6e41136750c6e9b3c3354e84b45ec0c19f18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7", "node_type": "1", "metadata": {}, "hash": "7635f24d4912042dbf0bd976cd42e25ccdf9bbdb8814fe677f399ae496f6b937", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Understanding the ML life \ncycle is crucial for building robust and successful ML solutions. In this section, we will explore the \nkey stages of the ML life cycle:\n1.\t\nProblem definition: The first stage of the ML life cycle is problem definition. It involves clearly \ndefining the problem you want to solve and understanding the goals and objectives of your \nML project. This stage requires collaboration between domain experts and data scientists to \nidentify the problem, define success metrics, and establish the scope of the project.\n2.\t\nData acquisition and understanding: Once the problem has been defined, the next step is to \nacquire the necessary data for training and evaluation. Data acquisition may involve collecting \ndata from various sources, such as databases, APIs, or external datasets. It is important to \nensure data quality, completeness, and relevance to the problem at hand.", "mimetype": "text/plain", "start_char_idx": 308704, "end_char_idx": 309602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7": {"__data__": {"id_": "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c230598-c4da-4bc7-9300-45b148ac5377", "node_type": "1", "metadata": {}, "hash": "319621d005208d7649725469e0582eee0be37a1eb121ea788a94e690991b5b05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7cb94a1-7139-4540-9335-22009c7d151e", "node_type": "1", "metadata": {}, "hash": "b9da684ed7f26e831ae6c6d445f28769c7d8bfa58e511f1fdd8fcc2e5a85b4bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.\t\nData acquisition and understanding: Once the problem has been defined, the next step is to \nacquire the necessary data for training and evaluation. Data acquisition may involve collecting \ndata from various sources, such as databases, APIs, or external datasets. It is important to \nensure data quality, completeness, and relevance to the problem at hand. Additionally, data \nunderstanding involves exploring and analyzing the acquired data to gain insights into its \nstructure, distributions, and potential issues.\n3.\t\nData preparation and feature engineering: Data preparation and feature engineering are \ncrucial steps in the ML life cycle. It involves transforming and preprocessing the data to make it \nsuitable for training ML models. This includes tasks such as cleaning the data, handling missing \nvalues, encoding categorical variables, scaling features, and creating new features through \nfeature engineering techniques. Proper data preparation and feature engineering significantly \nimpact the performance and accuracy of ML models.\n4.", "mimetype": "text/plain", "start_char_idx": 309243, "end_char_idx": 310293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7cb94a1-7139-4540-9335-22009c7d151e": {"__data__": {"id_": "a7cb94a1-7139-4540-9335-22009c7d151e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7", "node_type": "1", "metadata": {}, "hash": "7635f24d4912042dbf0bd976cd42e25ccdf9bbdb8814fe677f399ae496f6b937", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd813494-2c4f-492c-b0b6-5ce159aacebf", "node_type": "1", "metadata": {}, "hash": "8d355d6c28d84fb00e253074d921f932ab5195f25436eca0a7737e9ce7d62e77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It involves transforming and preprocessing the data to make it \nsuitable for training ML models. This includes tasks such as cleaning the data, handling missing \nvalues, encoding categorical variables, scaling features, and creating new features through \nfeature engineering techniques. Proper data preparation and feature engineering significantly \nimpact the performance and accuracy of ML models.\n4.\t\nModel training and evaluation: In this stage, ML models are trained on the prepared data. \nModel training involves selecting an appropriate algorithm, defining the model architecture, \nand optimizing its parameters using training data. The trained model is then evaluated using \nevaluation metrics and validation techniques to assess its performance. This stage often requires \niterating and fine-tuning the model to achieve the desired accuracy and generalization.\n5.\t\nModel deployment: Once the model has been trained and evaluated, it is ready for deployment.", "mimetype": "text/plain", "start_char_idx": 309891, "end_char_idx": 310857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd813494-2c4f-492c-b0b6-5ce159aacebf": {"__data__": {"id_": "cd813494-2c4f-492c-b0b6-5ce159aacebf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7cb94a1-7139-4540-9335-22009c7d151e", "node_type": "1", "metadata": {}, "hash": "b9da684ed7f26e831ae6c6d445f28769c7d8bfa58e511f1fdd8fcc2e5a85b4bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfc1edf9-64d3-4107-9238-076f724b3f42", "node_type": "1", "metadata": {}, "hash": "691a6daed7e8b499a872aea6d34c4eb4ba7a9b136484b4b24661f10842ab8b05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model training involves selecting an appropriate algorithm, defining the model architecture, \nand optimizing its parameters using training data. The trained model is then evaluated using \nevaluation metrics and validation techniques to assess its performance. This stage often requires \niterating and fine-tuning the model to achieve the desired accuracy and generalization.\n5.\t\nModel deployment: Once the model has been trained and evaluated, it is ready for deployment. \nModel deployment involves integrating the model into the production environment, making \npredictions on new data, and monitoring its performance. This may involve setting up APIs, \ncreating batch or real-time inference systems, and ensuring the model\u2019s scalability and reliability. \nDeployment also includes considerations for model versioning, monitoring, and retraining to \nmaintain the model\u2019s effectiveness over time.\n6.\t\nModel monitoring and maintenance: Once the model has been deployed, it is important to \ncontinuously monitor its performance and maintain its effectiveness.", "mimetype": "text/plain", "start_char_idx": 310386, "end_char_idx": 311441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfc1edf9-64d3-4107-9238-076f724b3f42": {"__data__": {"id_": "dfc1edf9-64d3-4107-9238-076f724b3f42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd813494-2c4f-492c-b0b6-5ce159aacebf", "node_type": "1", "metadata": {}, "hash": "8d355d6c28d84fb00e253074d921f932ab5195f25436eca0a7737e9ce7d62e77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb08a789-9991-4a62-8535-d8c2d119ff7d", "node_type": "1", "metadata": {}, "hash": "cf217e127c7864f87c003f5e70dcfb4c734a795eca88844b7f125640bbc10ad1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This may involve setting up APIs, \ncreating batch or real-time inference systems, and ensuring the model\u2019s scalability and reliability. \nDeployment also includes considerations for model versioning, monitoring, and retraining to \nmaintain the model\u2019s effectiveness over time.\n6.\t\nModel monitoring and maintenance: Once the model has been deployed, it is important to \ncontinuously monitor its performance and maintain its effectiveness. Monitoring involves tracking \nmodel predictions, detecting anomalies, and collecting feedback from users or domain experts. \nIt also includes periodic retraining of the model using new data to adapt to changing patterns \nor concepts. Model maintenance involves addressing model drift, updating dependencies, and \nmanaging the model\u2019s life cycle in the production environment.\n\nProblem statement\n\n7.\t\nModel iteration and improvement: The ML life cycle is an iterative process, and models often \nrequire improvement over time.", "mimetype": "text/plain", "start_char_idx": 311005, "end_char_idx": 311966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb08a789-9991-4a62-8535-d8c2d119ff7d": {"__data__": {"id_": "bb08a789-9991-4a62-8535-d8c2d119ff7d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfc1edf9-64d3-4107-9238-076f724b3f42", "node_type": "1", "metadata": {}, "hash": "691a6daed7e8b499a872aea6d34c4eb4ba7a9b136484b4b24661f10842ab8b05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d8ea624-c29e-4971-9cf5-bc20283668e0", "node_type": "1", "metadata": {}, "hash": "6f4a8054cf3e845190f51d3197255fe8bab0ae10b0ebe5e94957264b11f38ec7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It also includes periodic retraining of the model using new data to adapt to changing patterns \nor concepts. Model maintenance involves addressing model drift, updating dependencies, and \nmanaging the model\u2019s life cycle in the production environment.\n\nProblem statement\n\n7.\t\nModel iteration and improvement: The ML life cycle is an iterative process, and models often \nrequire improvement over time. Based on user feedback, performance metrics, and changing \nbusiness requirements, models may need to be updated, retrained, or replaced. Iteration and \nimprovement are essential for keeping the models up-to-date and ensuring they continue to \ndeliver accurate predictions.\nThe ML life cycle involves problem definition, data acquisition, data preparation, model training, model \ndeployment, model monitoring, and model iteration. Each stage plays a critical role in developing \nsuccessful ML solutions.", "mimetype": "text/plain", "start_char_idx": 311567, "end_char_idx": 312469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d8ea624-c29e-4971-9cf5-bc20283668e0": {"__data__": {"id_": "1d8ea624-c29e-4971-9cf5-bc20283668e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb08a789-9991-4a62-8535-d8c2d119ff7d", "node_type": "1", "metadata": {}, "hash": "cf217e127c7864f87c003f5e70dcfb4c734a795eca88844b7f125640bbc10ad1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "787f9c30-5634-4c73-9919-79abe8828d15", "node_type": "1", "metadata": {}, "hash": "df7dd3a52a3db0239fb78aaed00b0353b7408a79216511d9b20129fa0a34117e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Iteration and \nimprovement are essential for keeping the models up-to-date and ensuring they continue to \ndeliver accurate predictions.\nThe ML life cycle involves problem definition, data acquisition, data preparation, model training, model \ndeployment, model monitoring, and model iteration. Each stage plays a critical role in developing \nsuccessful ML solutions. By following a well-defined life cycle, organizations can effectively build, \ndeploy, and maintain ML models to solve complex problems and derive valuable insights from their data.\nProblem statement\nLet\u2019s dive into a case study where we\u2019ll explore the art of predicting house prices using historical data. \nPicture this: we have a treasure trove of valuable information about houses, including details such as \nzoning, lot area, building type, overall condition, year built, and sale price. Our goal is to harness the \npower of ML to accurately forecast the price of a new house that comes our way.", "mimetype": "text/plain", "start_char_idx": 312104, "end_char_idx": 313068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "787f9c30-5634-4c73-9919-79abe8828d15": {"__data__": {"id_": "787f9c30-5634-4c73-9919-79abe8828d15", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d8ea624-c29e-4971-9cf5-bc20283668e0", "node_type": "1", "metadata": {}, "hash": "6f4a8054cf3e845190f51d3197255fe8bab0ae10b0ebe5e94957264b11f38ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c35a4f8d-15d6-41e0-b4c8-a94810415cc2", "node_type": "1", "metadata": {}, "hash": "dcf8f9ce192413438f9a5273068c4b4d62e7274c56c795b7d8d3522c340cf678", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Problem statement\nLet\u2019s dive into a case study where we\u2019ll explore the art of predicting house prices using historical data. \nPicture this: we have a treasure trove of valuable information about houses, including details such as \nzoning, lot area, building type, overall condition, year built, and sale price. Our goal is to harness the \npower of ML to accurately forecast the price of a new house that comes our way.\nTo accomplish this feat, we\u2019ll embark on a journey to construct an ML model exclusively designed for \npredicting house prices. This model will leverage the existing historical data and incorporate additional \nfeatures. By carefully analyzing and understanding the relationships between these features and the \ncorresponding sale prices, our model will become a reliable tool for estimating the value of any new \nhouse that enters the market.\nTo achieve this, we will go through some of the steps defined in the previous section, where we talked \nabout the ML life cycle.", "mimetype": "text/plain", "start_char_idx": 312651, "end_char_idx": 313639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c35a4f8d-15d6-41e0-b4c8-a94810415cc2": {"__data__": {"id_": "c35a4f8d-15d6-41e0-b4c8-a94810415cc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "787f9c30-5634-4c73-9919-79abe8828d15", "node_type": "1", "metadata": {}, "hash": "df7dd3a52a3db0239fb78aaed00b0353b7408a79216511d9b20129fa0a34117e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eff9877d-a2a3-4a14-991f-0b71c20fe109", "node_type": "1", "metadata": {}, "hash": "27b5a14cf497a56f535a54e9bbca0fdc9ac2c05472cb319b3a4f70f1ca449196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This model will leverage the existing historical data and incorporate additional \nfeatures. By carefully analyzing and understanding the relationships between these features and the \ncorresponding sale prices, our model will become a reliable tool for estimating the value of any new \nhouse that enters the market.\nTo achieve this, we will go through some of the steps defined in the previous section, where we talked \nabout the ML life cycle. Since housing prices are continuous, we will use a linear regression model \nto predict these prices.\nWe will start by preparing the data to make it usable for an ML model.\nData preparation and feature engineering\nAs we know, data preparation and feature engineering are crucial steps in the ML process. Proper \ndata preparation and feature engineering techniques can significantly improve the performance and \naccuracy of models. In this section, we will explore common data preparation and feature engineering \ntasks with code examples.", "mimetype": "text/plain", "start_char_idx": 313196, "end_char_idx": 314177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eff9877d-a2a3-4a14-991f-0b71c20fe109": {"__data__": {"id_": "eff9877d-a2a3-4a14-991f-0b71c20fe109", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c35a4f8d-15d6-41e0-b4c8-a94810415cc2", "node_type": "1", "metadata": {}, "hash": "dcf8f9ce192413438f9a5273068c4b4d62e7274c56c795b7d8d3522c340cf678", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1674922-8c3e-44af-a3f2-e44025eb77a5", "node_type": "1", "metadata": {}, "hash": "287d5d3143b9d643c88ebd86dd10c1c4e1cfd819d19d0ae99f052f9d1e52c47a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will start by preparing the data to make it usable for an ML model.\nData preparation and feature engineering\nAs we know, data preparation and feature engineering are crucial steps in the ML process. Proper \ndata preparation and feature engineering techniques can significantly improve the performance and \naccuracy of models. In this section, we will explore common data preparation and feature engineering \ntasks with code examples.\nIntroduction to the dataset\nThe first step in building a model is to find the relevant data. We are going to use house price data \n(located at https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQp\nMiIMJrmiTGarbs/edit#gid=1150341366) for this purpose. This data has 2,920 rows and \n13 columns.", "mimetype": "text/plain", "start_char_idx": 313741, "end_char_idx": 314475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1674922-8c3e-44af-a3f2-e44025eb77a5": {"__data__": {"id_": "a1674922-8c3e-44af-a3f2-e44025eb77a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eff9877d-a2a3-4a14-991f-0b71c20fe109", "node_type": "1", "metadata": {}, "hash": "27b5a14cf497a56f535a54e9bbca0fdc9ac2c05472cb319b3a4f70f1ca449196", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70e3c771-f466-4eff-980a-cfce511b758c", "node_type": "1", "metadata": {}, "hash": "6fb161dea1057eed96e706f858b0b553aad4f188c4654681a53313e5219fc4a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We are going to use house price data \n(located at https://docs.google.com/spreadsheets/d/1caaR9pT24GNmq3rDQp\nMiIMJrmiTGarbs/edit#gid=1150341366) for this purpose. This data has 2,920 rows and \n13 columns.\n\nMachine Learning with Spark ML\n\nThis dataset has the following columns:\n\u2022\t Id: Unique identifier for each row of the data\n\u2022\t MSSubClass: Subclass of the property\n\u2022\t MSZoning: Zoning of the property\n\u2022\t LotArea: Total area of the lot where the property is situated\n\u2022\t LotConfig: Configuration of the lot \u2013 for example, if it\u2019s a corner lot\n\u2022\t BldgType: Type of home \u2013 for example, single, family,", "mimetype": "text/plain", "start_char_idx": 314271, "end_char_idx": 314871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70e3c771-f466-4eff-980a-cfce511b758c": {"__data__": {"id_": "70e3c771-f466-4eff-980a-cfce511b758c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1674922-8c3e-44af-a3f2-e44025eb77a5", "node_type": "1", "metadata": {}, "hash": "287d5d3143b9d643c88ebd86dd10c1c4e1cfd819d19d0ae99f052f9d1e52c47a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bf3a187-af73-40c4-bb1a-0930404fc821", "node_type": "1", "metadata": {}, "hash": "60f2668b15ce857862e11d0ff90b367002dcecdff3cd54ff8bc7f9b5247b5abb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "if it\u2019s a corner lot\n\u2022\t BldgType: Type of home \u2013 for example, single, family, and so on\n\u2022\t OverallCond: General condition of the house\n\u2022\t YearBuilt: The year the house was built in\n\u2022\t YearRemodAdd: The year any remodeling was done\n\u2022\t Exterior1st: Type of exterior \u2013 for example, vinyl, siding, and so on\n\u2022\t BsmtFinSF2: Total size of finished basement\n\u2022\t TotalBsmtSF: Total size of basement\n\u2022\t SalePrice: The sale price of the house\nWe will download this data from the link provided at the beginning of this section.\nNow that we know some of the data points that exist in the data, let\u2019s learn how to load it.\nLoading data\nAt this point, we already have the data downloaded on our computer and to our Databricks environment \nas a CSV file.", "mimetype": "text/plain", "start_char_idx": 314794, "end_char_idx": 315532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bf3a187-af73-40c4-bb1a-0930404fc821": {"__data__": {"id_": "5bf3a187-af73-40c4-bb1a-0930404fc821", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70e3c771-f466-4eff-980a-cfce511b758c", "node_type": "1", "metadata": {}, "hash": "6fb161dea1057eed96e706f858b0b553aad4f188c4654681a53313e5219fc4a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4", "node_type": "1", "metadata": {}, "hash": "bbc127d93381ab5f51945f8f3b3ea446b619ea59203c163f346bd9068c369fa0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now that we know some of the data points that exist in the data, let\u2019s learn how to load it.\nLoading data\nAt this point, we already have the data downloaded on our computer and to our Databricks environment \nas a CSV file. As you may recall from the previous chapters, we learned how to load a dataset into a \nDataFrame through various techniques. We will use a CSV file here to load the data:\nhousing_data = spark.read.csv(\"HousePricePrediction.csv\")\n# Printing first 5 records of the dataset\nhousing_data.show(5)\n\nProblem statement\n\nWe can see the result in Figure 8.1. Please note that we can see only part of the result in the image \nsince the dataset is too large to be displayed in full.", "mimetype": "text/plain", "start_char_idx": 315310, "end_char_idx": 316003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4": {"__data__": {"id_": "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bf3a187-af73-40c4-bb1a-0930404fc821", "node_type": "1", "metadata": {}, "hash": "60f2668b15ce857862e11d0ff90b367002dcecdff3cd54ff8bc7f9b5247b5abb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "476a25de-34d5-4641-8e65-40eb694c34fc", "node_type": "1", "metadata": {}, "hash": "dcaef618e95376709ff6d1f093aa3fc0be0859ce782965f29d1db15b9904aafb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will use a CSV file here to load the data:\nhousing_data = spark.read.csv(\"HousePricePrediction.csv\")\n# Printing first 5 records of the dataset\nhousing_data.show(5)\n\nProblem statement\n\nWe can see the result in Figure 8.1. Please note that we can see only part of the result in the image \nsince the dataset is too large to be displayed in full.:\nLet\u2019s print the schema of this dataset:\nhousing_data.printSchema\nWe will get the following schema:\n<bound method DataFrame.printSchema of DataFrame[Id: bigint, \nMSSubClass: bigint, MSZoning: string, LotArea: bigint, LotConfig: \nstring, BldgType: string, OverallCond: bigint, YearBuilt: bigint, \nYearRemodAdd: bigint, Exterior1st: string, BsmtFinSF2: bigint, \nTotalBsmtSF: bigint, SalePrice: bigint]>\nAs you may have noticed, some of the column types are strings.", "mimetype": "text/plain", "start_char_idx": 315658, "end_char_idx": 316467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "476a25de-34d5-4641-8e65-40eb694c34fc": {"__data__": {"id_": "476a25de-34d5-4641-8e65-40eb694c34fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4", "node_type": "1", "metadata": {}, "hash": "bbc127d93381ab5f51945f8f3b3ea446b619ea59203c163f346bd9068c369fa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff28540a-e68b-4306-bad2-7b96b0a9d3e2", "node_type": "1", "metadata": {}, "hash": "b93f793ea9edeb2a717bab4200ea93d00a80a14732f4a880bcc215989df298fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will clean up this data in the \nnext section.\nCleaning data\nCleaning the data involves handling missing values, outliers, and inconsistent data. Before we clean up \nthe data, we will see how many rows are in the data. We can do this by using the count() function:\nhousing_data.count()\nThe result of this statement is shown here:\n\nMachine Learning with Spark ML\n\nThis means the data contains 2,919 rows before we apply any cleaning. Now, we will drop missing \nvalues from this dataset, like so:\n# Remove rows with missing values\ncleaned_data = housing_data.dropna()\ncleaned_data.count()\nThe result of this code is as follows:\n\nThis shows that we have dropped some rows of data and that the data size is smaller now.\nIn the next section, we will discuss categorical variables and how to handle them, specifically those \nrepresented as strings in our example.", "mimetype": "text/plain", "start_char_idx": 316468, "end_char_idx": 317327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff28540a-e68b-4306-bad2-7b96b0a9d3e2": {"__data__": {"id_": "ff28540a-e68b-4306-bad2-7b96b0a9d3e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "476a25de-34d5-4641-8e65-40eb694c34fc", "node_type": "1", "metadata": {}, "hash": "dcaef618e95376709ff6d1f093aa3fc0be0859ce782965f29d1db15b9904aafb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f23ca4c5-064f-4d32-85a2-d27d994bd92f", "node_type": "1", "metadata": {}, "hash": "9d5e6d437c94131fbd9956c78a4c55a11f6325161e90c018a6ae9a8418bea1c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, we will drop missing \nvalues from this dataset, like so:\n# Remove rows with missing values\ncleaned_data = housing_data.dropna()\ncleaned_data.count()\nThe result of this code is as follows:\n\nThis shows that we have dropped some rows of data and that the data size is smaller now.\nIn the next section, we will discuss categorical variables and how to handle them, specifically those \nrepresented as strings in our example.\nHandling categorical variables\nIn the realm of statistics and data analysis, a categorical variable is a type of variable that represents \ncategories or groups and can take on a limited, fixed number of distinct values or levels. These variables \nsignify qualitative characteristics and do not possess inherent numerical significance or magnitude. \nInstead, they represent different attributes or labels that classify data into specific groups or classes. \nCategorical variables need to be encoded to numerical values before training machine learning models.\nIn our example, we have a few columns that are string types.", "mimetype": "text/plain", "start_char_idx": 316903, "end_char_idx": 317947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f23ca4c5-064f-4d32-85a2-d27d994bd92f": {"__data__": {"id_": "f23ca4c5-064f-4d32-85a2-d27d994bd92f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff28540a-e68b-4306-bad2-7b96b0a9d3e2", "node_type": "1", "metadata": {}, "hash": "b93f793ea9edeb2a717bab4200ea93d00a80a14732f4a880bcc215989df298fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2", "node_type": "1", "metadata": {}, "hash": "d94f484fe99ea32b20b1dc983f0ae3b198d4a822d813e01088e6cd10e6526444", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These variables \nsignify qualitative characteristics and do not possess inherent numerical significance or magnitude. \nInstead, they represent different attributes or labels that classify data into specific groups or classes. \nCategorical variables need to be encoded to numerical values before training machine learning models.\nIn our example, we have a few columns that are string types. Those need to be encoded into numerical \nvalues so that the model can correctly use them. For this purpose, we\u2019ll use Spark\u2019s StringIndexer \nlibrary to index the string columns.\nThe following code shows how to use StringIndexer:\n#import required libraries\nfrom pyspark.ml.feature import StringIndexer\nmszoning_indexer = StringIndexer(inputCol=\"MSZoning\", \noutputCol=\"MSZoningIndex\")\n#Fits a model to the input dataset with optional parameters.", "mimetype": "text/plain", "start_char_idx": 317558, "end_char_idx": 318391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2": {"__data__": {"id_": "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f23ca4c5-064f-4d32-85a2-d27d994bd92f", "node_type": "1", "metadata": {}, "hash": "9d5e6d437c94131fbd9956c78a4c55a11f6325161e90c018a6ae9a8418bea1c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5640bde-89b8-41e4-a8c2-042b4e6180a6", "node_type": "1", "metadata": {}, "hash": "fcb1f9c59bde3cd46490afe5d8475c5e7d8b8b3f20321bc8f81c4cb60bf2f9d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this purpose, we\u2019ll use Spark\u2019s StringIndexer \nlibrary to index the string columns.\nThe following code shows how to use StringIndexer:\n#import required libraries\nfrom pyspark.ml.feature import StringIndexer\nmszoning_indexer = StringIndexer(inputCol=\"MSZoning\", \noutputCol=\"MSZoningIndex\")\n#Fits a model to the input dataset with optional parameters.\ndf_mszoning = mszoning_indexer.fit(cleaned_data).transform(cleaned_\ndata)\ndf_mszoning.show()\nIn the preceding code, we are taking the MSZoning column and converting it into an indexed column. \nTo achieve this, we created a StringIndexer value by the name of mszoning indexer. We gave it \nMSZoning as the input column to work on. The output column\u2019s name is MSZoningIndex. We will \nuse this output column in the next step.", "mimetype": "text/plain", "start_char_idx": 318038, "end_char_idx": 318813, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5640bde-89b8-41e4-a8c2-042b4e6180a6": {"__data__": {"id_": "a5640bde-89b8-41e4-a8c2-042b4e6180a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2", "node_type": "1", "metadata": {}, "hash": "d94f484fe99ea32b20b1dc983f0ae3b198d4a822d813e01088e6cd10e6526444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73f10ddd-c20e-4e55-a685-1570d3e37c0e", "node_type": "1", "metadata": {}, "hash": "52945598259cb49a685c5150f5a55f906e38f3079c8e94bb8dcb7e9b07be03de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To achieve this, we created a StringIndexer value by the name of mszoning indexer. We gave it \nMSZoning as the input column to work on. The output column\u2019s name is MSZoningIndex. We will \nuse this output column in the next step. After that, we\u2019ll fit mszoning_indexer to cleaned_data.\nIn the resulting DataFrame, you will notice that one additional column was added by the name \nof MSZoningIndex.\n\nProblem statement\n\nNow, we will use a pipeline to transform all the features in the DataFrame.\nA pipeline brings together a series of essential steps, each contributing to transforming raw data into \nvaluable predictions and analyses. The pipeline serves as a structured pathway, composed of distinct \nstages or components, arranged in a specific order. Each stage represents a unique operation or \ntransformation that refines the data, molding it into a more suitable format for ML tasks.", "mimetype": "text/plain", "start_char_idx": 318585, "end_char_idx": 319472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73f10ddd-c20e-4e55-a685-1570d3e37c0e": {"__data__": {"id_": "73f10ddd-c20e-4e55-a685-1570d3e37c0e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5640bde-89b8-41e4-a8c2-042b4e6180a6", "node_type": "1", "metadata": {}, "hash": "fcb1f9c59bde3cd46490afe5d8475c5e7d8b8b3f20321bc8f81c4cb60bf2f9d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a410a23a-b234-4c96-92b5-15b8a2b73cbe", "node_type": "1", "metadata": {}, "hash": "95237c79ec5762ffdf8cd2d7c8764f34fc0772628048c7913e4402dfd4dd6b9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A pipeline brings together a series of essential steps, each contributing to transforming raw data into \nvaluable predictions and analyses. The pipeline serves as a structured pathway, composed of distinct \nstages or components, arranged in a specific order. Each stage represents a unique operation or \ntransformation that refines the data, molding it into a more suitable format for ML tasks.\nAt the heart of a pipeline lies its ability to seamlessly connect these stages, forming a well-coordinated \nflow of transformations. This orchestration ensures that the data flows effortlessly through each \nstage, with the output of one stage becoming the input for the next. It eradicates the need for manual \nintervention, automating the entire process and saving us valuable time and effort. We integrate a \nvariety of operations into the pipeline, such as data cleaning, feature engineering, encoding categorical \nvariables, scaling numerical features, and much more.", "mimetype": "text/plain", "start_char_idx": 319078, "end_char_idx": 320044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a410a23a-b234-4c96-92b5-15b8a2b73cbe": {"__data__": {"id_": "a410a23a-b234-4c96-92b5-15b8a2b73cbe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73f10ddd-c20e-4e55-a685-1570d3e37c0e", "node_type": "1", "metadata": {}, "hash": "52945598259cb49a685c5150f5a55f906e38f3079c8e94bb8dcb7e9b07be03de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79770ee6-0798-4906-922d-180f21e449a8", "node_type": "1", "metadata": {}, "hash": "b98d5abca784819b8675dfa8cc9b81eff67a32c37aa87b460caf99e692ee8a26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This orchestration ensures that the data flows effortlessly through each \nstage, with the output of one stage becoming the input for the next. It eradicates the need for manual \nintervention, automating the entire process and saving us valuable time and effort. We integrate a \nvariety of operations into the pipeline, such as data cleaning, feature engineering, encoding categorical \nvariables, scaling numerical features, and much more. Each operation plays its part in transforming \nthe data, to make it usable for the ML model.\nThe ML pipeline empowers us to streamline our workflows, experiment with different combinations \nof transformations, and maintain consistency in our data processing tasks. It provides a structured \nframework that allows us to effortlessly reproduce and share our work, fostering collaboration and \nfostering a deeper understanding of the data transformation process.", "mimetype": "text/plain", "start_char_idx": 319606, "end_char_idx": 320504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79770ee6-0798-4906-922d-180f21e449a8": {"__data__": {"id_": "79770ee6-0798-4906-922d-180f21e449a8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a410a23a-b234-4c96-92b5-15b8a2b73cbe", "node_type": "1", "metadata": {}, "hash": "95237c79ec5762ffdf8cd2d7c8764f34fc0772628048c7913e4402dfd4dd6b9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b", "node_type": "1", "metadata": {}, "hash": "7588ee301f08cbb9a671fa19d41bae87432037f2773373e1e0ab1c343eafc274", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each operation plays its part in transforming \nthe data, to make it usable for the ML model.\nThe ML pipeline empowers us to streamline our workflows, experiment with different combinations \nof transformations, and maintain consistency in our data processing tasks. It provides a structured \nframework that allows us to effortlessly reproduce and share our work, fostering collaboration and \nfostering a deeper understanding of the data transformation process.\nIn ML and data preprocessing, a one-hot encoder is a technique that\u2019s used to convert categorical \nvariables into a numerical format, allowing algorithms to better understand and process categorical \ndata. It\u2019s particularly useful when working with categorical features that lack ordinal relationships or \nnumerical representation.\nWe will use StringIndexer and OneHotEncoder in this pipeline. Let\u2019s see how we can achieve this:\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.", "mimetype": "text/plain", "start_char_idx": 320045, "end_char_idx": 321037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b": {"__data__": {"id_": "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79770ee6-0798-4906-922d-180f21e449a8", "node_type": "1", "metadata": {}, "hash": "b98d5abca784819b8675dfa8cc9b81eff67a32c37aa87b460caf99e692ee8a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2", "node_type": "1", "metadata": {}, "hash": "a37a2204672709adfe615136e1d90d888f7c891d1d27bdb14b8306431a90baaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\u2019s particularly useful when working with categorical features that lack ordinal relationships or \nnumerical representation.\nWe will use StringIndexer and OneHotEncoder in this pipeline. Let\u2019s see how we can achieve this:\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml import Pipeline\nmszoning_indexer = StringIndexer(inputCol=\"MSZoning\", \noutputCol=\"MSZoningIndex\")\nlotconfig_indexer = StringIndexer(inputCol=\"LotConfig\", \noutputCol=\"LotConfigIndex\")\nbldgtype_indexer = StringIndexer(inputCol=\"BldgType\", \noutputCol=\"BldgTypeIndex\")\nexterior1st_indexer = StringIndexer(inputCol=\"Exterior1st\", \noutputCol=\"Exterior1stIndex\")\nonehotencoder_mszoning_vector = \nOneHotEncoder(inputCol=\"MSZoningIndex\",", "mimetype": "text/plain", "start_char_idx": 320711, "end_char_idx": 321468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2": {"__data__": {"id_": "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b", "node_type": "1", "metadata": {}, "hash": "7588ee301f08cbb9a671fa19d41bae87432037f2773373e1e0ab1c343eafc274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de2ad7c2-f46f-400d-bbb5-75104477c545", "node_type": "1", "metadata": {}, "hash": "10e6d2821a70e7fe6f82943084b3c59c91c7540a92c80ce24cde82e1c018428f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "outputCol=\"LotConfigIndex\")\nbldgtype_indexer = StringIndexer(inputCol=\"BldgType\", \noutputCol=\"BldgTypeIndex\")\nexterior1st_indexer = StringIndexer(inputCol=\"Exterior1st\", \noutputCol=\"Exterior1stIndex\")\nonehotencoder_mszoning_vector = \nOneHotEncoder(inputCol=\"MSZoningIndex\", outputCol=\"MSZoningVector\")\nonehotencoder_lotconfig_vector = \nOneHotEncoder(inputCol=\"LotConfigIndex\", outputCol=\"LotConfigVector\")\nonehotencoder_bldgtype_vector = \nOneHotEncoder(inputCol=\"BldgTypeIndex\", outputCol=\"BldgTypeVector\")\nonehotencoder_exterior1st_vector = \nOneHotEncoder(inputCol=\"Exterior1stIndex\",", "mimetype": "text/plain", "start_char_idx": 321195, "end_char_idx": 321780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de2ad7c2-f46f-400d-bbb5-75104477c545": {"__data__": {"id_": "de2ad7c2-f46f-400d-bbb5-75104477c545", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2", "node_type": "1", "metadata": {}, "hash": "a37a2204672709adfe615136e1d90d888f7c891d1d27bdb14b8306431a90baaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe", "node_type": "1", "metadata": {}, "hash": "1363e22399ff7e0755e7dc40f06f12f65f9a2a3b9ce4d759c1ae381249d6b087", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "outputCol=\"MSZoningVector\")\nonehotencoder_lotconfig_vector = \nOneHotEncoder(inputCol=\"LotConfigIndex\", outputCol=\"LotConfigVector\")\nonehotencoder_bldgtype_vector = \nOneHotEncoder(inputCol=\"BldgTypeIndex\", outputCol=\"BldgTypeVector\")\nonehotencoder_exterior1st_vector = \nOneHotEncoder(inputCol=\"Exterior1stIndex\", \n\nMachine Learning with Spark ML\n\noutputCol=\"Exterior1stVector\")\n#Create pipeline and pass all stages\npipeline = Pipeline(stages=[mszoning_indexer,\nlotconfig_indexer,\nbldgtype_indexer,\nexterior1st_indexer,\nonehotencoder_mszoning_vector,\nonehotencoder_lotconfig_vector,\nonehotencoder_bldgtype_vector,\nonehotencoder_exterior1st_vector])\nTo begin our code, we import the required modules from the PySpark library.", "mimetype": "text/plain", "start_char_idx": 321469, "end_char_idx": 322191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe": {"__data__": {"id_": "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de2ad7c2-f46f-400d-bbb5-75104477c545", "node_type": "1", "metadata": {}, "hash": "10e6d2821a70e7fe6f82943084b3c59c91c7540a92c80ce24cde82e1c018428f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faf06094-75cc-43cd-a3b7-48709a602788", "node_type": "1", "metadata": {}, "hash": "c52189c660741a36220da56f62ee5df411830a7f8aa801343506be74ce417122", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "lotconfig_indexer,\nbldgtype_indexer,\nexterior1st_indexer,\nonehotencoder_mszoning_vector,\nonehotencoder_lotconfig_vector,\nonehotencoder_bldgtype_vector,\nonehotencoder_exterior1st_vector])\nTo begin our code, we import the required modules from the PySpark library. The StringIndexer \nand OneHotEncoder modules will be used to handle the string columns of the housing dataset.\nAs we embark on the process of transforming categorical columns into numerical representations \nthat can be understood by ML algorithms, let\u2019s take a closer look at the magic happening in our code.\nThe first step is to create StringIndexer instances for each categorical column we wish to \ntransform. Each instance takes an input column, such as MSZoning or LotConfig, and produces \na corresponding output column with a numerical index.", "mimetype": "text/plain", "start_char_idx": 321929, "end_char_idx": 322739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faf06094-75cc-43cd-a3b7-48709a602788": {"__data__": {"id_": "faf06094-75cc-43cd-a3b7-48709a602788", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe", "node_type": "1", "metadata": {}, "hash": "1363e22399ff7e0755e7dc40f06f12f65f9a2a3b9ce4d759c1ae381249d6b087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c6e5bf7-5917-4245-96f5-69674b123640", "node_type": "1", "metadata": {}, "hash": "8a34cf361fb26a5f489d168778212d8043eca842001c3859898e192c81a79041", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we embark on the process of transforming categorical columns into numerical representations \nthat can be understood by ML algorithms, let\u2019s take a closer look at the magic happening in our code.\nThe first step is to create StringIndexer instances for each categorical column we wish to \ntransform. Each instance takes an input column, such as MSZoning or LotConfig, and produces \na corresponding output column with a numerical index. For example, the MSZoningIndex column \ncaptures the transformed index values of the MSZoning column.\nWith the categorical columns successfully indexed, we progress to the next stage. Now, we want to \nconvert these indices into binary vectors. For that, we can use OneHotEncoder. The resulting \nvectors represent each categorical value as a binary array, with a value of 1 indicating the presence of \nthat category and 0 otherwise.", "mimetype": "text/plain", "start_char_idx": 322303, "end_char_idx": 323170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c6e5bf7-5917-4245-96f5-69674b123640": {"__data__": {"id_": "0c6e5bf7-5917-4245-96f5-69674b123640", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf06094-75cc-43cd-a3b7-48709a602788", "node_type": "1", "metadata": {}, "hash": "c52189c660741a36220da56f62ee5df411830a7f8aa801343506be74ce417122", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1371a77d-5622-4ba8-abdd-ccf4227cf76f", "node_type": "1", "metadata": {}, "hash": "1a2eeb501c96340afa9f22a678e0c5900c7bbf7e6ae1c3752b7d56c861c5f8f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With the categorical columns successfully indexed, we progress to the next stage. Now, we want to \nconvert these indices into binary vectors. For that, we can use OneHotEncoder. The resulting \nvectors represent each categorical value as a binary array, with a value of 1 indicating the presence of \nthat category and 0 otherwise.\nWe create OneHotEncoder instances for each indexed column, such as MSZoningIndex or \nLotConfigIndex, and generate new output columns holding the binary vector representations. \nThese output columns, such as MSZoningVector or LotConfigVector, are used to capture \nthe encoded information.\nAs our code progresses, we assemble a pipeline \u2013 a sequence of transformations \u2013 where each \ntransformation represents a stage. In our case, each stage encompasses the steps of indexing and \none-hot encoding for a specific categorical column. We arrange the stages in the pipeline, ensuring \nthe correct order of transformations.", "mimetype": "text/plain", "start_char_idx": 322841, "end_char_idx": 323788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1371a77d-5622-4ba8-abdd-ccf4227cf76f": {"__data__": {"id_": "1371a77d-5622-4ba8-abdd-ccf4227cf76f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c6e5bf7-5917-4245-96f5-69674b123640", "node_type": "1", "metadata": {}, "hash": "8a34cf361fb26a5f489d168778212d8043eca842001c3859898e192c81a79041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f126aed6-3569-4d33-8dcf-e136f9e77880", "node_type": "1", "metadata": {}, "hash": "71384e7422e848b0fc816ffdd6a1459a7c316cc7a79a72231dd66ef4b2873aa2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These output columns, such as MSZoningVector or LotConfigVector, are used to capture \nthe encoded information.\nAs our code progresses, we assemble a pipeline \u2013 a sequence of transformations \u2013 where each \ntransformation represents a stage. In our case, each stage encompasses the steps of indexing and \none-hot encoding for a specific categorical column. We arrange the stages in the pipeline, ensuring \nthe correct order of transformations.\nBy structuring our pipeline, we orchestrate a seamless flow of operations. The pipeline connects the \ndots between different stages, making it effortless to apply these transformations to our dataset as a \nwhole. Our pipeline acts as a conductor, leading our data through the transformations, ultimately \nmaking it into a format ready for ML.", "mimetype": "text/plain", "start_char_idx": 323348, "end_char_idx": 324131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f126aed6-3569-4d33-8dcf-e136f9e77880": {"__data__": {"id_": "f126aed6-3569-4d33-8dcf-e136f9e77880", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1371a77d-5622-4ba8-abdd-ccf4227cf76f", "node_type": "1", "metadata": {}, "hash": "1a2eeb501c96340afa9f22a678e0c5900c7bbf7e6ae1c3752b7d56c861c5f8f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a71b0b0f-490e-4dce-b09f-3b0e403f73a5", "node_type": "1", "metadata": {}, "hash": "724ae1762db989e3f0828aa0d8fdbd42e74b8611b8a4d409f78a5053f792d5e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We arrange the stages in the pipeline, ensuring \nthe correct order of transformations.\nBy structuring our pipeline, we orchestrate a seamless flow of operations. The pipeline connects the \ndots between different stages, making it effortless to apply these transformations to our dataset as a \nwhole. Our pipeline acts as a conductor, leading our data through the transformations, ultimately \nmaking it into a format ready for ML.\n\nProblem statement\n\nNow, we will fit this pipeline to our cleaned dataset so that all the columns can be transformed together:\ndf_transformed = pipeline.fit(cleaned_data).transform(cleaned_data)\ndf_transformed.show(5)\nThe resulting DataFrame will have the additional columns that we created in the pipeline with \ntransformation. We have created index and vector columns for each of the string columns.\nNow, we need to remove the unnecessary and redundant columns from our dataset. We will do this \nin the next section.", "mimetype": "text/plain", "start_char_idx": 323702, "end_char_idx": 324650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a71b0b0f-490e-4dce-b09f-3b0e403f73a5": {"__data__": {"id_": "a71b0b0f-490e-4dce-b09f-3b0e403f73a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f126aed6-3569-4d33-8dcf-e136f9e77880", "node_type": "1", "metadata": {}, "hash": "71384e7422e848b0fc816ffdd6a1459a7c316cc7a79a72231dd66ef4b2873aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62857948-04f1-4c60-86a7-f43a859465ef", "node_type": "1", "metadata": {}, "hash": "2a032a165e94601afa19181429d14fcfde81b0606177b95fafe1422e9a198838", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have created index and vector columns for each of the string columns.\nNow, we need to remove the unnecessary and redundant columns from our dataset. We will do this \nin the next section.\nData cleanup\nIn this step, we will make sure that we are only using the features needed by ML. To achieve this, we \nwill remove different additional columns, such as the identity column, which don\u2019t serve the model. \nMoreover, we will also remove the features that we have already applied transformations to, such as \nstring columns.\nThe following code shows how to delete the columns:\ndrop_column_list = [\"Id\", \"MSZoning\",\"LotConfig\",\"BldgType\", \n\"Exterior1st\"]\ndf_dropped_cols = df_transformed.select([column for column in df_\ntransformed.columns if column not in drop_column_list])\ndf_dropped_cols.columns\nHere\u2019s the result:\n['MSSubClass',", "mimetype": "text/plain", "start_char_idx": 324461, "end_char_idx": 325293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62857948-04f1-4c60-86a7-f43a859465ef": {"__data__": {"id_": "62857948-04f1-4c60-86a7-f43a859465ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a71b0b0f-490e-4dce-b09f-3b0e403f73a5", "node_type": "1", "metadata": {}, "hash": "724ae1762db989e3f0828aa0d8fdbd42e74b8611b8a4d409f78a5053f792d5e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7", "node_type": "1", "metadata": {}, "hash": "0f59384d12170b8f2a991fca53a0ec85376e2b7290b5b5a326e7ac49233f3e44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following code shows how to delete the columns:\ndrop_column_list = [\"Id\", \"MSZoning\",\"LotConfig\",\"BldgType\", \n\"Exterior1st\"]\ndf_dropped_cols = df_transformed.select([column for column in df_\ntransformed.columns if column not in drop_column_list])\ndf_dropped_cols.columns\nHere\u2019s the result:\n['MSSubClass',\n 'LotArea',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF2',\n 'TotalBsmtSF',\n 'SalePrice',\n 'MSZoningIndex',\n 'LotConfigIndex',\n 'BldgTypeIndex',\n 'Exterior1stIndex',\n 'MSZoningVector',\n 'LotConfigVector',\n 'BldgTypeVector',", "mimetype": "text/plain", "start_char_idx": 324985, "end_char_idx": 325531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7": {"__data__": {"id_": "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62857948-04f1-4c60-86a7-f43a859465ef", "node_type": "1", "metadata": {}, "hash": "2a032a165e94601afa19181429d14fcfde81b0606177b95fafe1422e9a198838", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2aca2bf-0501-48c3-b8c3-897167c19b36", "node_type": "1", "metadata": {}, "hash": "e3daee0d452112949df82f9c70a1824d11856201bce9611fafde461bd599197b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF2',\n 'TotalBsmtSF',\n 'SalePrice',\n 'MSZoningIndex',\n 'LotConfigIndex',\n 'BldgTypeIndex',\n 'Exterior1stIndex',\n 'MSZoningVector',\n 'LotConfigVector',\n 'BldgTypeVector',\n 'Exterior1stVector']\n\nMachine Learning with Spark ML\n\nAs you can see from the resulting column list, the Id, MSZoning, LotConfig, BldgType, and \nExterior1st columns have been deleted from the resulting DataFrame.\nThe next step in the process is assembling the data.\nAssembling the vector\nIn this step, we will assemble a vector based on the features that we want. This step is necessary for \nSpark ML to work with data.", "mimetype": "text/plain", "start_char_idx": 325307, "end_char_idx": 325952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2aca2bf-0501-48c3-b8c3-897167c19b36": {"__data__": {"id_": "a2aca2bf-0501-48c3-b8c3-897167c19b36", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7", "node_type": "1", "metadata": {}, "hash": "0f59384d12170b8f2a991fca53a0ec85376e2b7290b5b5a326e7ac49233f3e44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "411ab0f0-1d4d-4c98-ab72-fabcef265118", "node_type": "1", "metadata": {}, "hash": "afd767da5026485dec45a63edb69f18b53e08afe85636f857145c8905b2d9e30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the Id, MSZoning, LotConfig, BldgType, and \nExterior1st columns have been deleted from the resulting DataFrame.\nThe next step in the process is assembling the data.\nAssembling the vector\nIn this step, we will assemble a vector based on the features that we want. This step is necessary for \nSpark ML to work with data.\nThe following code captures how we can achieve this:\nfrom pyspark.ml.feature import VectorAssembler\n#Assembling features\nfeature_assembly = VectorAssembler(inputCols = ['MSSubClass',\n 'LotArea',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF2',\n 'TotalBsmtSF',\n 'MSZoningIndex',\n 'LotConfigIndex',\n 'BldgTypeIndex',\n 'Exterior1stIndex',\n 'MSZoningVector',\n 'LotConfigVector',\n 'BldgTypeVector',\n 'Exterior1stVector'], outputCol = 'features')\noutput = feature_assembly.transform(df_dropped_cols)\noutput.show(3)\nIn the preceding code block, we have created a features column that contains the assembled vector.", "mimetype": "text/plain", "start_char_idx": 325634, "end_char_idx": 326573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "411ab0f0-1d4d-4c98-ab72-fabcef265118": {"__data__": {"id_": "411ab0f0-1d4d-4c98-ab72-fabcef265118", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2aca2bf-0501-48c3-b8c3-897167c19b36", "node_type": "1", "metadata": {}, "hash": "e3daee0d452112949df82f9c70a1824d11856201bce9611fafde461bd599197b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff5efbe0-f2e2-4c85-ba00-048af7280fd1", "node_type": "1", "metadata": {}, "hash": "0bb625ace8f9252feb0f70a330b3bf034d2f283f060cb52e6c5432506aceba90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will use this column for our model training after scaling it.\nOnce the vector has been assembled, the next step in the process is to scale the data.\nFeature scaling\nFeature scaling ensures that all features are on a similar scale, preventing certain features from \ndominating the learning process.\n\nProblem statement\n\nFor this, we can use the following code:\n#Normalizing the features\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol=\"features\", \noutputCol=\"scaledFeatures\",withStd=True, withMean=False)\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(output)\n# Normalize each feature to have unit standard deviation.", "mimetype": "text/plain", "start_char_idx": 326575, "end_char_idx": 327261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff5efbe0-f2e2-4c85-ba00-048af7280fd1": {"__data__": {"id_": "ff5efbe0-f2e2-4c85-ba00-048af7280fd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "411ab0f0-1d4d-4c98-ab72-fabcef265118", "node_type": "1", "metadata": {}, "hash": "afd767da5026485dec45a63edb69f18b53e08afe85636f857145c8905b2d9e30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b680c56a-403d-4cb0-8ac0-2caa70e22897", "node_type": "1", "metadata": {}, "hash": "b044072a5c21f961cdc4e25377a2c960c6e6d94efbbd5bd14a0b987726d44e74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Problem statement\n\nFor this, we can use the following code:\n#Normalizing the features\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol=\"features\", \noutputCol=\"scaledFeatures\",withStd=True, withMean=False)\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(output)\n# Normalize each feature to have unit standard deviation.\nscaledOutput = scalerModel.transform(output)\nscaledOutput.show(3)\nThe following code selects only the scaled features and the target column \u2013 that is, SalePrice:\n#Selecting input and output column from output\ndf_model_final = scaledOutput.select(['SalePrice', 'scaledFeatures'])\ndf_model_final.show(3)\nWe\u2019ll get the following output:\n+---------+--------------------+\n|SalePrice|scaledFeatures|\n+---------+--------------------+\n|208500|(37,[0,1,2,3,4,6,...|\n|181500|(37,[0,1,2,3,4,6,...|\n|223500|(37,[0,1,2,3,4,6,...|\n+---------+--------------------+\nAs you can see, df_model_final now only has two columns.", "mimetype": "text/plain", "start_char_idx": 326877, "end_char_idx": 327868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b680c56a-403d-4cb0-8ac0-2caa70e22897": {"__data__": {"id_": "b680c56a-403d-4cb0-8ac0-2caa70e22897", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff5efbe0-f2e2-4c85-ba00-048af7280fd1", "node_type": "1", "metadata": {}, "hash": "0bb625ace8f9252feb0f70a330b3bf034d2f283f060cb52e6c5432506aceba90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9", "node_type": "1", "metadata": {}, "hash": "9db9bd22ab409ff4e783819f2ca2eaab996211adacfa991b15f5507272e8b7c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SalePrice is the column that \nwe\u2019re going to predict so that is our target column. scaledFeatures contains all the features that \nwe are going to use to train our ML model.\nThese examples demonstrate common data preparation and feature engineering tasks using PySpark. \nHowever, the specific techniques and methods applied may vary, depending on the dataset and the \nrequirements of the ML task. It is essential to understand the characteristics of the data and choose \nappropriate techniques to preprocess and engineer features effectively. Proper data preparation and \nfeature engineering lay the foundation for building accurate and robust ML models.\nThe next step in this process is training and evaluating the ML model.\n\nMachine Learning with Spark ML\n\nModel training and evaluation\nModel training and evaluation are crucial steps in the ML process. In this section, we will explore \nhow to train ML models and evaluate their performance using various metrics and techniques. We \nwill use PySpark as the framework for model training and evaluation.", "mimetype": "text/plain", "start_char_idx": 327869, "end_char_idx": 328922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9": {"__data__": {"id_": "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b680c56a-403d-4cb0-8ac0-2caa70e22897", "node_type": "1", "metadata": {}, "hash": "b044072a5c21f961cdc4e25377a2c960c6e6d94efbbd5bd14a0b987726d44e74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b46f3a14-c8e1-4b2a-833a-efc217e15506", "node_type": "1", "metadata": {}, "hash": "620f2758487badbc6a47e9b0454884f10834c5cb3fac5eede82d903c4e4a8e22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The next step in this process is training and evaluating the ML model.\n\nMachine Learning with Spark ML\n\nModel training and evaluation\nModel training and evaluation are crucial steps in the ML process. In this section, we will explore \nhow to train ML models and evaluate their performance using various metrics and techniques. We \nwill use PySpark as the framework for model training and evaluation.\nSplitting the data\nBefore training a model, it is important to split the dataset into training and testing sets. The training \nset is used to train the model, while the testing set is used to evaluate its performance.", "mimetype": "text/plain", "start_char_idx": 328523, "end_char_idx": 329140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b46f3a14-c8e1-4b2a-833a-efc217e15506": {"__data__": {"id_": "b46f3a14-c8e1-4b2a-833a-efc217e15506", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9", "node_type": "1", "metadata": {}, "hash": "9db9bd22ab409ff4e783819f2ca2eaab996211adacfa991b15f5507272e8b7c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d1ae5c1-fb8e-488f-baf5-06c71192c66f", "node_type": "1", "metadata": {}, "hash": "22d01955b560fcfbaf9e1c5abe8031b56a4cb05ee6a048d205e1afca45b9baaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this section, we will explore \nhow to train ML models and evaluate their performance using various metrics and techniques. We \nwill use PySpark as the framework for model training and evaluation.\nSplitting the data\nBefore training a model, it is important to split the dataset into training and testing sets. The training \nset is used to train the model, while the testing set is used to evaluate its performance. Here\u2019s an \nexample of how to split the data using PySpark:\n#test train split\ndf_train, df_test = df_model_final.randomSplit([0.75, 0.25])\nIn the preceding code, we are doing a random split of the data, putting 75% of the data into the \ntraining set and 25% of the data into the test set. There are other split techniques as well. You should \nlook at your data carefully and then define the split that works best for your data and model training.", "mimetype": "text/plain", "start_char_idx": 328724, "end_char_idx": 329586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d1ae5c1-fb8e-488f-baf5-06c71192c66f": {"__data__": {"id_": "3d1ae5c1-fb8e-488f-baf5-06c71192c66f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b46f3a14-c8e1-4b2a-833a-efc217e15506", "node_type": "1", "metadata": {}, "hash": "620f2758487badbc6a47e9b0454884f10834c5cb3fac5eede82d903c4e4a8e22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e7fdcc-3872-4afe-a876-c0f6a4729dc1", "node_type": "1", "metadata": {}, "hash": "cf1a24f85694d8da749581451e38cad662bfaf754018a9898742f16d8f80fb05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are other split techniques as well. You should \nlook at your data carefully and then define the split that works best for your data and model training.\nThe reason we split the data is that once we train the model, we want to see how the trained model \npredicts on a dataset that it has never seen. In this case, that is our test dataset. This would help us \nevaluate the model and determine the quality of the model. Based on this, we can deploy different \ntechniques to improve our model.\nThe next step is model training.\nModel training\nOnce the data has been split, we can train an ML model on the training data. PySpark provides a \nwide range of algorithms for various types of ML tasks. For this example, we are going to use linear \nregression as our model of choice.", "mimetype": "text/plain", "start_char_idx": 329429, "end_char_idx": 330206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82e7fdcc-3872-4afe-a876-c0f6a4729dc1": {"__data__": {"id_": "82e7fdcc-3872-4afe-a876-c0f6a4729dc1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d1ae5c1-fb8e-488f-baf5-06c71192c66f", "node_type": "1", "metadata": {}, "hash": "22d01955b560fcfbaf9e1c5abe8031b56a4cb05ee6a048d205e1afca45b9baaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13f1038b-c82c-4105-bc97-ccce611ef899", "node_type": "1", "metadata": {}, "hash": "15440d033fe92f54344b13af261a0d12cbafeb81fbd2ff2cb42ef0086a9fc0aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Based on this, we can deploy different \ntechniques to improve our model.\nThe next step is model training.\nModel training\nOnce the data has been split, we can train an ML model on the training data. PySpark provides a \nwide range of algorithms for various types of ML tasks. For this example, we are going to use linear \nregression as our model of choice.\nHere\u2019s an example of training a linear regression model:\nfrom pyspark.ml.regression import LinearRegression\n# Instantiate the linear regression model\nregressor = LinearRegression(featuresCol = 'scaledFeatures', labelCol \n= 'SalePrice')\n# Fit the model on the training data\nregressor = regressor.fit(df_train)\nIn the preceding code, we are using the training data and fitting it into a linear regressor model. We \nalso added a parameter for labelCol that tells the model that this is the column that is our target \ncolumn to predict.", "mimetype": "text/plain", "start_char_idx": 329852, "end_char_idx": 330739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13f1038b-c82c-4105-bc97-ccce611ef899": {"__data__": {"id_": "13f1038b-c82c-4105-bc97-ccce611ef899", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e7fdcc-3872-4afe-a876-c0f6a4729dc1", "node_type": "1", "metadata": {}, "hash": "cf1a24f85694d8da749581451e38cad662bfaf754018a9898742f16d8f80fb05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb53b8c4-f434-457b-bb22-b65ff52d837a", "node_type": "1", "metadata": {}, "hash": "bc67ec1f0482b12863154d5b06a0ac17b5731ae5846c885050624d37cfbbf6f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We \nalso added a parameter for labelCol that tells the model that this is the column that is our target \ncolumn to predict.\nOnce the model has been trained, the next step is to determine how good the model is. We\u2019ll do this \nin the next section by evaluating the model.\n\nProblem statement\n\nModel evaluation\nAfter training the model, we need to evaluate its performance on the test data. Evaluation metrics \nprovide insights into how well the model is performing.\nMean squared error (MSE) is a fundamental statistical metric that\u2019s used to evaluate the performance \nof regression models by quantifying the average of the squared differences between predicted and \nactual values.\nR-squared, often denoted as R2, is a statistical measure that represents the proportion of the variance \nin the dependent variable that is predictable or explained by the independent variables in a regression \nmodel. It serves as an indicator of how well the independent variables explain the variability of the \ndependent variable.", "mimetype": "text/plain", "start_char_idx": 330616, "end_char_idx": 331626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb53b8c4-f434-457b-bb22-b65ff52d837a": {"__data__": {"id_": "fb53b8c4-f434-457b-bb22-b65ff52d837a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13f1038b-c82c-4105-bc97-ccce611ef899", "node_type": "1", "metadata": {}, "hash": "15440d033fe92f54344b13af261a0d12cbafeb81fbd2ff2cb42ef0086a9fc0aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "531880ca-8e66-4f4f-8783-fed0a61b6ca0", "node_type": "1", "metadata": {}, "hash": "95db3f7870cad3e2e32c7ded212f5076b2bd78f5204022c278032afacec0a077", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R-squared, often denoted as R2, is a statistical measure that represents the proportion of the variance \nin the dependent variable that is predictable or explained by the independent variables in a regression \nmodel. It serves as an indicator of how well the independent variables explain the variability of the \ndependent variable.\nHere\u2019s an example of evaluating a regression model using the MSE and R2 metrics:\n#MSE for the train data\npred_results = regressor.evaluate(df_train)\nprint(\"The train MSE for the model is: %2f\"% pred_results.", "mimetype": "text/plain", "start_char_idx": 331294, "end_char_idx": 331834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "531880ca-8e66-4f4f-8783-fed0a61b6ca0": {"__data__": {"id_": "531880ca-8e66-4f4f-8783-fed0a61b6ca0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb53b8c4-f434-457b-bb22-b65ff52d837a", "node_type": "1", "metadata": {}, "hash": "bc67ec1f0482b12863154d5b06a0ac17b5731ae5846c885050624d37cfbbf6f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a856c980-97cd-4458-b0f5-a774f6f540b1", "node_type": "1", "metadata": {}, "hash": "4f5fac4500066a88a6700eb76c729f9e22aa1de3b5a8ef4ae29c505e2b195db3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It serves as an indicator of how well the independent variables explain the variability of the \ndependent variable.\nHere\u2019s an example of evaluating a regression model using the MSE and R2 metrics:\n#MSE for the train data\npred_results = regressor.evaluate(df_train)\nprint(\"The train MSE for the model is: %2f\"% pred_results.\nmeanAbsoluteError)\nprint(\"The train r2 for the model is: %2f\"% pred_results.r2)\nHere\u2019s the result:\nThe MSE for the model is: 32287.153682\nThe r2 for the model is: 0.614926\nWe can check the test data\u2019s performance as depicted here:\n#Checking test performance\npred_results = regressor.evaluate(df_test)\nprint(\"The test MSE for the model is: %2f\"% pred_results.", "mimetype": "text/plain", "start_char_idx": 331511, "end_char_idx": 332193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a856c980-97cd-4458-b0f5-a774f6f540b1": {"__data__": {"id_": "a856c980-97cd-4458-b0f5-a774f6f540b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "531880ca-8e66-4f4f-8783-fed0a61b6ca0", "node_type": "1", "metadata": {}, "hash": "95db3f7870cad3e2e32c7ded212f5076b2bd78f5204022c278032afacec0a077", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be5db69-892a-4f30-8f8f-579119d8d870", "node_type": "1", "metadata": {}, "hash": "e4f5fe6abde113c07d7bcbd7863b6cd418fa90b2d44313bd7a0ba22dbf8e96ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "meanAbsoluteError)\nprint(\"The test r2 for the model is: %2f\"% pred_results.r2)\nHere\u2019s the result:\nThe MSE for the model is: 31668.331218\nThe r2 for the model is: 0.613300\nBased on the results of the model, we can tune it further. We\u2019ll see some of the techniques to achieve \nthis in the next section.\nCross-validation\nCross-validation is one of the different methods to improve an ML model\u2019s performance.\n\nMachine Learning with Spark ML\n\nCross-validation is used to assess the model\u2019s performance more robustly by dividing the data into \nmultiple subsets for training and evaluation. So, instead of just using train and test data, we use a \nvalidation set as well, where the model never sees that data and is only used for measuring performance.", "mimetype": "text/plain", "start_char_idx": 332194, "end_char_idx": 332939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3be5db69-892a-4f30-8f8f-579119d8d870": {"__data__": {"id_": "3be5db69-892a-4f30-8f8f-579119d8d870", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a856c980-97cd-4458-b0f5-a774f6f540b1", "node_type": "1", "metadata": {}, "hash": "4f5fac4500066a88a6700eb76c729f9e22aa1de3b5a8ef4ae29c505e2b195db3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433", "node_type": "1", "metadata": {}, "hash": "46e0061d60bc72e2734f766437eadab1843650e295c76c84338e0ed7f21b0870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cross-validation\nCross-validation is one of the different methods to improve an ML model\u2019s performance.\n\nMachine Learning with Spark ML\n\nCross-validation is used to assess the model\u2019s performance more robustly by dividing the data into \nmultiple subsets for training and evaluation. So, instead of just using train and test data, we use a \nvalidation set as well, where the model never sees that data and is only used for measuring performance.\nCross-validation follows a simple principle: rather than relying on a single train-test split, we divide \nour dataset into multiple subsets, or folds. Each fold acts as a mini train-test split, with a portion of \nthe data used for training and the remainder reserved for testing. By rotating the folds, we ensure that \nevery data point gets an opportunity to be part of the test set, thereby mitigating biases and providing \na more representative evaluation.\nThe most common form of cross-validation is k-fold cross-validation.", "mimetype": "text/plain", "start_char_idx": 332495, "end_char_idx": 333467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433": {"__data__": {"id_": "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3be5db69-892a-4f30-8f8f-579119d8d870", "node_type": "1", "metadata": {}, "hash": "e4f5fe6abde113c07d7bcbd7863b6cd418fa90b2d44313bd7a0ba22dbf8e96ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07", "node_type": "1", "metadata": {}, "hash": "5a183d2a7dfa09a1964236ba58b559ed59c87c08f2aa24382cc2f19648510ab7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each fold acts as a mini train-test split, with a portion of \nthe data used for training and the remainder reserved for testing. By rotating the folds, we ensure that \nevery data point gets an opportunity to be part of the test set, thereby mitigating biases and providing \na more representative evaluation.\nThe most common form of cross-validation is k-fold cross-validation. In this method, the dataset is \ndivided into k equal-sized folds. The model is trained and evaluated k times, with each fold serving as the \ntest set once while the remaining folds collectively form the training set. By averaging the performance \nmetrics obtained from each iteration, we obtain a more robust estimation of our model\u2019s performance.\nThrough cross-validation, we gain valuable insights into the generalization capabilities of our model. \nIt allows us to gauge its performance across different subsets of the data, capturing the inherent \nvariations and nuances that exist within our dataset.", "mimetype": "text/plain", "start_char_idx": 333091, "end_char_idx": 334073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07": {"__data__": {"id_": "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433", "node_type": "1", "metadata": {}, "hash": "46e0061d60bc72e2734f766437eadab1843650e295c76c84338e0ed7f21b0870", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c8979b2-5470-4054-a04c-e08b4efd6381", "node_type": "1", "metadata": {}, "hash": "809c8c54bc300cd14dd6d89b84e48114c81f615381e3491628ffafd321b33435", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By averaging the performance \nmetrics obtained from each iteration, we obtain a more robust estimation of our model\u2019s performance.\nThrough cross-validation, we gain valuable insights into the generalization capabilities of our model. \nIt allows us to gauge its performance across different subsets of the data, capturing the inherent \nvariations and nuances that exist within our dataset. This technique helps us detect potential issues \nsuch as overfitting, where the model performs exceptionally well on the training set but fails to \ngeneralize to unseen data.\nIn addition to k-fold cross-validation, there are variations and extensions tailored to specific scenarios. \nStratified cross-validation ensures that each fold maintains the same class distribution as the original \ndataset, preserving the representativeness of the splits. Leave-one-out cross-validation, on the other \nhand, treats each data point as a separate fold, providing a stringent evaluation but at the cost of \nincreased computational complexity.\nNext, we will learn about hyperparameter tuning.", "mimetype": "text/plain", "start_char_idx": 333685, "end_char_idx": 334754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c8979b2-5470-4054-a04c-e08b4efd6381": {"__data__": {"id_": "3c8979b2-5470-4054-a04c-e08b4efd6381", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07", "node_type": "1", "metadata": {}, "hash": "5a183d2a7dfa09a1964236ba58b559ed59c87c08f2aa24382cc2f19648510ab7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77fea69e-7952-4016-97dd-23e453e6af4e", "node_type": "1", "metadata": {}, "hash": "6e71ed3c340f239cc9f88802285d11e260a5410e122bf3f72fe7e7a1096b53a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stratified cross-validation ensures that each fold maintains the same class distribution as the original \ndataset, preserving the representativeness of the splits. Leave-one-out cross-validation, on the other \nhand, treats each data point as a separate fold, providing a stringent evaluation but at the cost of \nincreased computational complexity.\nNext, we will learn about hyperparameter tuning.\nHyperparameter tuning\nHyperparameter tuning is the process of optimizing the hyperparameters of an ML algorithm to \nimprove its performance. Hyperparameters are settings or configurations that are external to the \nmodel and cannot be learned from the training data directly. Unlike model parameters, which are \nlearned during the training process, hyperparameters need to be specified beforehand and are crucial \nin determining the behavior and performance of an ML model. We will use hyperparameter tuning \nto improve model performance. Hyperparameters are parameters that are not learned from the data \nbut are set before training.", "mimetype": "text/plain", "start_char_idx": 334358, "end_char_idx": 335388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77fea69e-7952-4016-97dd-23e453e6af4e": {"__data__": {"id_": "77fea69e-7952-4016-97dd-23e453e6af4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c8979b2-5470-4054-a04c-e08b4efd6381", "node_type": "1", "metadata": {}, "hash": "809c8c54bc300cd14dd6d89b84e48114c81f615381e3491628ffafd321b33435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a775a992-453d-4e07-9f58-27ebd8679bbe", "node_type": "1", "metadata": {}, "hash": "d93b8578a6a704f852fcb3a67b965d14df59ece290d18b61b862911d61221e5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unlike model parameters, which are \nlearned during the training process, hyperparameters need to be specified beforehand and are crucial \nin determining the behavior and performance of an ML model. We will use hyperparameter tuning \nto improve model performance. Hyperparameters are parameters that are not learned from the data \nbut are set before training. Tuning hyperparameters can significantly impact the model\u2019s performance.\nPicture this: our model is a complex piece of machinery, composed of various knobs and levers known \nas hyperparameters. These hyperparameters govern the behavior and characteristics of our model, \ninfluencing its ability to learn, generalize, and make accurate predictions. However, finding the optimal \nconfiguration for these hyperparameters is no easy feat.\n\nProblem statement\n\nHyperparameter tuning is the art of systematically searching and selecting the best combination \nof hyperparameters for our model.", "mimetype": "text/plain", "start_char_idx": 335030, "end_char_idx": 335974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a775a992-453d-4e07-9f58-27ebd8679bbe": {"__data__": {"id_": "a775a992-453d-4e07-9f58-27ebd8679bbe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77fea69e-7952-4016-97dd-23e453e6af4e", "node_type": "1", "metadata": {}, "hash": "6e71ed3c340f239cc9f88802285d11e260a5410e122bf3f72fe7e7a1096b53a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb25a683-4134-4aaa-9806-24d34b346ba4", "node_type": "1", "metadata": {}, "hash": "ec0643a327123f7fbb0e6d1dca86b1a031ddb05cf76ec81774fe2705cc59a196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These hyperparameters govern the behavior and characteristics of our model, \ninfluencing its ability to learn, generalize, and make accurate predictions. However, finding the optimal \nconfiguration for these hyperparameters is no easy feat.\n\nProblem statement\n\nHyperparameter tuning is the art of systematically searching and selecting the best combination \nof hyperparameters for our model. It allows us to venture beyond default settings and discover the \nconfigurations that align harmoniously with our data, extracting the most meaningful insights and \ndelivering superior performance.\nThe goal of hyperparameter tuning is to get optimal values. We explore different hyperparameter \nsettings, traversing through a multidimensional landscape of possibilities. This exploration can take \nvarious forms, such as grid search, random search, or more advanced techniques such as Bayesian \noptimization or genetic algorithms.\nGrid search, a popular method, involves defining a grid of potential values for each hyperparameter.", "mimetype": "text/plain", "start_char_idx": 335583, "end_char_idx": 336606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb25a683-4134-4aaa-9806-24d34b346ba4": {"__data__": {"id_": "cb25a683-4134-4aaa-9806-24d34b346ba4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a775a992-453d-4e07-9f58-27ebd8679bbe", "node_type": "1", "metadata": {}, "hash": "d93b8578a6a704f852fcb3a67b965d14df59ece290d18b61b862911d61221e5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f65a7f6f-d547-450f-8a8a-de48de825c76", "node_type": "1", "metadata": {}, "hash": "a3ae5c40b779faf66922b80c5447da5d1083d4aa7c3cfd37ae5a1c856a0be190", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The goal of hyperparameter tuning is to get optimal values. We explore different hyperparameter \nsettings, traversing through a multidimensional landscape of possibilities. This exploration can take \nvarious forms, such as grid search, random search, or more advanced techniques such as Bayesian \noptimization or genetic algorithms.\nGrid search, a popular method, involves defining a grid of potential values for each hyperparameter. \nThe model is then trained and evaluated for every possible combination within the grid. By exhaustively \nsearching through the grid, we unearth the configuration that yields the highest performance, providing \nus with a solid foundation for further refinement.\nRandom search takes a different approach. It samples hyperparameter values randomly from predefined \ndistributions and evaluates the model\u2019s performance for each sampled configuration. This randomized \nexploration enables us to cover a wider range of possibilities, potentially discovering unconventional \nyet highly effective configurations.\nThese examples demonstrate the process of model training and evaluation using PySpark.", "mimetype": "text/plain", "start_char_idx": 336173, "end_char_idx": 337298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f65a7f6f-d547-450f-8a8a-de48de825c76": {"__data__": {"id_": "f65a7f6f-d547-450f-8a8a-de48de825c76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb25a683-4134-4aaa-9806-24d34b346ba4", "node_type": "1", "metadata": {}, "hash": "ec0643a327123f7fbb0e6d1dca86b1a031ddb05cf76ec81774fe2705cc59a196", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5d44fab-1ec8-45d1-9128-c872b56aa921", "node_type": "1", "metadata": {}, "hash": "78c0d46c28f54794266b1c624c93fe0846a709eab5b6a298a4c198df42eee9c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Random search takes a different approach. It samples hyperparameter values randomly from predefined \ndistributions and evaluates the model\u2019s performance for each sampled configuration. This randomized \nexploration enables us to cover a wider range of possibilities, potentially discovering unconventional \nyet highly effective configurations.\nThese examples demonstrate the process of model training and evaluation using PySpark. However, \nthe specific algorithms, evaluation metrics, and techniques applied may vary, depending on the ML \ntask at hand. It is important to understand the problem domain, select appropriate algorithms, and \nchoose relevant evaluation metrics to train and evaluate models effectively.\nModel deployment\nModel deployment is the process of making trained ML models available for use in production \nenvironments. In this section, we will explore various approaches and techniques for deploying ML \nmodels effectively:\n\u2022\t Serialization and persistence: Once a model has been trained, it needs to be serialized and \npersisted to disk for later use.", "mimetype": "text/plain", "start_char_idx": 336869, "end_char_idx": 337942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5d44fab-1ec8-45d1-9128-c872b56aa921": {"__data__": {"id_": "c5d44fab-1ec8-45d1-9128-c872b56aa921", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f65a7f6f-d547-450f-8a8a-de48de825c76", "node_type": "1", "metadata": {}, "hash": "a3ae5c40b779faf66922b80c5447da5d1083d4aa7c3cfd37ae5a1c856a0be190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4923b01-f539-49f6-8039-d5a2e73374a3", "node_type": "1", "metadata": {}, "hash": "8ca25913a4f3614221ea00ab01fe94ce3904bf26dbc3ac0b7c0192002e4a07d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model deployment\nModel deployment is the process of making trained ML models available for use in production \nenvironments. In this section, we will explore various approaches and techniques for deploying ML \nmodels effectively:\n\u2022\t Serialization and persistence: Once a model has been trained, it needs to be serialized and \npersisted to disk for later use. Serialization is the process of converting the model object \ninto a format that can be stored, while persistence involves saving the serialized model to a \nstorage system.\n\u2022\t Model serving: Model serving involves making the trained model available as an API endpoint \nor service that can receive input data and return predictions. This allows other applications or \nsystems to integrate and use the model for real-time predictions.\n\nMachine Learning with Spark ML\n\nModel monitoring and management\nOnce a model has been deployed, it is important to monitor its performance and behavior in the \nproduction environment and maintain its effectiveness over time.", "mimetype": "text/plain", "start_char_idx": 337585, "end_char_idx": 338600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4923b01-f539-49f6-8039-d5a2e73374a3": {"__data__": {"id_": "e4923b01-f539-49f6-8039-d5a2e73374a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5d44fab-1ec8-45d1-9128-c872b56aa921", "node_type": "1", "metadata": {}, "hash": "78c0d46c28f54794266b1c624c93fe0846a709eab5b6a298a4c198df42eee9c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c27224-881f-440b-8d24-2eedc771cf3b", "node_type": "1", "metadata": {}, "hash": "9e41cd3f9c0989415906f2b04af48b96e5638a1aa2f6c80a70afff40d4fdd880", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Model serving: Model serving involves making the trained model available as an API endpoint \nor service that can receive input data and return predictions. This allows other applications or \nsystems to integrate and use the model for real-time predictions.\n\nMachine Learning with Spark ML\n\nModel monitoring and management\nOnce a model has been deployed, it is important to monitor its performance and behavior in the \nproduction environment and maintain its effectiveness over time. Monitoring can help identify \nissues such as data drift, model degradation, or anomalies. Additionally, model management involves \nversioning, tracking, and maintaining multiple versions of the deployed models. These practices ensure \nthat models remain up to date and perform optimally over time. In this section, we will explore the \nkey aspects of model monitoring and maintenance:\n\u2022\t Scalability and performance: When deploying ML models, scalability and performance are \nessential considerations.", "mimetype": "text/plain", "start_char_idx": 338115, "end_char_idx": 339102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06c27224-881f-440b-8d24-2eedc771cf3b": {"__data__": {"id_": "06c27224-881f-440b-8d24-2eedc771cf3b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4923b01-f539-49f6-8039-d5a2e73374a3", "node_type": "1", "metadata": {}, "hash": "8ca25913a4f3614221ea00ab01fe94ce3904bf26dbc3ac0b7c0192002e4a07d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "416f1092-cdd6-4a53-bf45-f08ef0b4df4d", "node_type": "1", "metadata": {}, "hash": "288826590b75d44265ac85dccd0b3a07582ee07fee763324c62ad5fbbc61e22a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, model management involves \nversioning, tracking, and maintaining multiple versions of the deployed models. These practices ensure \nthat models remain up to date and perform optimally over time. In this section, we will explore the \nkey aspects of model monitoring and maintenance:\n\u2022\t Scalability and performance: When deploying ML models, scalability and performance are \nessential considerations. Models should be designed and deployed in a way that allows for \nefficient processing of large volumes of data and can handle high throughput requirements. \nTechnologies such as Apache Spark provide distributed computing capabilities that enable \nscalable and high-performance model deployment.\n\u2022\t Model updates and retraining: ML models may need to be updated or retrained periodically \nto adapt to changing data patterns or improve performance. Deployed models should have \nmechanisms in place to facilitate updates and retraining without interrupting the serving \nprocess.", "mimetype": "text/plain", "start_char_idx": 338691, "end_char_idx": 339678, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "416f1092-cdd6-4a53-bf45-f08ef0b4df4d": {"__data__": {"id_": "416f1092-cdd6-4a53-bf45-f08ef0b4df4d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06c27224-881f-440b-8d24-2eedc771cf3b", "node_type": "1", "metadata": {}, "hash": "9e41cd3f9c0989415906f2b04af48b96e5638a1aa2f6c80a70afff40d4fdd880", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f8a60a1-7373-4c1b-8966-a95fb158f23f", "node_type": "1", "metadata": {}, "hash": "1e68194cc338d8c5ea50b330bc10384a70ebc48d266ed0dc4bafa1be1e06b008", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Technologies such as Apache Spark provide distributed computing capabilities that enable \nscalable and high-performance model deployment.\n\u2022\t Model updates and retraining: ML models may need to be updated or retrained periodically \nto adapt to changing data patterns or improve performance. Deployed models should have \nmechanisms in place to facilitate updates and retraining without interrupting the serving \nprocess. This can involve automated processes, such as monitoring for data drift or retraining \ntriggers based on specific conditions.\n\u2022\t Performance metrics: To monitor a deployed model, it is important to define and track relevant \nperformance metrics. These metrics can vary, depending on the type of ML problem and the \nspecific requirements of the application. Some commonly used performance metrics include \naccuracy, precision, recall, F1 score, and area under the ROC curve (AUC).", "mimetype": "text/plain", "start_char_idx": 339260, "end_char_idx": 340158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f8a60a1-7373-4c1b-8966-a95fb158f23f": {"__data__": {"id_": "7f8a60a1-7373-4c1b-8966-a95fb158f23f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "416f1092-cdd6-4a53-bf45-f08ef0b4df4d", "node_type": "1", "metadata": {}, "hash": "288826590b75d44265ac85dccd0b3a07582ee07fee763324c62ad5fbbc61e22a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc8d754f-e798-4605-9c37-5e3978383ca9", "node_type": "1", "metadata": {}, "hash": "7e738ae5ff8855a44c0a6408d715375af4d49a65dca61d5b7c887fd91ceb2a2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Performance metrics: To monitor a deployed model, it is important to define and track relevant \nperformance metrics. These metrics can vary, depending on the type of ML problem and the \nspecific requirements of the application. Some commonly used performance metrics include \naccuracy, precision, recall, F1 score, and area under the ROC curve (AUC). By regularly \nevaluating these metrics, deviations from the expected performance can be identified, indicating \nthe need for further investigation or maintenance actions.\n\u2022\t Data drift detection: Data drift refers to the phenomenon where the statistical properties of the \ninput data change over time, leading to a degradation in model performance. Monitoring for \ndata drift is crucial to ensure that the deployed model continues to provide accurate predictions. \nTechniques such as statistical tests, feature distribution comparison, and outlier detection can \nbe employed to detect data drift.", "mimetype": "text/plain", "start_char_idx": 339805, "end_char_idx": 340755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc8d754f-e798-4605-9c37-5e3978383ca9": {"__data__": {"id_": "dc8d754f-e798-4605-9c37-5e3978383ca9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f8a60a1-7373-4c1b-8966-a95fb158f23f", "node_type": "1", "metadata": {}, "hash": "1e68194cc338d8c5ea50b330bc10384a70ebc48d266ed0dc4bafa1be1e06b008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae", "node_type": "1", "metadata": {}, "hash": "f180f483d2b2cf18d5ed693bfa1e57fa5b97605d63adf28896b5a08f344fe46f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Data drift detection: Data drift refers to the phenomenon where the statistical properties of the \ninput data change over time, leading to a degradation in model performance. Monitoring for \ndata drift is crucial to ensure that the deployed model continues to provide accurate predictions. \nTechniques such as statistical tests, feature distribution comparison, and outlier detection can \nbe employed to detect data drift. When data drift is detected, it may be necessary to update \nthe model or retrain it using more recent data.\n\u2022\t Model performance monitoring: Monitoring the performance of a deployed model involves \ntracking its predictions and comparing them with ground truth values. This can be done by \nperiodically sampling a subset of the predictions and evaluating them against the actual outcomes. \nMonitoring can also include analyzing prediction errors, identifying patterns or anomalies, \nand investigating the root causes of any performance degradation. By regularly monitoring \nthe model\u2019s performance, issues can be identified early on and corrective actions can be taken.", "mimetype": "text/plain", "start_char_idx": 340330, "end_char_idx": 341424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae": {"__data__": {"id_": "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc8d754f-e798-4605-9c37-5e3978383ca9", "node_type": "1", "metadata": {}, "hash": "7e738ae5ff8855a44c0a6408d715375af4d49a65dca61d5b7c887fd91ceb2a2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2", "node_type": "1", "metadata": {}, "hash": "742b742f10791a0a802658f85294dd260ea679ee691ce55cc37a354dd8de628e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can be done by \nperiodically sampling a subset of the predictions and evaluating them against the actual outcomes. \nMonitoring can also include analyzing prediction errors, identifying patterns or anomalies, \nand investigating the root causes of any performance degradation. By regularly monitoring \nthe model\u2019s performance, issues can be identified early on and corrective actions can be taken.\n\u2022\t Model retraining and updates: Models that are deployed in production may require periodic \nupdates or retraining to maintain their effectiveness. When new data becomes available or \nsignificant changes occur in the application domain, retraining the model with fresh data can \n\nProblem statement\n\nhelp improve its performance. Additionally, bug fixes, feature enhancements, or algorithmic \nimprovements may necessitate updating the deployed model. It is important to have a well-\ndefined process and infrastructure in place to handle model retraining and updates efficiently.", "mimetype": "text/plain", "start_char_idx": 341024, "end_char_idx": 342003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2": {"__data__": {"id_": "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae", "node_type": "1", "metadata": {}, "hash": "f180f483d2b2cf18d5ed693bfa1e57fa5b97605d63adf28896b5a08f344fe46f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5aa112ad-5b84-494b-85b3-8436f2cfeaa2", "node_type": "1", "metadata": {}, "hash": "cfcf1cd40e4b595b4e66bb6e94b0be5ad16fddc7474b22b8373292082c916649", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When new data becomes available or \nsignificant changes occur in the application domain, retraining the model with fresh data can \n\nProblem statement\n\nhelp improve its performance. Additionally, bug fixes, feature enhancements, or algorithmic \nimprovements may necessitate updating the deployed model. It is important to have a well-\ndefined process and infrastructure in place to handle model retraining and updates efficiently.\n\u2022\t Versioning and model governance: Maintaining proper versioning and governance of deployed \nmodels is crucial for tracking changes, maintaining reproducibility, and ensuring regulatory \ncompliance. Version control systems can be used to manage model versions, track changes, \nand provide a historical record of model updates. Additionally, maintaining documentation \nrelated to model changes, dependencies, and associated processes contributes to effective \nmodel governance.\n\u2022\t Collaboration and feedback: Model monitoring and maintenance often involve collaboration \namong different stakeholders, including data scientists, engineers, domain experts, and business \nusers.", "mimetype": "text/plain", "start_char_idx": 341574, "end_char_idx": 342679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5aa112ad-5b84-494b-85b3-8436f2cfeaa2": {"__data__": {"id_": "5aa112ad-5b84-494b-85b3-8436f2cfeaa2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2", "node_type": "1", "metadata": {}, "hash": "742b742f10791a0a802658f85294dd260ea679ee691ce55cc37a354dd8de628e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "164e1c2b-07bb-496a-aa48-34dbd2253102", "node_type": "1", "metadata": {}, "hash": "5d73e79c62705cd865169f04e043b57fa18585a348700dff4d7b1f59f7754df0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Version control systems can be used to manage model versions, track changes, \nand provide a historical record of model updates. Additionally, maintaining documentation \nrelated to model changes, dependencies, and associated processes contributes to effective \nmodel governance.\n\u2022\t Collaboration and feedback: Model monitoring and maintenance often involve collaboration \namong different stakeholders, including data scientists, engineers, domain experts, and business \nusers. Establishing channels for feedback and communication can facilitate the exchange of \ninsights, identification of issues, and implementation of necessary changes. Regular meetings \nor feedback loops can help align the model\u2019s performance with the evolving requirements of \nthe application.\nOverall, model deployment is a critical step in the ML life cycle. It involves serializing and persisting \ntrained models, serving them as APIs or services, monitoring their performance, ensuring scalability \nand performance, and managing updates and retraining.", "mimetype": "text/plain", "start_char_idx": 342204, "end_char_idx": 343231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "164e1c2b-07bb-496a-aa48-34dbd2253102": {"__data__": {"id_": "164e1c2b-07bb-496a-aa48-34dbd2253102", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5aa112ad-5b84-494b-85b3-8436f2cfeaa2", "node_type": "1", "metadata": {}, "hash": "cfcf1cd40e4b595b4e66bb6e94b0be5ad16fddc7474b22b8373292082c916649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2147ce1-2c9a-472b-928a-984d2329ad01", "node_type": "1", "metadata": {}, "hash": "b1dc3c75cfc0d8ec7d8f72924cbfaf13ea064bc057ac5bbd9a2f365c13f2e0ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Regular meetings \nor feedback loops can help align the model\u2019s performance with the evolving requirements of \nthe application.\nOverall, model deployment is a critical step in the ML life cycle. It involves serializing and persisting \ntrained models, serving them as APIs or services, monitoring their performance, ensuring scalability \nand performance, and managing updates and retraining.\nBy actively monitoring and maintaining deployed models, organizations can ensure that their ML \nsystems continue to provide accurate and reliable predictions. Effective model monitoring techniques, \ncoupled with proactive maintenance strategies, enable timely identification of performance issues and \nsupport the necessary actions to keep the models up to date and aligned with business objectives.\nModel iteration and improvement\nModel iteration and improvement is a crucial phase in the ML life cycle that focuses on enhancing the \nperformance and effectiveness of deployed models. By continuously refining and optimizing models, \norganizations can achieve better predictions and drive greater value from their ML initiatives.", "mimetype": "text/plain", "start_char_idx": 342842, "end_char_idx": 343961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2147ce1-2c9a-472b-928a-984d2329ad01": {"__data__": {"id_": "e2147ce1-2c9a-472b-928a-984d2329ad01", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "164e1c2b-07bb-496a-aa48-34dbd2253102", "node_type": "1", "metadata": {}, "hash": "5d73e79c62705cd865169f04e043b57fa18585a348700dff4d7b1f59f7754df0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5eb0fb65-28df-41fd-b755-23ab423a2dda", "node_type": "1", "metadata": {}, "hash": "94cbcde0f9265f4ce8a7da6f446dd1ef820bd4c0c1ade4e464d1cb4a50c96b07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model iteration and improvement\nModel iteration and improvement is a crucial phase in the ML life cycle that focuses on enhancing the \nperformance and effectiveness of deployed models. By continuously refining and optimizing models, \norganizations can achieve better predictions and drive greater value from their ML initiatives. In this \nsection, we will explore the key aspects of model iteration and improvement:\n\u2022\t Collecting feedback and gathering insights: The first step in model iteration and improvement \nis to gather feedback from various stakeholders, including end users, domain experts, and \nbusiness teams. Feedback can provide valuable insights into the model\u2019s performance, areas \nfor improvement, and potential issues encountered in real-world scenarios. This feedback \ncan be collected through surveys, user interviews, or monitoring the model\u2019s behavior in the \nproduction environment.\n\u2022\t Analyzing model performance: To identify areas for improvement, it is important to thoroughly \nanalyze the model\u2019s performance.", "mimetype": "text/plain", "start_char_idx": 343632, "end_char_idx": 344667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5eb0fb65-28df-41fd-b755-23ab423a2dda": {"__data__": {"id_": "5eb0fb65-28df-41fd-b755-23ab423a2dda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2147ce1-2c9a-472b-928a-984d2329ad01", "node_type": "1", "metadata": {}, "hash": "b1dc3c75cfc0d8ec7d8f72924cbfaf13ea064bc057ac5bbd9a2f365c13f2e0ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7e345af-a79f-4c15-9319-47e83c14034b", "node_type": "1", "metadata": {}, "hash": "1c9ccc5b153ad4c2043bff32cae58353e3f7c4ea2a1facfa1c5b63416833fe52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Feedback can provide valuable insights into the model\u2019s performance, areas \nfor improvement, and potential issues encountered in real-world scenarios. This feedback \ncan be collected through surveys, user interviews, or monitoring the model\u2019s behavior in the \nproduction environment.\n\u2022\t Analyzing model performance: To identify areas for improvement, it is important to thoroughly \nanalyze the model\u2019s performance. This includes examining performance metrics, evaluating \nprediction errors, and conducting in-depth analyses of misclassified or poorly predicted \n\nMachine Learning with Spark ML\n\ninstances. By understanding the strengths and weaknesses of the model, data scientists can \nfocus their efforts on specific areas that require attention.\n\u2022\t Exploring new features and data: One way to improve model performance is by incorporating \nnew features or utilizing additional data sources. Exploratory data analysis can help identify \npotential features that may have a strong impact on predictions.", "mimetype": "text/plain", "start_char_idx": 344253, "end_char_idx": 345256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7e345af-a79f-4c15-9319-47e83c14034b": {"__data__": {"id_": "f7e345af-a79f-4c15-9319-47e83c14034b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eb0fb65-28df-41fd-b755-23ab423a2dda", "node_type": "1", "metadata": {}, "hash": "94cbcde0f9265f4ce8a7da6f446dd1ef820bd4c0c1ade4e464d1cb4a50c96b07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "230fc3b2-8b8b-4ff8-be28-587e563a578d", "node_type": "1", "metadata": {}, "hash": "634a5dd33cab7b8f5abec3c0a2b702b412c114db67d3d512caed0f86329b116b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By understanding the strengths and weaknesses of the model, data scientists can \nfocus their efforts on specific areas that require attention.\n\u2022\t Exploring new features and data: One way to improve model performance is by incorporating \nnew features or utilizing additional data sources. Exploratory data analysis can help identify \npotential features that may have a strong impact on predictions. Feature engineering techniques, \nsuch as creating interaction terms, scaling, or transforming variables, can also be employed to \nenhance the representation of the data. Additionally, incorporating new data from different \nsources can provide fresh insights and improve the model\u2019s generalization capabilities.\n\u2022\t Algorithm selection and hyperparameter tuning: Experimenting with different algorithms and \nhyperparameters can lead to significant improvements in model performance. Data scientists \ncan explore alternative algorithms or variations of the existing algorithm to identify the best \napproach for the given problem.", "mimetype": "text/plain", "start_char_idx": 344859, "end_char_idx": 345883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "230fc3b2-8b8b-4ff8-be28-587e563a578d": {"__data__": {"id_": "230fc3b2-8b8b-4ff8-be28-587e563a578d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7e345af-a79f-4c15-9319-47e83c14034b", "node_type": "1", "metadata": {}, "hash": "1c9ccc5b153ad4c2043bff32cae58353e3f7c4ea2a1facfa1c5b63416833fe52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332", "node_type": "1", "metadata": {}, "hash": "72fb8f8647dddc8eec1de7d07bfc28f5e3126824d61273ebc1c7191947644616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, incorporating new data from different \nsources can provide fresh insights and improve the model\u2019s generalization capabilities.\n\u2022\t Algorithm selection and hyperparameter tuning: Experimenting with different algorithms and \nhyperparameters can lead to significant improvements in model performance. Data scientists \ncan explore alternative algorithms or variations of the existing algorithm to identify the best \napproach for the given problem. Hyperparameter tuning techniques, such as grid search or \nBayesian optimization, can be used to find optimal values for model parameters. This iterative \nprocess helps identify the best algorithm and parameter settings that yield superior results.\n\u2022\t Ensemble methods: Ensemble methods involve combining multiple models to create a more \nrobust and accurate prediction. Techniques such as bagging, boosting, or stacking can be \napplied to build an ensemble model from multiple base models. Ensemble methods can often \nimprove model performance by reducing bias, variance, and overfitting.", "mimetype": "text/plain", "start_char_idx": 345427, "end_char_idx": 346472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332": {"__data__": {"id_": "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "230fc3b2-8b8b-4ff8-be28-587e563a578d", "node_type": "1", "metadata": {}, "hash": "634a5dd33cab7b8f5abec3c0a2b702b412c114db67d3d512caed0f86329b116b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "293b03d2-491d-47ee-8f74-0277e644c3dc", "node_type": "1", "metadata": {}, "hash": "de1f2e37a4c1db265e15bf59146f56cc40ff1757146a67592be5efa09d27184e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This iterative \nprocess helps identify the best algorithm and parameter settings that yield superior results.\n\u2022\t Ensemble methods: Ensemble methods involve combining multiple models to create a more \nrobust and accurate prediction. Techniques such as bagging, boosting, or stacking can be \napplied to build an ensemble model from multiple base models. Ensemble methods can often \nimprove model performance by reducing bias, variance, and overfitting. Experimenting with \ndifferent ensemble strategies and model combinations can lead to further enhancements in \nprediction accuracy.\n\u2022\t A/B testing and controlled experiments: A/B testing or controlled experiments can be \nconducted to evaluate the impact of model improvements in a controlled setting. By randomly \nassigning users or data samples to different versions of the model, organizations can measure \nthe performance of the new model against the existing one. This approach provides statistically \nsignificant results to determine if the proposed changes lead to desired improvements or not.", "mimetype": "text/plain", "start_char_idx": 346022, "end_char_idx": 347071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "293b03d2-491d-47ee-8f74-0277e644c3dc": {"__data__": {"id_": "293b03d2-491d-47ee-8f74-0277e644c3dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332", "node_type": "1", "metadata": {}, "hash": "72fb8f8647dddc8eec1de7d07bfc28f5e3126824d61273ebc1c7191947644616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "956f83eb-73b4-4cf2-aded-318b665f4bb6", "node_type": "1", "metadata": {}, "hash": "9ad9e7452a9eaae151e3ad9e7f48e70ade00b9c292968c58809e41837a737919", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t A/B testing and controlled experiments: A/B testing or controlled experiments can be \nconducted to evaluate the impact of model improvements in a controlled setting. By randomly \nassigning users or data samples to different versions of the model, organizations can measure \nthe performance of the new model against the existing one. This approach provides statistically \nsignificant results to determine if the proposed changes lead to desired improvements or not.\n\u2022\t Continuous monitoring and evaluation: Once the improved model has been deployed, \ncontinuous monitoring and evaluation are essential to ensure its ongoing performance. \nMonitoring for data drift, analyzing performance metrics, and conducting periodic evaluations \nhelp identify potential degradation or the need for further improvements. This feedback loop \nallows for continuous iteration and refinement of the deployed model.\nBy embracing a culture of iteration and improvement, organizations can continuously enhance the \nperformance and accuracy of their ML models.", "mimetype": "text/plain", "start_char_idx": 346604, "end_char_idx": 347644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "956f83eb-73b4-4cf2-aded-318b665f4bb6": {"__data__": {"id_": "956f83eb-73b4-4cf2-aded-318b665f4bb6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "293b03d2-491d-47ee-8f74-0277e644c3dc", "node_type": "1", "metadata": {}, "hash": "de1f2e37a4c1db265e15bf59146f56cc40ff1757146a67592be5efa09d27184e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb904b2a-e3cf-4192-962c-7412286f24a1", "node_type": "1", "metadata": {}, "hash": "e740ec86963473457ed29990905b25d230b325dd04971bf524de637a40003e9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Monitoring for data drift, analyzing performance metrics, and conducting periodic evaluations \nhelp identify potential degradation or the need for further improvements. This feedback loop \nallows for continuous iteration and refinement of the deployed model.\nBy embracing a culture of iteration and improvement, organizations can continuously enhance the \nperformance and accuracy of their ML models. Through collecting feedback, analyzing model performance, \nexploring new features and algorithms, conducting experiments, and continuous monitoring, models \ncan be iteratively refined to achieve better predictions and drive tangible business outcomes.\n\nCase studies and real-world examples\n\nCase studies and real-world examples\nIn this section, we will explore two prominent use cases of ML: customer churn prediction and fraud \ndetection. These examples demonstrate the practical applications of ML techniques in addressing \nreal-world challenges and achieving significant business value.\nCustomer churn prediction\nCustomer churn refers to the phenomenon where customers discontinue their relationship with a \ncompany, typically by canceling a subscription or switching to a competitor.", "mimetype": "text/plain", "start_char_idx": 347244, "end_char_idx": 348432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb904b2a-e3cf-4192-962c-7412286f24a1": {"__data__": {"id_": "cb904b2a-e3cf-4192-962c-7412286f24a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "956f83eb-73b4-4cf2-aded-318b665f4bb6", "node_type": "1", "metadata": {}, "hash": "9ad9e7452a9eaae151e3ad9e7f48e70ade00b9c292968c58809e41837a737919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0315fd1d-f86d-437b-804e-aa0d24e7bdc0", "node_type": "1", "metadata": {}, "hash": "c11dee5638b8fa0f6cd26383bf3c4679991732dd254573a40fa74d77b453cf8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These examples demonstrate the practical applications of ML techniques in addressing \nreal-world challenges and achieving significant business value.\nCustomer churn prediction\nCustomer churn refers to the phenomenon where customers discontinue their relationship with a \ncompany, typically by canceling a subscription or switching to a competitor. Predicting customer churn \nis crucial for businesses as it allows them to proactively identify customers who are at risk of leaving \nand take appropriate actions to retain them. ML models can analyze various customer attributes and \nbehavior patterns to predict churn likelihood. Let\u2019s dive into a customer churn prediction case study.\nCase study \u2013 telecommunications company\nA telecommunications company wants to reduce customer churn by predicting which customers are \nmost likely to cancel their subscriptions. The company collects extensive customer data, including \ndemographics, call records, service usage, and customer complaints.", "mimetype": "text/plain", "start_char_idx": 348085, "end_char_idx": 349071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0315fd1d-f86d-437b-804e-aa0d24e7bdc0": {"__data__": {"id_": "0315fd1d-f86d-437b-804e-aa0d24e7bdc0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb904b2a-e3cf-4192-962c-7412286f24a1", "node_type": "1", "metadata": {}, "hash": "e740ec86963473457ed29990905b25d230b325dd04971bf524de637a40003e9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8", "node_type": "1", "metadata": {}, "hash": "57bd4d65637cd2071145e9fc76b816c2a3b055ac599b3c83a19866c75ca7752f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ML models can analyze various customer attributes and \nbehavior patterns to predict churn likelihood. Let\u2019s dive into a customer churn prediction case study.\nCase study \u2013 telecommunications company\nA telecommunications company wants to reduce customer churn by predicting which customers are \nmost likely to cancel their subscriptions. The company collects extensive customer data, including \ndemographics, call records, service usage, and customer complaints. By leveraging ML, they aim to \nidentify key indicators of churn and build a predictive model to forecast future churners:\n\u2022\t Data preparation: The company gathers and preprocesses the customer data, ensuring it is \ncleaned, formatted, and ready for analysis. They combine customer profiles with historical \nchurn information to create a labeled dataset.\n\u2022\t Feature engineering: To capture meaningful patterns, the company engineers relevant features \nfrom the available data. These features may include variables such as average call duration, \nnumber of complaints, monthly service usage, and tenure.", "mimetype": "text/plain", "start_char_idx": 348611, "end_char_idx": 349673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8": {"__data__": {"id_": "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0315fd1d-f86d-437b-804e-aa0d24e7bdc0", "node_type": "1", "metadata": {}, "hash": "c11dee5638b8fa0f6cd26383bf3c4679991732dd254573a40fa74d77b453cf8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b", "node_type": "1", "metadata": {}, "hash": "9192bf05ef5c9c2eae4a7b638e6a67bd4e224d946f049e83848cbc26fd4e81e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They combine customer profiles with historical \nchurn information to create a labeled dataset.\n\u2022\t Feature engineering: To capture meaningful patterns, the company engineers relevant features \nfrom the available data. These features may include variables such as average call duration, \nnumber of complaints, monthly service usage, and tenure.\n\u2022\t Model selection and training: The company selects an appropriate ML algorithm, such as \nlogistic regression, decision trees, or random forests, to build the churn prediction model. \nThey split the dataset into training and testing sets, train the model on the training data, and \nevaluate its performance on the testing data.\n\u2022\t Model evaluation: The model\u2019s performance is assessed using evaluation metrics such as \naccuracy, precision, recall, and F1 score. The company analyzes the model\u2019s ability to correctly \nidentify churners and non-churners, striking a balance between false positives and false negatives.", "mimetype": "text/plain", "start_char_idx": 349331, "end_char_idx": 350291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b": {"__data__": {"id_": "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8", "node_type": "1", "metadata": {}, "hash": "57bd4d65637cd2071145e9fc76b816c2a3b055ac599b3c83a19866c75ca7752f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d51831c-08cd-492d-a872-0c11266808cf", "node_type": "1", "metadata": {}, "hash": "e7ffce70f174871ee2ff54239a5f04a3dfe5fed4e3f56bd824c3a66810ede861", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They split the dataset into training and testing sets, train the model on the training data, and \nevaluate its performance on the testing data.\n\u2022\t Model evaluation: The model\u2019s performance is assessed using evaluation metrics such as \naccuracy, precision, recall, and F1 score. The company analyzes the model\u2019s ability to correctly \nidentify churners and non-churners, striking a balance between false positives and false negatives.\n\u2022\t Model deployment and monitoring: Once the model meets the desired performance criteria, \nit is deployed into the production environment. The model continuously monitors incoming \ncustomer data, generates churn predictions, and triggers appropriate retention strategies for \nat-risk customers.\n\nMachine Learning with Spark ML\n\nFraud detection\nFraud detection is another critical application of ML that aims to identify fraudulent activities and \nprevent financial losses. ML models can learn patterns of fraudulent behavior from historical data \nand flag suspicious transactions or activities in real time. Let\u2019s explore a fraud detection case study.", "mimetype": "text/plain", "start_char_idx": 349859, "end_char_idx": 350944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d51831c-08cd-492d-a872-0c11266808cf": {"__data__": {"id_": "8d51831c-08cd-492d-a872-0c11266808cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b", "node_type": "1", "metadata": {}, "hash": "9192bf05ef5c9c2eae4a7b638e6a67bd4e224d946f049e83848cbc26fd4e81e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fda3f3f-65ea-4c26-babf-48d8761086fb", "node_type": "1", "metadata": {}, "hash": "f5e59404c3e83402e3b025b28cca821d340f7b051b37b24fb5986b35307ed22c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model continuously monitors incoming \ncustomer data, generates churn predictions, and triggers appropriate retention strategies for \nat-risk customers.\n\nMachine Learning with Spark ML\n\nFraud detection\nFraud detection is another critical application of ML that aims to identify fraudulent activities and \nprevent financial losses. ML models can learn patterns of fraudulent behavior from historical data \nand flag suspicious transactions or activities in real time. Let\u2019s explore a fraud detection case study.\nCase study \u2013 financial institution\nA financial institution wants to detect fraudulent transactions in real time to protect its customers \nand prevent monetary losses. The institution collects transaction data, including transaction amounts, \ntimestamps, merchant information, and customer details. By leveraging ML algorithms, they aim to \nbuild a robust fraud detection system:\n\u2022\t Data preprocessing: The financial institution processes and cleans the transaction data, ensuring \ndata integrity and consistency.", "mimetype": "text/plain", "start_char_idx": 350432, "end_char_idx": 351457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fda3f3f-65ea-4c26-babf-48d8761086fb": {"__data__": {"id_": "4fda3f3f-65ea-4c26-babf-48d8761086fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d51831c-08cd-492d-a872-0c11266808cf", "node_type": "1", "metadata": {}, "hash": "e7ffce70f174871ee2ff54239a5f04a3dfe5fed4e3f56bd824c3a66810ede861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c6f3e73-f130-4fc0-acc6-693c353f4725", "node_type": "1", "metadata": {}, "hash": "2f69e4104a4c6f2e1eb5dc2ba71da67b2d3fab6157bd7f3eb5bb1999a9d0b700", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Case study \u2013 financial institution\nA financial institution wants to detect fraudulent transactions in real time to protect its customers \nand prevent monetary losses. The institution collects transaction data, including transaction amounts, \ntimestamps, merchant information, and customer details. By leveraging ML algorithms, they aim to \nbuild a robust fraud detection system:\n\u2022\t Data preprocessing: The financial institution processes and cleans the transaction data, ensuring \ndata integrity and consistency. They may also enrich the data by incorporating additional \ninformation, such as IP addresses or device identifiers, to enhance fraud detection capabilities.\n\u2022\t Feature engineering: Relevant features are extracted from the transaction data to capture \npotential indicators of fraudulent activity. These features may include transaction amounts, \nfrequency, geographical location, deviation from typical spending patterns, and customer \ntransaction history.", "mimetype": "text/plain", "start_char_idx": 350945, "end_char_idx": 351913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c6f3e73-f130-4fc0-acc6-693c353f4725": {"__data__": {"id_": "6c6f3e73-f130-4fc0-acc6-693c353f4725", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fda3f3f-65ea-4c26-babf-48d8761086fb", "node_type": "1", "metadata": {}, "hash": "f5e59404c3e83402e3b025b28cca821d340f7b051b37b24fb5986b35307ed22c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fa2aac6-e1be-48e6-9438-dbc221d8b634", "node_type": "1", "metadata": {}, "hash": "2994cb371be17f65dfc2052d99fea7c3f8b574c8edec9de4a9bcd02b195691f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They may also enrich the data by incorporating additional \ninformation, such as IP addresses or device identifiers, to enhance fraud detection capabilities.\n\u2022\t Feature engineering: Relevant features are extracted from the transaction data to capture \npotential indicators of fraudulent activity. These features may include transaction amounts, \nfrequency, geographical location, deviation from typical spending patterns, and customer \ntransaction history.\n\u2022\t Model training: The financial institution selects suitable ML algorithms, such as anomaly \ndetection techniques or supervised learning methods (for example, logistic regression and \ngradient boosting), to train the fraud detection model. The model is trained on historical data \nlabeled as fraudulent or non-fraudulent.\n\u2022\t Real-time monitoring: Once the model has been trained, it is deployed to analyze incoming \ntransactions in real time. The model assigns a fraud probability score to each transaction, and \ntransactions exceeding a certain threshold are flagged for further investigation or intervention.", "mimetype": "text/plain", "start_char_idx": 351458, "end_char_idx": 352525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fa2aac6-e1be-48e6-9438-dbc221d8b634": {"__data__": {"id_": "2fa2aac6-e1be-48e6-9438-dbc221d8b634", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c6f3e73-f130-4fc0-acc6-693c353f4725", "node_type": "1", "metadata": {}, "hash": "2f69e4104a4c6f2e1eb5dc2ba71da67b2d3fab6157bd7f3eb5bb1999a9d0b700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4", "node_type": "1", "metadata": {}, "hash": "8da989b7ccb075429ba3b8ca3b5a4d1fd95ce443aa9171a7e8fa1147725ab98b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model is trained on historical data \nlabeled as fraudulent or non-fraudulent.\n\u2022\t Real-time monitoring: Once the model has been trained, it is deployed to analyze incoming \ntransactions in real time. The model assigns a fraud probability score to each transaction, and \ntransactions exceeding a certain threshold are flagged for further investigation or intervention.\n\u2022\t Continuous improvement: The financial institution continuously refines the fraud detection \nmodel by monitoring its performance and incorporating new data. They periodically evaluate \nthe model\u2019s effectiveness, adjust thresholds, and update the model to adapt to evolving fraud \npatterns and techniques.\nBy applying ML techniques to customer churn prediction and fraud detection, organizations can \nenhance their decision-making processes, improve customer retention, and mitigate financial risks. \nThese case studies highlight the practical application of ML in real-world scenarios, demonstrating \nits value in various industries.", "mimetype": "text/plain", "start_char_idx": 352155, "end_char_idx": 353161, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4": {"__data__": {"id_": "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fa2aac6-e1be-48e6-9438-dbc221d8b634", "node_type": "1", "metadata": {}, "hash": "2994cb371be17f65dfc2052d99fea7c3f8b574c8edec9de4a9bcd02b195691f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92aa068c-c24c-4b8d-a496-104c1fd81b1e", "node_type": "1", "metadata": {}, "hash": "73dc283b958e9c272d6a12195771f5102d4000b11074fd6dfe80ede0a26bbce5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They periodically evaluate \nthe model\u2019s effectiveness, adjust thresholds, and update the model to adapt to evolving fraud \npatterns and techniques.\nBy applying ML techniques to customer churn prediction and fraud detection, organizations can \nenhance their decision-making processes, improve customer retention, and mitigate financial risks. \nThese case studies highlight the practical application of ML in real-world scenarios, demonstrating \nits value in various industries.\nFuture trends in Spark ML and distributed ML\nAs the field of ML continues to evolve, there are several future trends and advancements that we can \nexpect in Spark ML and distributed ML. Here are a few key areas to watch:\n\nSummary\n\n\u2022\t Deep learning integration: Spark ML is likely to see deeper integration with deep learning \nframeworks such as TensorFlow and PyTorch. This will enable users to seamlessly incorporate \ndeep learning models into their Spark ML pipelines, unlocking the power of neural networks \nfor complex tasks such as image recognition and natural language processing.", "mimetype": "text/plain", "start_char_idx": 352685, "end_char_idx": 353749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92aa068c-c24c-4b8d-a496-104c1fd81b1e": {"__data__": {"id_": "92aa068c-c24c-4b8d-a496-104c1fd81b1e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4", "node_type": "1", "metadata": {}, "hash": "8da989b7ccb075429ba3b8ca3b5a4d1fd95ce443aa9171a7e8fa1147725ab98b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c04758-5c7f-4ca5-9dff-36766898c7dc", "node_type": "1", "metadata": {}, "hash": "b2eab9bdd9e66a4aff219deaed9bd08507900c9a6b1377c0ab4102e329dac5be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here are a few key areas to watch:\n\nSummary\n\n\u2022\t Deep learning integration: Spark ML is likely to see deeper integration with deep learning \nframeworks such as TensorFlow and PyTorch. This will enable users to seamlessly incorporate \ndeep learning models into their Spark ML pipelines, unlocking the power of neural networks \nfor complex tasks such as image recognition and natural language processing.\n\u2022\t Automated ML: Automation will play a significant role in simplifying and accelerating the \nmachine learning process. We can anticipate advancements in automated feature engineering, \nhyperparameter tuning, and model selection techniques within Spark ML. These advancements \nwill make it easier for users to build high-performing models with minimal manual effort.\n\u2022\t Explainable AI: As the demand for transparency and interpretability in machine learning \nmodels grows, Spark ML is likely to incorporate techniques for model interpretability.", "mimetype": "text/plain", "start_char_idx": 353348, "end_char_idx": 354295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49c04758-5c7f-4ca5-9dff-36766898c7dc": {"__data__": {"id_": "49c04758-5c7f-4ca5-9dff-36766898c7dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92aa068c-c24c-4b8d-a496-104c1fd81b1e", "node_type": "1", "metadata": {}, "hash": "73dc283b958e9c272d6a12195771f5102d4000b11074fd6dfe80ede0a26bbce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03013851-f68b-4beb-adee-2ebfe0dab352", "node_type": "1", "metadata": {}, "hash": "1a4d23892b895b2512c59b17ef1212f8e0e5c48575f02b79f7281881108d44f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can anticipate advancements in automated feature engineering, \nhyperparameter tuning, and model selection techniques within Spark ML. These advancements \nwill make it easier for users to build high-performing models with minimal manual effort.\n\u2022\t Explainable AI: As the demand for transparency and interpretability in machine learning \nmodels grows, Spark ML is likely to incorporate techniques for model interpretability. This will \nenable users to understand and explain the predictions made by their models, making them \nmore trustworthy and compliant with regulatory requirements.\n\u2022\t Generative AI (GenAI): GenAI is the latest rage. As use cases for GenAI become more in \ndemand, the current platforms may incorporate some of the LLMs that are used in GenAI.\n\u2022\t Edge computing and IoT: With the rise of edge computing and the Internet of Things (IoT), \nSpark ML is expected to extend its capabilities to support ML inference and training on edge \ndevices.", "mimetype": "text/plain", "start_char_idx": 353870, "end_char_idx": 354832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03013851-f68b-4beb-adee-2ebfe0dab352": {"__data__": {"id_": "03013851-f68b-4beb-adee-2ebfe0dab352", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49c04758-5c7f-4ca5-9dff-36766898c7dc", "node_type": "1", "metadata": {}, "hash": "b2eab9bdd9e66a4aff219deaed9bd08507900c9a6b1377c0ab4102e329dac5be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be", "node_type": "1", "metadata": {}, "hash": "b830de06a56fae46610aaa050a5afc073bd7396047e1240701852535649d1f3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022\t Generative AI (GenAI): GenAI is the latest rage. As use cases for GenAI become more in \ndemand, the current platforms may incorporate some of the LLMs that are used in GenAI.\n\u2022\t Edge computing and IoT: With the rise of edge computing and the Internet of Things (IoT), \nSpark ML is expected to extend its capabilities to support ML inference and training on edge \ndevices. This will enable real-time, low-latency predictions and distributed learning across edge \ndevices, opening up new possibilities for applications in areas like smart cities, autonomous \nvehicles, and industrial IoT.\nAnd with that we\u2019ve concluded the learning portion of the book. Let\u2019s briefly recap what we\u2019ve covered.\nSummary\nIn conclusion, Spark ML provides a powerful and scalable framework for distributed ML tasks. Its \nintegration with Apache Spark offers significant advantages in terms of processing large-scale datasets, \nparallel computing, and fault tolerance.", "mimetype": "text/plain", "start_char_idx": 354458, "end_char_idx": 355404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be": {"__data__": {"id_": "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03013851-f68b-4beb-adee-2ebfe0dab352", "node_type": "1", "metadata": {}, "hash": "1a4d23892b895b2512c59b17ef1212f8e0e5c48575f02b79f7281881108d44f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e44afe30-1077-48d5-be6f-b761db49ccc1", "node_type": "1", "metadata": {}, "hash": "f3a8d1d764c68d3b0a5b5eea30fe668f0e0a712204b525020d5b65cd62b7e4fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And with that we\u2019ve concluded the learning portion of the book. Let\u2019s briefly recap what we\u2019ve covered.\nSummary\nIn conclusion, Spark ML provides a powerful and scalable framework for distributed ML tasks. Its \nintegration with Apache Spark offers significant advantages in terms of processing large-scale datasets, \nparallel computing, and fault tolerance. Throughout this chapter, we explored the key concepts, \ntechniques, and real-world examples of Spark ML.\nWe discussed the ML life cycle, emphasizing the importance of data preparation, model training, \nevaluation, deployment, monitoring, and continuous improvement. We also compared Spark MLlib \nand Spark ML, highlighting their respective features and use cases.\nThroughout this chapter, we discussed various key concepts and techniques related to Spark ML. \nWe explored different types of ML, such as classification, regression, time series analysis, supervised \nlearning, and unsupervised learning.", "mimetype": "text/plain", "start_char_idx": 355048, "end_char_idx": 356006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e44afe30-1077-48d5-be6f-b761db49ccc1": {"__data__": {"id_": "e44afe30-1077-48d5-be6f-b761db49ccc1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be", "node_type": "1", "metadata": {}, "hash": "b830de06a56fae46610aaa050a5afc073bd7396047e1240701852535649d1f3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fef73a08-75f7-4516-95af-041751750caf", "node_type": "1", "metadata": {}, "hash": "b01dc98e25234fdbd11e6ea2b71f19824e3a22f580ac045cbdb97cd4a8fd758e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also compared Spark MLlib \nand Spark ML, highlighting their respective features and use cases.\nThroughout this chapter, we discussed various key concepts and techniques related to Spark ML. \nWe explored different types of ML, such as classification, regression, time series analysis, supervised \nlearning, and unsupervised learning. We highlighted the importance of data preparation and feature \nengineering in building effective ML pipelines. We also touched upon fault-tolerance and reliability \naspects in Spark ML, ensuring robustness and data integrity.\nFurthermore, we examined real-world use cases, including customer churn prediction and fraud \ndetection, to demonstrate the practical applications of Spark ML in solving complex business challenges. \n\nMachine Learning with Spark ML\n\nThese case studies showcased how organizations can leverage Spark ML to enhance decision-making, \nimprove customer retention, and mitigate financial risks.\nAs you continue your journey in ML with Spark ML, it is important to keep the iterative and dynamic \nnature of the field in mind.", "mimetype": "text/plain", "start_char_idx": 355671, "end_char_idx": 356751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fef73a08-75f7-4516-95af-041751750caf": {"__data__": {"id_": "fef73a08-75f7-4516-95af-041751750caf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13890c15-bd07-4cec-910d-21908a050065", "node_type": "4", "metadata": {}, "hash": "62a1c77e7a5657063f4cdffe7d6de3250768b450c3b252df5c413199aa36eb96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e44afe30-1077-48d5-be6f-b761db49ccc1", "node_type": "1", "metadata": {}, "hash": "f3a8d1d764c68d3b0a5b5eea30fe668f0e0a712204b525020d5b65cd62b7e4fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine Learning with Spark ML\n\nThese case studies showcased how organizations can leverage Spark ML to enhance decision-making, \nimprove customer retention, and mitigate financial risks.\nAs you continue your journey in ML with Spark ML, it is important to keep the iterative and dynamic \nnature of the field in mind. Stay updated with the latest advancements, explore new techniques, and \nembrace a mindset of continuous learning and improvement.\nBy harnessing the power of Spark ML, you can unlock valuable insights from your data, build \nsophisticated ML models, and make informed decisions that drive business success. So, leverage the \ncapabilities of Spark ML, embrace the future trends, and embark on your journey toward mastering \ndistributed ML.\nThat concludes this chapter. Hopefully, it will help you on your exciting journey in the world of ML \nmodels. The next two chapters are mock tests to prepare you for the certification exam.", "mimetype": "text/plain", "start_char_idx": 356434, "end_char_idx": 357378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"2afa0d72-7f52-4451-a38d-3322fb2a08cf": {"doc_hash": "c6a422d8e54dd401d26baf9fcfab3d088d33681947d2a30cba851925ec0a1e18", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a6917fa1-934d-4975-8793-ef67d7c9acf9": {"doc_hash": "073e59c91492867a2a5a73597cd4160dbf0bf3d0121cb25f8b2e203855ffc2a6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9faf2d21-870f-4215-bdd2-9b39f53126cb": {"doc_hash": "389c42887948d229b1a302b7b66f8c8998881eba9272698cf8d9e1e98a042aa3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f": {"doc_hash": "b96f843f53185f1aaaa5a6530703eec76e2101e782701dfbd22985aba8a9b40f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fccfcde2-2c44-4979-9a61-b81bcfea9602": {"doc_hash": "5f665659380bf01e48a71b0b719f25b2c4325867beca1f86b5f8465902668bac", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "42fe3c04-6a3a-4434-9e7d-3ad7cda93957": {"doc_hash": "9cac4befd4b659917540578c67fb3deb0d7bd6ee887b5417b334bb9cc70c18b4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0345a04c-8d7e-4d6c-b5b5-9935453a1423": {"doc_hash": "42b24c40da872e350cfb71883b4eed7b366f54de3e86d1fa794c54fb9cc665b8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8f2511fc-3a2b-4d8b-884a-e14b56568452": {"doc_hash": "b9516663300986de68fab27b62a5b675d146b843d5162f52dfdb0da641d2ffe8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f8421a5b-6167-46d5-ad75-d1cba902608c": {"doc_hash": "f889ca2f15029b43933ddb14e72ed87557efe39850693d42eeb7a2eab458f339", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ea93af9a-efce-40a1-8bf2-0f0168c5505a": {"doc_hash": "3faef5411e3f50a94aeeb7c0faf10dc26c66983033b982c63c5fdfd43801e1a1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0": {"doc_hash": "f49d8616b4452a1b8a658cf0cf3cedd0debe9f2b20d64ed9d1754511d089e91b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0f186c54-f6c2-44a5-b000-8454a7cd2583": {"doc_hash": "78092afcbbdd060ddb9ef2754e0a218dc12c193bfb89fb18126b67a4bebf67c3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "46aa50c7-6504-4f14-a013-f630ee4f702e": {"doc_hash": "832225c52925e437e8179b41257464eb7dfa8db0803ae70e489f513caf64ea6d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "85d2fc81-458b-4d94-9e69-621021c8db1e": {"doc_hash": "d60a8ba83800c7886db3e908317e6bdea535d5cc3bf40db6d23713ad4addbb6c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "625aafe0-01af-4e9d-876f-a9b193273e54": {"doc_hash": "cc141c13b7ce4dd94eacc130ab5c5dfbc7468aa029461c55598dbf943afa54b4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "581d5a9f-fe03-4dca-adbb-836bd8a8daad": {"doc_hash": "b33d2f6c59890f1f83bd697b5e37b23078e7fc14838975254d799ad20f9d9003", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7d474804-d117-42ce-9cc9-e77f5f16dadb": {"doc_hash": "e5e5fa5410dc343a1a9333a97eca351409657c4ad465e68bad25ba0ace3c00c3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "afb5be51-ce36-454a-9eaa-6c70f3a2103e": {"doc_hash": "730ea9fb949ba2e3210980858ef5b7bf4a1a403594cc26f0465419e3f59e5d60", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e03dfe0d-b27b-4dec-b027-02ef8723ee8d": {"doc_hash": "6b780f7b2333de969c0d2f67600dd19429efa5c374facf76cfa831930c81b032", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413": {"doc_hash": "a086b59e1d05f71e8212848b8ef98c9aa08c3ecdf0eef73dfc4ea0450fcb28c5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "71194748-3662-41af-abf5-46b3bcf89469": {"doc_hash": "6d70e3d80fe146f9ef3b7f26f5b369fa6e1279c26b3011842502b68e983985cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "77657998-374f-43cd-8fa7-3ab3b8bb6481": {"doc_hash": "96c7cea3b07a556b82bf22e654ee608d0e49e83ba80c6e313d89ffe36398207f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c9025230-0639-4a62-89e7-ab148d9c4d46": {"doc_hash": "521cf29a6670aa221493be3655f9c7e7adbfd73c9b78dd8f86fcdfa048321aa3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6fb75d86-4dd7-44bc-87cb-50028a021caa": {"doc_hash": "f2b3476cd080ac146bb2138db85bdfa0bf325fcbd22a3e12c89035686b1f6ce9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6": {"doc_hash": "c0fde22bdb010655252e5949bf7dd3f1783d557a0f92e606045e286075ed6946", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7ca3e729-9fdd-45ba-bf62-de9a769a736b": {"doc_hash": "6a00ea447919046b31621047b81625ecbfd9446ecb1ba2b5dc2411825c6e8a64", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "89951293-a6a9-4b08-a15f-b1dc99ff4743": {"doc_hash": "0d52279e22ad457fbf6d5d565f0f67aa4aa551b98427447235e69236820c9bb9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c14088ed-de4e-4045-8cb4-15738b17d83a": {"doc_hash": "572c9bbea44109f9cdda8b618a87d696fd18d6c67cf74a66f91635e4743ee2e5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2e615998-dd28-429a-93b9-73dae6d44046": {"doc_hash": "f944ba56d5c452bf495c873c62a081439738ed60a862d610a4421ec4bf4adb12", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0ef8788d-aeb8-44d7-ae94-741ac6e7b271": {"doc_hash": "5fec30fef7bf38b58551a7bd1dc97bcebb77b33f7e3929954a9cfd9f11dcc07f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "895ea528-317b-4787-82c4-a4222d01dd4f": {"doc_hash": "01bed3781c9228ffc8d0528fe514f49c7caebeb50d8247c8f92e7f344eadd1e4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "431d04f7-ec0f-438d-bc97-d3ab5d917981": {"doc_hash": "7335965d451feef158ddfda467ca27a70e17237cc75e26f0ff7f983e5c623b03", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d13972a6-39df-4843-96a1-9e4a22242409": {"doc_hash": "e8328061bdf0f7597c1aaed9ffcaae5d149e59ff7fd3592feaa307cc177aa011", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6600a00e-bfe9-4cb3-8eff-747df138760e": {"doc_hash": "efda4e74a036b746c9844286e32686c1555b6104d88dbbe7993397df12ae57f8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bd2b5ffb-9dcd-4894-a023-6c32e08d4956": {"doc_hash": "8849c395c453ae98d8997e132916c69aea019a2f921fb271c68c1ac66d5b0b3e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d7efac5f-e3a5-4130-b855-7072616332fc": {"doc_hash": "a540b931be0012419b9ffd061c47d3f07841a609cfe0c692d168520c6582058f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6f3bd974-df10-46dc-92e7-f9341b6adb91": {"doc_hash": "73addad95eef650e800b165c572affcf59e6cab07909f7fb2d13f4c5812cec45", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1d5074aa-b778-4a5f-9a11-d2440f4256c5": {"doc_hash": "d8f6260cd3c72453a598329cf74d5edea53c5cf19c014d9cf62dcdd084a00e60", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5f864c98-1fcc-437d-8d6e-33ea59fb02d5": {"doc_hash": "fcd3d07eb7aa52bb558dfcfe57b916d059025a1602dc83ce167915e64d575b1f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d": {"doc_hash": "26e3ee07a1b5ce6101bf203a366e4b75c64daa09398fd6611dc98ce01f8cf46f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084": {"doc_hash": "1188f884b92ec817724c191bf79d3993e54db15a2f3d17728bfb62a8af6f880d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d7773923-c9de-490f-bbc2-5f8f24e17f0e": {"doc_hash": "6e7159833674b93e74950549f85d13755744911708ed331af3d50e53a9168bc8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "86f4b4ef-75c3-44d3-9db7-fc0059332b91": {"doc_hash": "c5fe10371013bfed0c8c3e556ec063b9f77d2286ade9debd601ce2b24ad1981c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "61eb9bc5-1853-493a-b728-b123ac9cd444": {"doc_hash": "ade5096252ddedc161c52ec1813211ba83ef017f5c5858222acd39dc3f5571cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c6128816-6a90-46fb-a9d6-cbdef2f47640": {"doc_hash": "892b2af62417db25b90d3ad8845e3cd01f717585da841114f792ae5e5ff7eaa1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e2d4acba-d7c4-4788-9b3c-e928b0c37782": {"doc_hash": "e94b17fb4505f0510a0bf684370c5f13581ec1a21c499e5295b893c2622869ce", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2": {"doc_hash": "643667771d43320cf2ee21436dc0aa59311e056d7b739fe030603aa0233b14d4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "48a046e4-1c7a-473e-998d-245b521c1691": {"doc_hash": "993445754f88548a087c910de02a97935bf58cd5123370f6362967d172431686", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699": {"doc_hash": "f426fe4be86449f06bf06ff7ee31b11b8ed2824d80cb2a4857941807111c005a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "85c9e32b-8614-4f2a-8762-3c97bf6d081f": {"doc_hash": "eec9c2c3c508dad48172df0b482d7029e4408a9a89ce6ba95c8830d1c33a041f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5496aa0c-7d5b-4b69-8313-5c153bd66a8d": {"doc_hash": "5915e13e832c849dddd94d92af86bb2fdc3b7739514723178c28736c3dcd3bca", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275": {"doc_hash": "a75519c37d712ac912d359b822eb87e63727d019a36d324b69a05e9fe60aaadd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "08fbe868-004f-4339-a7c9-c37576f9a7fb": {"doc_hash": "39ab9c891105eaae7c349414fb31fd361253abe1530c5b09da9baad4de48bb3a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "091a7f7b-ca71-4b81-927a-845f08daf909": {"doc_hash": "778776fb4a5fb75e51a80481f06dff3208569c6cfb8b46541f8acba6e7aba0a8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd": {"doc_hash": "19d4092e6c366d4f979083cfad82d0db85cfc6c9ae4a1cee1d895069717e43df", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f": {"doc_hash": "67a52246bf036c988a1fc85236eb4ab27a9b83bf1022894b30bc7fb106ec5309", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dc45d385-72c3-4b81-98bf-d87784465900": {"doc_hash": "0d5c7f5e317885010f33f4be6f7d5db383de01b178f09fc5ceb2a5f8491a3e51", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a2f22521-0da6-4d25-904b-1019b606b2a7": {"doc_hash": "7f6840d12aac308206543c26318c913237068e9cd15896431a28f0cb9c14e376", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fcf50002-0242-47e1-8c2d-7bf4523eeeb5": {"doc_hash": "e5b08749b92abbec4737b4126505838bb149280f94ad24c3a1dae53930a29d6d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8": {"doc_hash": "0ab82ba63f08155b78f2c2c9a112d5d42c6d8234cf36eeda323a5a22c1ef5f0b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5c7f513d-77ff-4298-bd80-35e20c97617d": {"doc_hash": "bfda49ae1ce4b3f2d86ccd625e7338354311615fbef5f147f617062b76273772", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e23e7096-4355-4676-8497-4f0585b9255c": {"doc_hash": "5d481cbd7bdfce9e0242bf68a9f0dc399a4552f807d94dc43480ecb35ecd3f8d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ad757761-937b-46db-a109-75fc66bce2e5": {"doc_hash": "a8632ea92634dfc374a275aac84e74ef5c880fcb3501fb932f208d15de521066", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8fa74f19-76a5-40f7-8410-3a0c5c8cad31": {"doc_hash": "da47be949adeb873231fc1a29c031b07de745ef0be6748a9ff28293be67de75b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173": {"doc_hash": "e843d78f826b3fd984fc30abbc677d1c1bf299dbfb36575182cdea2cb034c930", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "df43e7e7-f667-4f2d-8c64-2b4a22816bf7": {"doc_hash": "ea9c65241f77ac1b7b4973dc1be5d91fab49dbf0036af06d0c2bb6a2013e243c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e7695727-f089-4ed4-9b73-724a5998cd38": {"doc_hash": "278ae7174b4d11c1d3c16b19bb2c53c5d71e8ca2343b424a44c030eddcc9466f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "877fe78c-f363-4c45-8dd7-7b38af52adbc": {"doc_hash": "2b18900696a1621ff75794a7606b1d2a0f73fb8b9e9015c29e2f7e97022416a1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bd9b58ff-eeda-46df-b657-2564ffa240c7": {"doc_hash": "c00d38f419e236130ea9a3c4378e41735a2a828e0d45e136c1d7c3ed85f8e1fb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1b97d233-1317-4e1e-a8c2-1850424a8396": {"doc_hash": "e59c1696bd822698e2f513ddf18fb5662cdaa6abfd493bafaf9258b35927cbe7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "da1c3963-f8b5-4712-8366-c7c4a6f5efeb": {"doc_hash": "cebc10a020ad1bf5d58aaec211e7a09a56af7fa9bb9e89474c68444fb32ac34a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c61e2a43-19d3-4ce5-b28e-bf5911134da0": {"doc_hash": "995d9aeb94aac629e59c9bc3e5b1362b5377b0bbe1af80ab4390d4296a28523f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7": {"doc_hash": "aad58222f7807840fe177daadb57eacd730ceead6ff764cbde4c0816278c0cb9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87": {"doc_hash": "06f01cdcdd12646c841719fa9ad128d48af3f81ff9ecfabaea7eb164b5f34b3f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e489e613-2c89-4859-a726-340a417e5eda": {"doc_hash": "2a2e4a14b4cca5caf9d07ba325fea453b92eedc7101da4a1a25a5cbc337a000a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4df34045-d2fe-4848-9b30-b1676fb1309b": {"doc_hash": "0ddb196bd7670172361dfd7336488f160d9204fcb0beb0bcf37ba73066d1642c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1060dad7-138a-45e5-bba5-fe384bc7569b": {"doc_hash": "6e89b62835d2b9eefa085f889183bb8f0eb10a368a495f900f24451b9b17dcd8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "02330974-dbba-4011-aba2-0e6b4f1c0a67": {"doc_hash": "c9ef5157db66f3f4c7517cf6ae2926cc949cc577e2fdd455e0bc52c6ad15042b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "994ea49c-462d-46f7-abb0-6b29b08749dc": {"doc_hash": "1760ce83010024e40df914db3006ce2f1eeb74e0c02952cf2844e8643ff24a4f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "342c4650-0350-4b78-a2bb-7f325b5aa8fa": {"doc_hash": "bb7053cf4d90d478b41f0654a49b5b7ca063c65f914c7bc83e18144ef301575d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "29dfc462-225b-4b63-909f-08375cfdfbe0": {"doc_hash": "9d6a3e863d168ed2844344a76ae194f9f78632ab76faf43b4c16e6c36739f3b2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c": {"doc_hash": "a076bcc2c77d44e3e29fcb9d562cc9c5a1e20221ed37f66a662de71aadd9f896", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a7e6b31b-969e-475c-9ac4-e3369d1e290e": {"doc_hash": "28b0c0e570ea1d096f1584e39d7954a754bd0031df3f3911f0d42d91ea182fe2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4d4439fb-acc2-4330-83e6-29c175f72014": {"doc_hash": "c9f3b52c7fe687850a97f863fef7090b255c5d0060a1ecc9dad04ccec563cee0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7405b6be-90e1-4a29-8eda-84001617cc06": {"doc_hash": "c93ac39a06c440f4af8cafa2e75549985c598d99ba5c048f33ee13b67b20d6a0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "64de67c2-506d-48dd-afaa-326decdcc837": {"doc_hash": "8c9538c4556356dd7d343e5a8383c5dd6078f955d471e170076e0d069cd53e08", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "eb6eb391-0300-40f8-a61e-b107bbbc0c2f": {"doc_hash": "ee8026bf655b8d75259b99c63f618ad748307b2239ad6a012b855f43bc0e607f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "32b284e1-bb6f-4eaa-933a-ebe480aa6dee": {"doc_hash": "bed1d20b63dd2e1faab5218ec415e3192691bce005886f8b6638aa442e52d83f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5be36c37-bd33-4489-9e82-30ec2f71e715": {"doc_hash": "4b8ab0cf9f31fdfb0bf453c187c8fadf9cce30bdf072b9875ea3add52ce4c798", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb": {"doc_hash": "e3211bd82aa68a1fce12c2b55aafb16b0c1c2dbbb2387b5aca3913e4b5b6792b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a5bd8d71-1d42-48a6-b126-30be2ae64025": {"doc_hash": "1deed967ff0520ec26cde0749d11e426b6078f1f1cb705c5caae487aae5e12d2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5db9014d-9b3a-404d-8196-e20299ef608c": {"doc_hash": "d04f81005c16594dc5f83df6c120fef7553e65bed6b996dbdc857a9f13d5d790", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa": {"doc_hash": "bd9b72f27517c2df5604a97d1010bae92b6f0efcb0b379217a536e8ec7c52820", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "82281413-8084-44d1-8d81-fc1b08032af0": {"doc_hash": "916c2a1950339c90498d1f6a78f88cfc8a81cf0ab54fdba6726bf76de388661e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "82c0aeef-ca56-4431-81df-3627966fc03e": {"doc_hash": "b3beebaf1d13c777fdc08217e693df63a08327a7ab7f2eb0bf52c4981549ffbd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "38507cad-f8a5-4c49-8a94-afd7c4ab52e1": {"doc_hash": "e084338573f45bca10bb97df750b546b904a328285584f7f25256d468781ca47", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7931d982-c024-41bc-b292-15c647a18a57": {"doc_hash": "0aeb91cdb8a2ec736e791dba0b715582330cddb24b1b7a7fcebd421f0f657412", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fcf22dec-41ed-4d55-9432-d48f0b4acbae": {"doc_hash": "7994ad8c82e0ef898450e598faf7cc4037c3f4fec28dec61f4d2b0574d6e56e4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "afea7bf3-6755-4d28-aa13-10523385b153": {"doc_hash": "63bb62414cfc8229c5a19362ba9ca9882c0c9e310d3f0aad2f44015d264e5465", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b6d10179-db9e-492b-a0c5-6966a7837c79": {"doc_hash": "623de37adf0454d3277819fbe38a10f6af0a7f2be6a02cb399d3379e8ff8f15e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0a4d8153-2591-4e70-8dff-a92c0dc0496d": {"doc_hash": "e704461c00fc15a484ca3ae26417044b6c85ff46ba19e204f520cbd4f1be9fde", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5d75e662-c547-4ba3-8f2e-4d05ad991f10": {"doc_hash": "5378d7c1ec485685eed94d092f6f306adadcd63567757f463e8684578e9c5974", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "90baf5a8-651b-4fb1-8eef-d90220395627": {"doc_hash": "2b99bf457f4b2777dc8a8cf1ec8927fdc1ec199b9418d3ca652ef42da08f32f7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5084e129-7b29-4991-b338-355406171b1a": {"doc_hash": "143f9cc037b3328dc67a5206258d687931c198b82322d90e98145626d7f46396", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4db0373e-93b6-415a-94cf-0493a82f1ba0": {"doc_hash": "036e225e863f62201e06bfedb05409aede9148e67fddb3d2fa733a1fb4b33653", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cd0039d0-ab32-45f3-aa93-10834e76228e": {"doc_hash": "4a5be4a3bd1fc26fa024da6afb18a5cb88479b974bfd20ffc1afc9761dcc6a72", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293": {"doc_hash": "1d4158759afa2e6fdcf645a795b71ad437c4534256f1d8cf2985797f1bf83adf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "82e6c009-d0f2-4faa-ac24-fd1786c0443e": {"doc_hash": "3d61acc5aee4cfdf7e1af0c06cd25126768ada9645551e8e7efeee2f9b036fd5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b": {"doc_hash": "2c2f69d0191faaf64434784a383d28c773541f7760a1703d68454e974c53ddf1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "18da6f96-247f-4fec-b2c7-74777312f774": {"doc_hash": "c1648d847a715109df490d82fb140eff7f48be6c7be58301336383a3ddd03b11", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4": {"doc_hash": "af6a7e1572cef1263c997133d29e4b7095d59763f612e4d1a655ffa08e3ee007", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9": {"doc_hash": "671972d965099b582a0ce34259a26205249aef9b014635e77c6e288def9d2b96", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "de3a88b4-0bab-4599-8b00-c305d8aa693f": {"doc_hash": "e78c01ee80d89646d7cb9356b4f86cace9f1dee48e82d8b3dc035100f76919d8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9075e7da-a7c3-49d3-86e3-98a8f728c72c": {"doc_hash": "292a380384d919838a95bca7ca20bb9dfb907981a97535d25262904b45a01029", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6d22501c-1b2b-4249-9a81-22f372250627": {"doc_hash": "5754724d781bb2446474b04398cbf473e7a878b8362384cbb5d0535ca549d8f0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9e003a72-00e0-4457-9fd8-b24ade8c6215": {"doc_hash": "3cf6d656bfc8078d28460fd4127d11e95ab084dc8cd31010e26e7ba8ec926add", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "367f703b-3d43-45ea-aaa9-7e6c1fd00231": {"doc_hash": "aa5737b647d84accc0e6203d87d6e77946e33ad7e0b05af207a635b5b757851e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8b096f3e-379b-4790-8e1b-20c4050d6c04": {"doc_hash": "da57571903b31c641598ba5afe79b8878dd652ea5099d92a6e7799c668b61d2b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "32b30a21-99c5-4984-84e3-8aa7fb3da840": {"doc_hash": "af25fac70d0cae35693e8ab971f921a03718577842104200af2c16da418dc048", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c1473f48-3fd4-4d9d-ae71-6ef716f6a443": {"doc_hash": "633db2af74ce02401cbc679774fa76a1d2beb45e02e04b74552508f9d4186fa2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a4450f7d-86ea-4489-a8f8-592d9e1106b2": {"doc_hash": "74c87d2b1e5e6a17008276d35cd7523adba48d6d343dd35558ee7cd6f7489c76", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bffc1a77-e752-46ff-98d0-926587199147": {"doc_hash": "ce307cc0c6f043051289ca53868dc3fce335f6ccb2de8143d6a96e0e918d8821", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "730711b2-f2e6-4881-97ae-60f2723fbcbd": {"doc_hash": "436027459b5038dfe317180593acb1cd9d33972cea40c9d279ddf5c544d9e9b1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1710651a-5562-4711-a26a-d5cd21cf1ce5": {"doc_hash": "da0e55105864e5e8d3957106e85fab81d2580a5b7a5b31cdafcf8a30965a8315", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ff7e4b28-68d9-4b8a-943d-51fc51745a35": {"doc_hash": "ec77cc96e69f64c88b89a2fc0eb3de01501874886dd66a65591a64e0531c6f03", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "05948547-c438-4254-a7fe-093e21294490": {"doc_hash": "b809e8d70cce357ac077578a41a25d21c10a95dfccc993599f8b43fcd3d60b51", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "92faecb8-d912-4363-9a3c-a1d659aaafcf": {"doc_hash": "1f9122d69b05f85c9e55a734f1b4f455735a2926f35b41e639f36b589c0386c6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4653ce15-0e56-4981-9575-4e1c769d11e4": {"doc_hash": "4aeb9ad7c21db5d40029921d25be0da1da32ed4a3ef3ca1df57459b41c043717", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "64eafbef-8222-4b69-a671-e92a39719c87": {"doc_hash": "8a88d312057a005acaa6e734a1e17f1c441cff9aa9e00a843477d8ecaed3cce3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "84713e50-4c3a-458f-a6c6-ea140ae0edac": {"doc_hash": "156d70221825077ec76800be2effc0dcd6caccda3e2c9cffe0e8b4c978297619", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e2d0444c-fb6b-4f0c-940c-8bcd78139052": {"doc_hash": "7a904b4cdb7642b25d3ffdc0d760dbab1e88c63fc09aa2eee6d0e6384d32effc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "18eaf744-9237-4f6e-b329-07628e71762a": {"doc_hash": "c0b5b71c65d6118a3c15f229c239efc87435aad37b1095c035c24100965179f3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f23d879f-3d91-4cfd-b415-f049b77e4ecc": {"doc_hash": "91ce3a4545275f828e257998821d20521966f0f33dc1fcdabef2c758ceec64ac", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9c288223-633c-432b-8af7-34a92f6d8db0": {"doc_hash": "1ae6895e7ad00d8c6cdb6313d0f374b0457c492a000bceabb51f1ff6569c2ea6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4": {"doc_hash": "f5aa62ffbd6c7f5bd1b17e0c4dc37ec6947591b4047c61e5a0168f2129ba1abe", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ccbc9915-12f6-4cc0-a1b1-0f036101603d": {"doc_hash": "c911a341f647a0ccf3a27df11e3cf063877668a6f0d2d516966d2584e464711d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d3f88296-eaf4-4f23-8b61-b2581e660d97": {"doc_hash": "9bbc1c4b8e8c3137ebcc54bb158918e6e99bb5982413efe9965ae02cb6f2d1e3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "118680ef-4928-4cc0-ab0c-dd869b4d5046": {"doc_hash": "e7f86f3195a680caa1465c0a42536efdf2daf58a10e0e5cb85b42dabfbf673fe", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "08599053-8a3c-42f2-bc72-866dc55b67fb": {"doc_hash": "35e99cfcdb83a9693cbcf21d020acd45bb1da168d4f79c3f2d98c2329b50ba97", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1cc734f1-2a46-4402-997f-ee1631ca6210": {"doc_hash": "d9eac80f4d3dfe4d057c5df914f9af51e09fed742707ec3cffba1590fadf0f32", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "22280c5a-dd62-4469-8ff3-40ca7f2738a9": {"doc_hash": "f06e48217f0f324e6492d352fe1c6a9ffd643e5ca44e3840c41a8377259ed840", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d8af5916-e05b-43a3-aef7-c50f5abfe8dd": {"doc_hash": "0d5c8f4204a2cd6e4755cf2f8dfebb38d00785782ae350c835ed3486fa504550", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "597869a5-0f27-457e-9e04-0b9792ee348e": {"doc_hash": "d960796e2d9db9fb7f972a1bb7921461786d54e910db6b74cf04c012a55d296e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5305be54-edac-476d-b3bb-37ce1fbf92a6": {"doc_hash": "71578de904899a59703bbcdb345180cdfb21eefbb3dde3d3d37ffdc6896f70f0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d3b9173a-e00b-4e15-8ebe-a9722495df0b": {"doc_hash": "715a23d1d8a278d8b0e2cb83660bb4169cd9d8840f9ec6186bda1d0d1ced964b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b967033a-0a65-42ba-bb95-35facc7eb69a": {"doc_hash": "81a378400b12f1bcc82322150373547ed0011bc457581be274381ac5bc499143", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "14d6ba39-f710-4c04-b382-741d40c6e92f": {"doc_hash": "758397ae78447e69263eced90105e66057f77530f3fe11daf9f14220b4ccb925", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ee3bc039-1de2-4475-8e4c-87fdb311afd1": {"doc_hash": "fb1b4703ffc2835a050168b13ea476e114d40e5b856c274b1ebd6548358a4238", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0": {"doc_hash": "5041756d1d3bed4f04bcb0ade24e811ebf2b694d87c9fd8fea4e1c0954d6e8fd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ff9f6d4e-5e6d-44b9-802e-42168c61411d": {"doc_hash": "cf12e02f5546ed4fd0e91b12e5573e968823bd0ac9edfda02354b60b5f428c81", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3": {"doc_hash": "21bd90bf8ca53875d6871534b057c566cbb86c8364e1679eb8f279891e23f2f0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f4b4b28e-143c-4f39-b291-0cbbe163dc3f": {"doc_hash": "e7afd65f559caf4de8000aa618a45175011d0e894c3a37f003dd10fb092672e9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fadc2540-05d9-4332-9ef0-d8a2edfd32e3": {"doc_hash": "c328cfe6ec97488049679f49b43f78588599cbcaa59797baf379a6ff55e0bdcd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ad5ec67d-3e88-4584-9be9-099f20811dde": {"doc_hash": "c69b805df9f71d96d33d4b48ba37aed235825a139964f431595027fd33bf31de", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d2909a32-6aba-48fc-aae9-12e5e4a339e7": {"doc_hash": "f21d8489f33f98b162038bef0fbe9f89500762f7bbbb9a214bfcbefa7e036da3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "419ff4c2-100f-461d-b09c-49d92f3e86d2": {"doc_hash": "147bb75b83ad0ad6574595381af437cb07edf12a5cf365325344f81020c75681", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8a284450-61e3-41c9-a5f5-7d7db3173362": {"doc_hash": "da20819100c7402db1cb241cefada62ab89a970d9d5454c3ca5870765baf9400", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "24619ca6-683b-40ed-bb32-292688ae6e01": {"doc_hash": "0a6ea1cd32f9acb33a31ddfd32cf917320b7d30da781e5d17b19182fd63ce52b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1decaea8-06b5-4566-be70-a1e74bf48e42": {"doc_hash": "1d24bfbd684677b89f0b2481963c2e3f62b0e05f4275602e26f345a6215f9f9f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a1dba6fb-db28-409f-b136-4c523f2861f9": {"doc_hash": "52c6a96cf5a01dfa770294ce6046c7b03ccac9bacbaa11ce0ded781708238cc1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c48c526e-d899-4330-8526-d2092bb7224a": {"doc_hash": "4a9e5d9f71786cf9c248ee1624c324b83b9924ab5b9c370ec6de42a47829ebbc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8e374941-a670-48ec-80df-6a23e3195988": {"doc_hash": "4cb2ee067e0e7f6daf047acd9e80bd61ca1c4e2224f92a38d8b85e47f3037d72", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3e503861-ceec-4d66-b756-c7e683b42921": {"doc_hash": "60a7c94415223af01586712d02ba2222caa8b238457701d7c949650e3ed9e782", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "360f4755-ca31-4e83-8d8e-4f95e708e11e": {"doc_hash": "444af46ae5d3c41f50cf61bc3ff8e65168ab7f358b371dca99342a3b0d0817e7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f7e7eb72-b500-4003-baeb-0161f13c7965": {"doc_hash": "9c5b7b23b4a337809d605eaafd9228381d6b51b02789630be830303c5db3cba7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c18b7eba-b3ff-468b-88a7-3dd7d5959a98": {"doc_hash": "bfdcc4ab30b1e0bbc003c298245ce0ddeaccb27244e9f9f83e88dc27fdaaeb6a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "938f022d-1d5b-4d15-9096-ab57320bed3f": {"doc_hash": "706f55061fe167c14185a1311bf34049b3d139d3603c342ce6c6de1ebbe66a2d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9810f677-9bff-476d-8b03-c7717759f185": {"doc_hash": "468f004d981603398280e95ed4a3fc2e91bdfac613015038cb7c4148f82a20bd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "690e4acf-bdef-438a-8a32-2852a380eec5": {"doc_hash": "2d3f0d8da4db353701a56b27741331e393920935bbffe404169c68811f7f5fee", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d4efb1a4-0128-4c52-993b-7f0e3c0fced0": {"doc_hash": "4931a803b557adb1593c156602bdb974639ec205ecfa211de7991229df63c43c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "be79215a-2e71-486f-b310-529a9ef91973": {"doc_hash": "9aa01ef7be265b63906003169ce62ca32ace350fd5b7375da211714d19e00689", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "023eff69-4e7e-41c2-b396-50cb5a708842": {"doc_hash": "cbfc1fd5bfec1aa00a5656b9f1a406a22d84fcfd5d92b7171c251dd4113d1bdf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3ec356b5-477a-4745-ba6e-88e5ae8c4627": {"doc_hash": "72b233672011825e0ca3a612f3a144c3153bd47a7218ade97774f9142a8dc92d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fc7742f7-e13f-434b-96ba-66b8bb2ad86b": {"doc_hash": "1416c80a1652539e7fbabc38ad43affdb3a1f1a3b932fb0e9ec141cb8e882d83", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "55f1ddd3-a23b-47bb-8d45-85ef1ee68645": {"doc_hash": "9aa8b0fb13ee3dcba5c5d87261f866f235a02dc3896f5d6ae3f1687597e84146", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7e8b3264-ab20-490b-b14c-c89aa4be3022": {"doc_hash": "67a2f800c992ef750e85c25e10ebe76f109f7f15116eb1cc9f02c56324aa2ade", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c139b91f-c56b-4745-9d0a-bdd63c9f44c6": {"doc_hash": "80f4436a0946287a712d0c43af7bce783e26b020c1d5b8a4e232d7ee7f8a5253", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a1233690-dd7a-45b4-9f14-6f108b61db4e": {"doc_hash": "4a0e904c16e327ff218f432fd788243c6a88d0dbada20ce6b3e81026601a9d89", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "67263779-31fd-46ae-862e-7c83bcae98bd": {"doc_hash": "f6fcf239abc30bce475a0e09ae6235c3ec58319fed8a4a69b18effb77a1656a6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e427c614-9315-4228-bea1-d8f0459d6009": {"doc_hash": "35a4a5e9f3dc039013bced5c5293d77cc2c28388fc9a80603b302f1511560550", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "01d02fa3-cfb1-40e2-88cc-801891efe245": {"doc_hash": "d3b718f4deb5a67cae825b05cb00e71fb5dd6e3109eb5fb5f7af3fe72fad45fb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3031ccef-e341-488b-a3f2-80fc1d2718b4": {"doc_hash": "c511f11122f7c8f7a6f082b78f6ff24d94faf64147a001d35f24f189c3cdf7fc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bff44c06-3069-4bf7-aa12-9c1662f37155": {"doc_hash": "e46cadd0c27f6384a405b97ddffdb63b03b1142c924b8279433c83a279a0a4d4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e": {"doc_hash": "ffd49872ff24f755c46e5d0c46d7e66155d17adca084295f06d3ab195920c9fb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "96f76069-1dd9-4587-9155-c0280df6b1d6": {"doc_hash": "8a439f8cf73586d58765a0d292c6c5c854da57b8518b707c6cb094414d20b092", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6a098279-cd1d-45be-a61b-9e549be819fe": {"doc_hash": "fe36d052ec42c29c8d2367ef0088447ffcc65e69ed9d25d29e1f2688bfc2fb13", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7654d8bb-f7d1-4b01-afc8-117ea54494d6": {"doc_hash": "cce2af2f14d4d02b1260078e956e924c4ca81662248642cb493e6ca9539cdc53", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b40fb0d2-cc97-4642-901a-68026202faa4": {"doc_hash": "f9315039102f62765562de5c6d2ae9d41645480a1cddf090be8820c0227de734", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c79739bb-d186-4311-8c11-c17ef8c0e999": {"doc_hash": "37701029563d39b19dc3338a400ed4e72aca0dbcd13efeda9ecf95dbcce74592", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "039645ce-26b7-40d6-aac6-d032fc917186": {"doc_hash": "cf5bc2d45aad2337b1d83d3a16ea1ab1d1616a935e26e8fea22012d6c184c3a9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "beb4bfe1-ae2b-45db-89bc-1092d6a866aa": {"doc_hash": "aaaa9fc97bd361ac9ddadd2736ae32f12f7ae79888f825d4419d04937b12e056", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7ffb57a7-3515-4979-a2ac-aa466597d7a2": {"doc_hash": "c0161ea189614508191d2f379eb2038521003c8f4eebb4870e122cc76c0ef9e6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2fe9b0a2-5081-4e1a-a90f-568e78017e3e": {"doc_hash": "38929ce94ce34d8660068c809c291e0740f187c20c20da55b770e8a1f8449cf2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4": {"doc_hash": "ed03ab4f12a0a18280a06a46f782b4ff9c4962bcc93707b522ec49c2a04109f8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c151e3f2-0172-4485-868a-8a31aa20a095": {"doc_hash": "4d5d9bf006346233fda0ee48f80c71d99bd935ee6509767813732de86278ed8c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "aef326da-0073-4176-9d55-1dfe4fb9fec5": {"doc_hash": "f1e5db43d206c79f1628cfaf1bd9cb6768bc2a01e013fd8f18afd96e87ae0d24", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1d04f3a5-b9e5-4e12-bc87-bfa511016c78": {"doc_hash": "c904704319d70ea7ed164af9ef09fa1d52175f5cc14cb9fea14fdfacb01e4d2c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7": {"doc_hash": "bf1da7b465cd45a0f1cd46b454eac286104616364f9152f6c297f1b925d4968e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9c69731a-be9a-4009-b3aa-8f4626ba5adb": {"doc_hash": "b36d8055da028491b7db7b91609533338250b6d20eaeeea1655acefe1c8525a5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e3483b7b-6c2b-48f3-8756-a3f79c09770c": {"doc_hash": "9d9079036c19a43b00f0b75eafd56147761fde067a74b3db25991a8062bd1c8e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c70def48-f634-49de-88ae-da400c8f32fd": {"doc_hash": "415bcfaf34887383f682c190a32ea4d01dcafbf03fa18399f51f3cc591f1fbc2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "75540c36-8d19-44ae-80e4-7e2d00207ad0": {"doc_hash": "3c349835e3a3ea28f9bef8e42c057a0bb6711526011da002f1a1eeb7a9970b37", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "43afd3f1-c638-47bf-868e-81fbb74bfa43": {"doc_hash": "c8cfdbda8a591595c1152b8ed1067518c6432459a52a46b587e02c8d2e43cd87", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9": {"doc_hash": "94c8e9014fc02e20a90c57b888b6f4bc0166df2dec5a7808cdea2a5830bde1eb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7f566fa3-378e-499c-9fc9-e413776e17ba": {"doc_hash": "6d2bb92cba31c1a3efae9db24ed0b0034df6009cd05f5302c473b0a8e66068f1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "314f3be1-65e6-401a-9e89-4ce9a0dbab08": {"doc_hash": "a07c5dc0157d3c02d5eaa2c3681df00877f5ae0b5a1bc33dc9b0348a915e24af", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "af07c2b6-0049-4443-8273-5d7f2dd1e868": {"doc_hash": "d3fdfbabaee47f9f936fd4bbf61ebd471d9c6ab42a22dd8baac11264e142a8cf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d07bed13-a8d7-47bf-a416-a27f19e2eee3": {"doc_hash": "2ac667ec3a04578c7608bc4fa42150cdc5a83ae9b6ea65d5a46c472f84a7e5e2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b": {"doc_hash": "758c6d59cf194f427e801ae355be0cac78d6c04f1a0898e3d4309fb01ae96437", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e314e09e-92ba-41d7-ab2b-979237ffee8e": {"doc_hash": "a03ca14659361a19e5f1bb635fc57448748834cd35a52d692586467b7754108e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b9526e2d-83b4-4945-a15e-bf9a5c214db0": {"doc_hash": "9134ac27249443e67dacbf03b4e616ce671d5fc1af62f47564409c2e50027862", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c0acda8b-015b-4e2f-9303-638f791d170a": {"doc_hash": "01675b53cc10172feff1ea4bd2e216950190e1fa59232b7bed034da370953f5b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8468dbe4-714e-4770-951c-e498ad245e3f": {"doc_hash": "3700a2b9b2b0b2a39a0e1e43a2eb8acd5e0c239fee06fc1cd34d0df61d94ddd9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5822eeba-5a06-4278-8df1-1ca75b61a3ec": {"doc_hash": "b4b8348f2c3a3fc195ad44209f11ca3da4c2a548250e078b8d7ef9a4965e79d3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "06cba9e4-c6ff-48a5-b3e3-a0508d82c529": {"doc_hash": "6a1b29fd0625e5f3636bc2bf9814c4537eefd2f2ac7ad86add130823bb0fb28f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "092ccb19-fb27-49da-9d9d-4b3397a7315d": {"doc_hash": "ba0a5a1c6b1f1de40566b35ba6785251ba7bc8dc56d70e8eb15a64c178e469b1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4443e3fa-29f9-4278-a011-af8e3042177c": {"doc_hash": "35be6ce33780fb345acec11c452a976f0ce9b5bc5f1a9e00941e090dfdab7146", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2053d1a9-a0eb-4913-adca-5f10204c5c21": {"doc_hash": "80812319139d88da0102033d415b3aabbbf3feacbc9adee94cad3fc773f6a0bf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e5cdd13b-20d9-4291-989b-e407b1183b83": {"doc_hash": "98fb4c731b3cb2b1a525e1a010df1bd6e0894a131ac835210d34de9f9c77ad0d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a3b01a48-b84e-40b7-9025-de2b14b2fd09": {"doc_hash": "0248b1de5c3ac0f97c567faab7a86b322eda065e2a20665deba9d2ff53da90b5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c3967a15-6aeb-4631-9d46-d6477070a86f": {"doc_hash": "4547b87b799a19fec81dcb2bf49793b94b1bd247fc423555b0d680c6d76a4c25", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a736e4f9-15a0-40de-967a-6e35134ae524": {"doc_hash": "5e3f8039ef8759fbe59e1eeeaa2047d9c441137afe94a4312a63d1c5196b8614", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d716e990-de77-4d34-a4a5-fd9f39686f53": {"doc_hash": "9dbf25ead4360f4099544fe32aae08f93d4270290790c00377efc1cdea5f4714", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4d45b420-02a7-4774-b242-06fa4268df16": {"doc_hash": "6ca3f81b28125e7843876e3b482863e00ce1b8b616cd3f6e049def51a77b6d6f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ae4e376a-2014-4c43-a327-8b33bc9c2196": {"doc_hash": "a16ffc19ed98391fe3c627a14ea54cb8222fc22cd09dbcf25cf1001aa4f7a925", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a506b590-0a4d-410c-b8c4-489e5beab966": {"doc_hash": "fbad04eb9c05ea90e0c56a484dda202306ea476274b80b4783f97c3731a880d5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b5613560-6433-4e09-8a9a-13a7d194d8db": {"doc_hash": "3f98743ba37e3e4bf3f809aa4cf7420e3a0d643d89096d0eb79f2b846294fe73", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7609f27f-9005-45e6-bf37-cd28313e0f1e": {"doc_hash": "b087c98fcf3d167ffddaf5ca5ff144dafaeb7c5580f8ad1afda83111551df89e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cf4ce08f-d95a-4d0b-b0f9-514709d96e28": {"doc_hash": "5c952316851776cff61d476df52a309ffbf90fd38de80ad553ca76340b72718e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "46b090a9-cfc8-4bf9-8c54-3085dd3539c3": {"doc_hash": "15cf924517700a3082b80534906277c1d5e551c55de88e568cae5f733a37471a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a23b8f52-cca3-4f99-a381-fef16cf0754b": {"doc_hash": "677c6e6af170b16be004f1875efecad5e77e33986e149dc2d1e52cdfc219d491", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "03342b7f-810a-47d0-9213-0267839e7c4e": {"doc_hash": "17521b5faa2ac2be8a534c424e32dac2be196497b0d18fdf298313ea54d25f05", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d78fe297-fc46-4458-b14d-1456143799a2": {"doc_hash": "38154368014a341f55bf27415d8195b0f34d9ebeb284a53dc26595e21e884314", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d072a57f-da17-4ec5-8da5-66a9091b9d73": {"doc_hash": "b2dd119c96ac63de3d5c3f0c0ed09b2cbb7f784c4c798acbebf97f820475a8e5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "52d8f2aa-cc48-49ff-8feb-e959db97b6cb": {"doc_hash": "aba1f56908025b79244f1a9a2f759f0c39dfa67157889844052644f2d04cd1b9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fde529aa-0156-4c5b-994a-b64edb137a6e": {"doc_hash": "d7f1effa995a1141f813a9c3ec98718302d1d38f027704ee1ce0f6fa5bfdb8c8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b7d87110-7365-4065-90e1-354cc47b22c0": {"doc_hash": "356d73e17ff720875047d727d9bb3e2a50f62d33137e5ddb0e79edaaa789e36f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5": {"doc_hash": "d32f959a5bca7ebfdadb353220f14f6e47b68481d63c7ceb28493c3e9c86b83d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df": {"doc_hash": "eb1fc7c056223468415c0a677805a1a4edd76c5e11a23ef2408957f1b93b5d4d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe": {"doc_hash": "a68d998be9e4d8e225ca13d5f35b6998b62ca86d335bcb7afe44f28fb34921ad", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6bd26748-b23b-495b-aae8-438cd255d06c": {"doc_hash": "dc599b9130bef3b02020f9b3b2f51a4e49d36610910cc646d5942c86a4ca4826", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5fe145b0-c0a5-4220-b961-c08a3da9ca00": {"doc_hash": "04a05553ce395e695addc85135fd6319f3bcd52c1c54dac9c83eb16d2c938870", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "31d9931b-bf83-4fed-b610-b3de97494e34": {"doc_hash": "baf6b8631005c0159ee11b27a9842901bd83648a982c4e8724fd4eb976a5779a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1bb40e13-bd0f-4554-9d77-37c1049a86cf": {"doc_hash": "1b89a15ce844408f94b70e1cb60fcec0062b79a8baaf527f740dfb6fafea8726", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "15bb5fa7-cd60-4856-80e8-c097e50c0891": {"doc_hash": "17723fd35e93df91b06cdf2a51ebef34c9d570fe0e720f7aa197461770fea1e2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f7946f54-1cd8-40fe-aeed-5a5996f21735": {"doc_hash": "ba23630d9356be1eee45e52924c83a490d8e967fcfd7cf2debfe918b4a90b4db", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "92bf1c5c-34a2-4149-b604-b4edc88ad719": {"doc_hash": "01af9af2aa5f21dd92ee17983901f6c1e4cec99e896fb534eb5f6abc2ca0a488", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "72ba7687-abff-4483-8f87-33301c608868": {"doc_hash": "317d359b226d59f6e826e6b4ddad75cd09f195e4181b9d1bb8bbaf4f71efaeba", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "31120862-6715-4546-81e3-d43f5ebbf3db": {"doc_hash": "f4e1c6c39c0ca844a6c0a75bba946c6a3661c6eb8502b9c721d87fd043050592", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8": {"doc_hash": "9864325c72532cb5a848de541eb340bf4613c1ac93e3ae4be01c1d83c5d97bce", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06": {"doc_hash": "d40833f2fd1bf96ee9699d1243a435831c1dd4e1d27091a4945e460f0418785b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "76fbad2e-6400-438b-a560-be46e856503e": {"doc_hash": "4fdac261dd08f51235ffe2ab9710b1348e0bd079addd30f0564275155e649288", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "918faf00-ebb7-492d-b043-0aff7fedfbe5": {"doc_hash": "62c10b304443e247e35ff49a9b8fd0ce7ab9b581cffab7984b6fef852d765f5a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "83059430-e440-42ad-aa3e-a9cfc4a3be03": {"doc_hash": "239adda0439b8ddde07f46bcaba88711598042635f12e96305910ffb9cd166de", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e984b3bc-8c6a-4f5f-b11c-26f258848ce7": {"doc_hash": "242adfb4424049a0163f8daee8e6e89f1507cfbce99b26602ba712874444fcb9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4860aa7e-93d2-4057-9273-b61143af6859": {"doc_hash": "2ff9be8a0cc6b2d402437f33a4b96da304388d933aad38ebc11028e3d001ae33", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e27568c8-9294-4ec9-bc11-5208a849a15e": {"doc_hash": "01303c5d8ac0ea2f5cd86664029ccca9489accf9b168660cb504367916d4c378", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "67db6a3f-4b76-4b66-960a-b52084360442": {"doc_hash": "0ad0203d458225294562349d49355d29dfef00e483166496dc8fe548bcdc8e13", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f149ad65-76ea-49ad-b1ec-e8be8ec89a23": {"doc_hash": "3fff3676339b39383bc4aa6e1641b6bdb23abc48a3618d04a051903b2b889a26", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "21d0cef0-d28d-4096-9c2d-c859a873286e": {"doc_hash": "1d2eb5b297e8f3db9903fd2534156c5b0cc3d54817fbe6024ca09fb37f4b6b8a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "65f9b2e0-b3e7-4056-88a4-859663f08582": {"doc_hash": "f40de5e1a308134dad8b84c312268e5bfb7c735ff2c220e20fc5935feb56dd33", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4b8758c7-9793-4811-8d87-814dd645f6d7": {"doc_hash": "d572a04d4c548240fef8b1ca8c81bbd828941ffa50264236ce1b031662911257", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "635520b9-4c10-44b4-b9ef-6e63f415f6df": {"doc_hash": "d719a535f3774431cc46713ed0ab6cb5d5903a4b9a7ba8704620813bf3391327", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "19b239f1-aca6-4429-bb9c-8895798fc507": {"doc_hash": "6704c2e7d7b703ff479b309bcaf2f83c106cbc7b8b5f44e4bca421ac35a67308", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367": {"doc_hash": "7187a2075f94fcb86b0c16f37835dd278b0741dc3697daa9e03ae88896d61c91", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f93f9750-eb9d-4469-85f2-ca95c8aa1824": {"doc_hash": "e15d4f7fc61ed496ab3a618444f6eebbe2e5966402228bdf0878eeb431f047da", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "90699bf6-8283-49e6-a729-33c82ff4a77e": {"doc_hash": "769fa6bb98931f78867814eb910b5c7202ef5d51d94ae89c7df5aebcb635513a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "064db35c-1d84-41e9-ad4a-91ba40ad0888": {"doc_hash": "e83b25124d8deb09ef07e31770e33fdd60d519b2ec75545ba39bf66e9c55d29d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "34f1216a-326d-4042-b7ae-82fc20263a7c": {"doc_hash": "b7478c6e3a0b7c9e6cdcb0ea9165af75f1427c9ff5c346a598daab1c99c93e9c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "89aea79b-b545-4b89-8247-6fdbfe3a63dd": {"doc_hash": "3e48a8f99d51eae850e637cf0c8617cdbbaae7c43e7c4f284db34e577b80447f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dd164969-2607-472d-a2ce-a2a0e078d1e7": {"doc_hash": "3e197af4c8a0d5ff2a4fff38da2db4685ef16fdd658a6328b88b041b2a1fef4b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f07533c3-5f01-4172-b371-ac2859efe5b5": {"doc_hash": "384e163243b2253ccda977bedb3515b658ed823e0a42da7ffe71b1ff4a877a07", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f2f7e06c-d897-4881-87fc-4cc7fa506c0e": {"doc_hash": "85d96e10bfad3fb7c1c4fb181564d7143784e024962b89b90325982afce843f7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2a770de1-cef2-494e-98f9-06ae5abd048b": {"doc_hash": "f1d194bea658a1adf171ed4cd20b21ab2281f278bc9a3c85ad406526da1384ad", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "464985a9-2857-4151-888c-5c6072a051b3": {"doc_hash": "a1d5510127de95b3a55091bc3e286d3412167eece0846a0e45e93b3d697e08e1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9fd8ad0b-305c-464e-bc28-68b84b940f6b": {"doc_hash": "76c3beb6e63433ade31df1befeb3e1c2ff1814718b0e16fb2be3f5cf688b2dd0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3ed3543b-380a-47b4-aeae-0f2c88e59108": {"doc_hash": "a0f091a30fd53b9bab4386f1367f5b284287d2027d322c687af1b3016b1490f2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cd63fa0e-04ff-4403-9eee-127f17f44a26": {"doc_hash": "c9463e5674d5ec24493ad7da64d6349ed55c10d8f41e61912817670bf20dec6b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dbb2288d-ef75-42d7-ab0c-96b58e170a53": {"doc_hash": "d2cdf7c1378840812161c6d750acfb3103e7b0820559e1ae884cd06ade37aac2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6a167a58-de5c-461c-bbef-e11384d4c374": {"doc_hash": "2b262276135c49b753b33f9fe8f32221286c863a08a69a7632df3a89c80da639", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0460ea95-4d81-43c0-bd74-78234b16f512": {"doc_hash": "c4d4862ec93db9a0670559e15b201c7cab32e12a4e7adccbd618d0a08996162f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "07a5fec4-52fb-41dc-b0a6-bee707405cfc": {"doc_hash": "b1f4e1d6a3b581f84a698a7aad545f9ee3b7a6e290ccd3ec6cf2bf336d2991a8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8": {"doc_hash": "3b1198c49aa0968adb651c6122468017d9af5c979176b7622792c77a646156db", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "da583324-82ae-47e8-a773-196fd3a1afb3": {"doc_hash": "13ff3e463952ad61aaa8896e952f894f6398e9ccba81d6e90618a310ff12dc31", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c754b380-afd3-444f-bf07-b93620cb348f": {"doc_hash": "85605a7be99cec743f0c3272792a0586db64575f418d114f0141ac14ebc30a5c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "66a026d4-de66-4c29-ac34-65539d538063": {"doc_hash": "57001731ca4fa50bc5d4ae90cfe9e357564cac0fee4f4044debfa163a9cc16dc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "76a5fa63-76a0-4a2d-baf2-be68f493517e": {"doc_hash": "567232e2dfdefa78e02f7af1aa68eaf385f14ec2b80ab1b22ae9b20ddd06fdb8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "26a69f3e-922a-4b3c-a435-6aadd83fe08f": {"doc_hash": "ef72a7c052a03164b10477fc5230fdcb24940c14ff40c59a11c9b2f34056ac50", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4e1521ad-d30a-43ee-99c9-e977ad27b0d5": {"doc_hash": "29267659ceeb1bc5079f9a0e1759439cff859d42fe6ba05119cde9d238e03604", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d603efb8-292d-4488-852a-4b2911132cf9": {"doc_hash": "1222cbeb9cf76a40a073e5744ff5d6e3f5aba02fa7d07feccc1672480c534270", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b12523b1-6057-4ae1-96f5-6d68b51e5b0f": {"doc_hash": "0e0361131acc244ff6f934fbb395343738d264944566490e0e5a90fc4badf876", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78": {"doc_hash": "b236513ae9fb6d7317e73c7504b59a1a0f85a67e1e40490846fba97066793f64", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0119dc57-76f9-4054-ae8b-99be2122b676": {"doc_hash": "cf9a0f543f0bfb77764b99b6e74c96a2a5b5b7552bfc60a538a4c52af66f7905", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2ba2df28-af2e-4f36-8fba-aa3b89410093": {"doc_hash": "59e83c6fc90108dfc06db7f5d9295eb3b5e36046ebcb191a6751214df584533d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ed893ac2-63e3-4735-99b4-d2d5341f41e9": {"doc_hash": "865e07b17a1861e096955bd171e914c814ea096462aab5ca2423f876930f3594", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2ff3cea2-27d4-47a5-8116-4139af197c5f": {"doc_hash": "9389a434a54f7f4e29b24c8acf2dab39af09d745d453666fb038917bc3c646a1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a0f42204-a9f4-4e63-82d6-cd4b452ccf18": {"doc_hash": "6d220ab9d128a173a0063f3f8b34d5e7975ab0fa447a40b85648b9b73293515d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c1cfb007-d9ab-4278-9294-ed1db025ec33": {"doc_hash": "545a720eb9f2d77fb5af8c7034a72c2c7962049081654afff1216e9fb3511a48", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c38611bf-f02c-45bc-9b89-236ac7424030": {"doc_hash": "75a3bbea088f8fa1f868305ed943c603ef3448cf748dd7dda89af769702bb357", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "07adbcfb-e300-462a-8794-4d0d3adc0efc": {"doc_hash": "638a9ba4955e644c6117920f599fe0d1d87d80053d5681cd3f7baab3335bc120", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b640beeb-69a8-4b36-b994-3d53d5aa6d10": {"doc_hash": "b52c0755abde7427e009ce3c62c6b2256f5cd5704d2ea17a6cb1d701c9a94919", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6cf877e8-cc97-40df-ad9f-aec4e3299025": {"doc_hash": "5e8c030d27083df71f6f332221ea05bf0560e51671a8dd56551845aaf49a8d48", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "365d6a92-e13a-463b-887d-b24c4a770740": {"doc_hash": "817884697f6bfa3a4eb53bca0c013a6768ece5352231f8a94e40d378874cbc33", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f": {"doc_hash": "a0b5f23ed1c7e562380ea8490a74d8a48cd95553b304274c3bbe1c70255f3d43", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "46f837f3-36c4-4da4-ab5d-24b1ff5feec1": {"doc_hash": "223aaa91989592ab367edb14101e15f93bb22177a48ff3ff76876cb2506365f4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "726fed06-4af9-45cd-9095-db49e93a1682": {"doc_hash": "5b4df9e1a576bdf36fb6b28cac0b26042fcffe982a0dc15df89e5cf1519d73af", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f72c7f07-7c7a-440c-9f04-a726e5ca0abb": {"doc_hash": "7610fe448550d681d239d110362ffb61e0819a6137c38eb96e7ad565358b300f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4a34583f-86a7-4439-80ff-5f60c350afe0": {"doc_hash": "b232772ed358e641c9602b414dc2fa30a4ed039946b269cdc9320c809130bff9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66": {"doc_hash": "d7dd585159d5e81155d482ff3bd10e258c01383bc9ffd43d5d365da9adc603cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7adf3fea-86b1-4bf2-ac04-1fff31ce545e": {"doc_hash": "8971c2974b09ae31c874ccd23714beedc2487a073314a84863cf0ba01dbe9a36", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "727aee78-1ea7-49a4-88b6-03d5f68aad5c": {"doc_hash": "ba0d7081fb11172f7a1478ec937dba906de8d92d785000e2b508900502db3795", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1cbf0323-5071-4235-92af-2c3614f43de5": {"doc_hash": "3f74bbf551f9fbf74c3398c7c8ddfe83bbba1bf9475ad99121523a650d6a948a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bbcef697-a7a6-485c-b633-32b07ed65641": {"doc_hash": "9dbfda37b073ed0c6214f19db51d5a4adb5aeeb309570b5bda16cea886a7eea8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "27185349-25e4-4e6d-91da-e59d0b74a82f": {"doc_hash": "5887ee93b51ca55d1994fc72b180751146c111b66c240c5202bdc153eb372901", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "65148b00-abc0-42fa-afc1-0f1e2bd69461": {"doc_hash": "8c457b49092f702eb0a5135e9dc7dc3b9f566c1e9f2bce8617d701eee796614a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "31c25542-a888-4561-b4a9-aa0a07a08d85": {"doc_hash": "062dadaf3b320165d12e0ecf636932678152540f3162897cbbedf5372628d537", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b8915a20-ad26-4ff9-bcd0-751c77caac3c": {"doc_hash": "88cf1fbac0868228295cc213bafd49c5d5678c0eee781a822e113eea336d3cee", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c": {"doc_hash": "9a8f936411516a218953aa3ba030ce2fd0828273550cb5d7d892a78a90e70082", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "529858d9-8603-4c08-ba49-00545531b1b4": {"doc_hash": "89af4ba0d213aedc764fbb0f5e7f974f8264eacbf2d5141ce6ce9c699995dc26", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "efca3ca7-2c18-4498-aa42-d647297b6ad7": {"doc_hash": "47ac06dfb3be6460288d5f0db8c0ca644cef5c60aadfc16c442db34a6127c030", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "827a96ad-1d49-4efa-b344-7152368b2f5d": {"doc_hash": "c41ba464e46b154ce214c8abc41abbcce43b68adb8cd1952a13ff6d9ed227ce8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a4fc6b85-7233-4ad0-8178-1217f14817a7": {"doc_hash": "b36a3a32cefe111c84d6efd3222735b1f241c0201087bcaf1f0ddb49a8cc86ed", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "278365a7-7adb-420b-9c85-a10842275f43": {"doc_hash": "8c7ff1a0337642a3010cfa89b4639747da71dac2fbd3660bec9a0a64036a6a67", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e0300cdd-ff72-4a91-a797-1bec8fda1677": {"doc_hash": "55f103441669c509ef9c7b41b4cbb1fada07bfae0aa12d788c9f1e278cf340f4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1b7b4486-5104-46f9-a50b-a08c8b5a6093": {"doc_hash": "bef14d001cb675c5ba926c7322d63b6613879ff393adf22a69f0a9ecb041667f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3ba9b9bd-8df4-4c5f-89c9-313880c233fd": {"doc_hash": "b92a2f1b1cea9c1603384d1f9d55acb419e408a81f66c4454362c8403e4c0532", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f2a6e367-9c40-4ed4-8868-565271494414": {"doc_hash": "be0e1e8d71ea4af419294ec3797a3df1c938223d3c848478c8e50d1418267e39", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3": {"doc_hash": "e41621285535813ff4c1ea9c5e5d5b5511ac307bca5f76f8fe38142163237b53", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ebf56c4e-4e13-4b06-92b4-331a09d3b374": {"doc_hash": "2dfa33e5a98eb41d08c8b7bcb3d5df9a16c17eb85ba2789b2db2e886eb8cc94c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f635381b-6b58-4a99-bed7-488cab94aefd": {"doc_hash": "f7f9a050e58a68cea6fc38c8769fce474556b4060b014552f1b50fa589b3994f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe": {"doc_hash": "1d7cffdf50e0883c107d81c0cd2ee765741fc08b97f0bd3acdca56ebf67fcd21", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "542fe48c-dee1-47dc-8ca7-75936d21d08f": {"doc_hash": "578c9c87394e9718c482c28fd9502bed50b04d03161744dc48e12d49681616c1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c63f546a-24f7-4733-a52c-013e7b99cb9a": {"doc_hash": "280635a5aaf56305113720bd91a9639ea527a8d986a75cadc8a473d6f3ddb627", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3a59884e-fd56-4b15-a9ea-4947007b65b7": {"doc_hash": "5ae0408cd150117425f267659d7873eb8111c6882b7b0b522d8e37a092f95df2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4ec86be3-a5a9-42db-b554-9b804aa096d3": {"doc_hash": "39383f516164c2a2142c924abb28151b25ee6e3dc7ba1e583192176585b0cfb8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dca2672f-a0f1-437f-a468-a331230b858d": {"doc_hash": "4261515efd0a4d1ef7ce0fb819191047c859fb59daea878cd41c9dabb7e9fc2d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "92701578-19f9-410c-9b3b-68375798b1ca": {"doc_hash": "4304a115e1bc30c8a475dd1ec5a52522254dc0b2a5feb0af97a917c47db5f93b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "14b60704-7953-4ed0-8cd9-1a4746ad1992": {"doc_hash": "7c81552c30877f11612fd54cefa5458881d400cf07711497ff15f45a24137195", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "41e3336e-316b-481c-a93f-8bdf0880e291": {"doc_hash": "7c545fa7b8d64eee77488744bb17322f69232e3a724d26a0a29741e4ada8d3a6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6678ef83-b426-4666-a2d3-72d33cbe5e87": {"doc_hash": "08680de64b09a3c9f1c05f93ebc3652ca173d8231bfee77d88cd99ecfae94ad4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dcb43b7e-7506-4082-abd0-0e8c312a1966": {"doc_hash": "19df5ccad4138ec990ff50823d6812d96ab4f67e9aba2f0dc37b9d52b079929d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "11b26dad-728e-44b1-8a97-aa93e0ea750e": {"doc_hash": "f3502fbd6d201ee4b28a6b9e4a82e65e1b0cccaed4732ea1e115b831e832969f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "96e83b06-b357-4f31-a650-ce86436d5a83": {"doc_hash": "b7b7761f427b5295f225bc43a114c4bf4ada4854c7c2408a5664c9dcdd3347b2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c64efb4e-fce5-4301-8d52-723b6cf02edc": {"doc_hash": "75a88f1aabcbd689dd349377c8cfb301f7c3ab38459b531296183a4be03a610b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3635ae71-2ec9-4b0d-b2cf-565873e2018a": {"doc_hash": "61ca35c897b95132ea59ac190736fb7bd93d0e9f79c0d88bd7babf0e272dc2e6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9a50eded-d210-4ff3-aad8-33e5223d4030": {"doc_hash": "44b61bc43497e8b5bad56d8122bf5fbea133512533eecdd2d0847474f1e5db47", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5": {"doc_hash": "20ba4e18119f125dd86d4d121f8fa2ee032edb0b9597975d9fe3355dcf41c718", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "518b0288-4861-42ab-8a85-d57486b02147": {"doc_hash": "a9b496ccd32d4af839eb687ec26cf7d7327b2274c7639aa4fc97c8475f5d1672", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6": {"doc_hash": "e9025ec21ce9408c9cd688afdb94647a35a35235921afa0d7993748b2bab0092", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7": {"doc_hash": "22087fdf24aad8401bea83557200137848461d1402acfd43c411fd682ce584f8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a3da216f-0f16-4b1c-b84b-d36f973e35b8": {"doc_hash": "6d1bc9402a85dc4786f3cb0386813f0665f42d58c7ba89cb155c018a2a3267a0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "938f102a-8d57-44d2-affb-e138b54e0297": {"doc_hash": "8bf31909dd858b63e3d86a483deaddd3e31c6e1c477e64b7d14dc4d99a4904af", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0c53f415-0e52-48a9-9476-927d9fd4a8c8": {"doc_hash": "611bf36e6f8b89552b8efafc020126b4074724c816d77e0bab87a5c45d50f786", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f8ad1767-2616-42d5-96d2-06ebee597123": {"doc_hash": "b4c05c2804734a3fb79235cc258c0fe9eb9cb87491313727aefbe02b25bbf5fd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "695dbcd0-7698-4b83-928d-b40a7b2b33cc": {"doc_hash": "159060bdb7899e1bc4838b033206f0ce97134fbe89071dbcd52e86a08794edd8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd": {"doc_hash": "ed5692434f0ffb7e67f68a3495b26ed0f0566d742d4fe95de6d54faa03720c4d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1aa8bf93-4bcf-4895-9c36-4f186903949f": {"doc_hash": "bac64d1d8622e47cf6ffb4a1619564d28493921c9160ea05f4be97870ad6a0d8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "403ce972-c506-48d0-8f0b-7ab4eb14daae": {"doc_hash": "0a1b249c332025b69d35d175d36be410803051cc0e8046724f0b6004a69d266d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "435e7b89-847f-4f05-8b16-0cad3200a604": {"doc_hash": "1e8c1f4c06a9f044ddb4805875740f4fcffb633c03dfe4bad81be58160398431", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "099615d1-37f0-435c-8582-005f9b081529": {"doc_hash": "ae795c86e99ef68bf5a100a2a880421ab675dd9e5d815bb5c3d99d7f69d94173", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913": {"doc_hash": "81bd734ac38653c695fabc00ace8c18cdd84a7beacd1c631dfa567fc79ef87ee", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dfffcb70-34aa-4ae4-a6a5-685eae048205": {"doc_hash": "073ada689fa25ef4dcc9bc2cef1857b281fdf949d9d8b7f084cc340ab0010d9d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ee2be34e-8be5-41b8-a7ba-93cd0d853104": {"doc_hash": "a4043511a18eb2c1da7f5e19f714c1b07486f3c20c83dc5d726bc5c7991de770", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5c17b870-b05b-4dc0-bb9f-5befcb47075f": {"doc_hash": "ed6bb649f1e24549559f46b6c9d3bf7617b4c44f63203a0410a329c550c5d043", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4b68c5cc-5fca-448c-96df-961092921b05": {"doc_hash": "05c0982187bff0a8e479c69d6a128507ac4e196802a8cc663920140020a6a290", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "46676818-433d-4dda-84e2-020463a3af95": {"doc_hash": "61e6a185feeb76f74eef74f6880f17af311596800eb7aaf3afbf44a1fc68cf81", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "30ed7bf9-0514-40fd-8b16-ce142d3a610e": {"doc_hash": "66bc05778c0ed1e8da51f9b105ca950a8cf7cf4779c987d3b5e323d17c77c370", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "39884f18-d161-41e0-bd57-93c788fc3788": {"doc_hash": "e92f54b5ad49e010aec5f4bcad7d7bd657ebaa679bdd97d47b06a156ca6dae86", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2e5ef68d-a7a4-4077-9c37-1649f46c629b": {"doc_hash": "0421499420776cda5dd547e99166473b54a726c7cb9d7969b6c4cbadf03d3f44", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bccef02b-9e01-4803-9211-0917a54f8a91": {"doc_hash": "0a8e6764c488d0d43a465edd0117975625b0e0f4a520635e46deec9751e71293", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "28ae4f16-7cce-424e-bc91-5102891a8aeb": {"doc_hash": "c9624ffe675e2cc6079b42449f7bfe9222b2c19d692c174952961123884f37c0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d7258df4-833f-412e-bdc2-024a826cba20": {"doc_hash": "345d14cb5788a9a531a5446e2306201b8a391cf497bb5059124c4cf994810e96", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "54f633a2-ea36-4a31-a64d-acbaea796743": {"doc_hash": "91a29da659d54ddba97459315fd72ddeb4ae651d1f1f90d2a11bc2e4100c5ac5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "983c1fe3-f347-48a0-ab56-42b5631800d3": {"doc_hash": "382827c1e46602b149497caf646c38ee9bb79a94c2438d8db98133283fbe6f3e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e9d90b45-2a94-4b72-a1b6-8c78c390c260": {"doc_hash": "84a5a229719201e2459083b8e5288a31910c763d605bbdaeb1c8f1930e18bb54", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cf3f631f-86ca-464f-8ca4-ba18ee455221": {"doc_hash": "6094d4a2de0dbe848186bfcf6bee3eae79494d1f2ae2bfa812e7973a1ddef7c3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9303bed2-daf6-46a7-af36-967b548f27f0": {"doc_hash": "5c3c1ed15abbd291b9ec7f89a2ddb73203c2494bc41e8c244990fb8b6cddbd73", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e2a8054c-f853-44db-8e14-f6078e68b90d": {"doc_hash": "44c6b203f40d1a0396191384f72b85041b07df52373fbd8e850421598456a361", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8a4a01bf-2da2-45c4-9308-a0105bb17ddb": {"doc_hash": "c817bc03649348ebd73d4eb8f544961e7a9c572ef97481de683ebece1cd1bdfe", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829": {"doc_hash": "506483ba66f2bc80530c382fd9e0b11461e2a35e61af8d527e0784f0ff6a052e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5b753f5f-0758-4880-992c-183d9bd368a5": {"doc_hash": "9a82671452f3960373e73bd19db3005b03d727b5b97923882f019439a675c4f2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2386b8b7-b35d-48d1-b683-bf4d97667f12": {"doc_hash": "026c093f5fe74301238db556ac73ce746258ad6c0b237eb30ed7176f4317b7c6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "807b3734-458d-44ff-a08c-26d72136d815": {"doc_hash": "8dd1c44a341b5483f2f452e973d9e111e4cb43f605e1ac2314aaa9f756be728b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "21c2eb3b-bceb-4a85-acc3-5545e6f8a985": {"doc_hash": "2dca60af81c67a3de74d77ea227cd5a6cefb843cd09f7074492f4e276023b283", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9799d198-6519-4412-a983-efff37170970": {"doc_hash": "5df6bf46a28edfa417460b1d75a72b78138a2f563430285db1f4005be90f3721", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17": {"doc_hash": "e68992fb907af5d31cc72b9a7ea798964e1c7f86b7246f37fb0d80c56add153e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076": {"doc_hash": "d9a8924c02f5e95eca7b76086b4308a51c92e585b2822e79fc19eacf564c399d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "40c989b7-a919-43d8-96bb-b8f3f4663938": {"doc_hash": "a1dff6dbaacaf1bd8d0191f20d4a702a9eac44dd52db5389c50cc6d57530f028", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84": {"doc_hash": "30e59884349d6c6856ed30ca370a18f4afb7376ff4c1e89d16760d16c06303dc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ddab7232-fd5b-474f-b865-d333d3ed0af5": {"doc_hash": "5d106ab366d08d3b0c5a16dad166ffc69a7e4acb3788ad884b1e479d3d06e787", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5c4f1ec5-b7fd-4412-9b82-ef84d2320779": {"doc_hash": "7863cf65400974cffd69702bd0e7f89c3263a2babf9013fc8198e7a0e27bba56", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "68511911-f243-49f7-9a48-79753276a56e": {"doc_hash": "ecf127715b34749a98c4003654ccdbd0546f781e6bba9ef82360fc0d0eff31f7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a11b71cc-ff01-4d32-97f8-27970c8a22e4": {"doc_hash": "516f9abe3d67ebb13a423090b35a02431687b328728fb171c3845bbae38c76b4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c77e3048-a464-477d-94b0-4a31e7c96a55": {"doc_hash": "eea46ca5a712c84b4fed76aacf980e4d7e6206247f9811c3bc68b960252e9e56", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "167a1ef0-9ed4-4cc8-a523-dfccf6e98066": {"doc_hash": "42557684170b4f4131ec723e9cab8092c44fcd7afe83d2cca372dc0b67c15824", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2085a59e-fb6b-46be-b090-7bf1c92ea15e": {"doc_hash": "86389a8485f947866ff59f4657fa96fb71a463aad103bf63956a7692f91942f2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "df9ff41f-0747-475a-8d43-7caae5e3b199": {"doc_hash": "0ac73f7d34799e99858cee5aefd7d7c0fe78c33c1ba3fd48c9c475dd9971be88", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "26094046-8c48-46b7-9ba4-9271dc0fef0c": {"doc_hash": "056f95195c42740ee31373af8e57eac29ca9c6fbecf2de5b81207a854413898f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a": {"doc_hash": "d0776f1eb4683f4a217d036bbe7e5ed3ae5294716f882a4c71f2cb3684d3b205", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "691d35b6-2000-4e81-ac07-6a1905056a7c": {"doc_hash": "db4ffaabb591338fa58b44e45663fd65b09bd2c1e69675115ba105ed2b788366", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4c3228df-0c16-4652-81e5-70005bbb573a": {"doc_hash": "72f1ea941174f4d407f77ec568f3e24bd5640efdea94b3e5a6cc83b3222552ca", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "86be2ce0-124b-413d-a8bd-6d3436f50424": {"doc_hash": "db2a2d9efad11c2cacfca048dd9c58828ce9439dad7812f3107b911aa5a6978b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae": {"doc_hash": "00d7cb2e7325199f1ac7bbed5898e73d26eaf1bc2ed1366c30ff1849052efdba", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "62dd54ac-704c-4ee0-a449-966779825fdd": {"doc_hash": "34fe3ef9bfbfcc64092567d25534ed4b42ee7b27c82e79ef846244c80a6a857d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "85479058-7981-4c8c-a0a4-987f8cb82697": {"doc_hash": "f4559753071ffd47346d93f67e8690bad3f5b443dd72857f1d3f19a2f272095e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e69f2877-a20a-4d40-9bf5-68e22ade50a7": {"doc_hash": "91b39ccec4ab01b4637784a4e481c5a1ccf3fa0cf15e474dce54da64b979c105", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c8e69be8-2bf0-4b01-8c62-71fda435f610": {"doc_hash": "7891c57f969d5d0ca4d6f01830aa36350e14caa7ba84bf45a49e2f519a28eb35", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120": {"doc_hash": "7307332290dc9d777029dab10c2a50cfb484d36aa090406d08f9c49af2cf81b5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d85dd27c-afa6-472f-93a6-0dd68a5f93a3": {"doc_hash": "3243ba0ae8b591d8b335f915a277e4186ece32a21e4208b3ffa5247169a0d9f9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "64e6f783-13db-4080-92b8-1686925a45e5": {"doc_hash": "1818b3f6cc4d8868f0101d79193485354efe3d1846327266bd58fb5cd19d22f8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9": {"doc_hash": "bafc568b7d184f804c48d9dd0790cd333cc37c7430f10c3784addebdcca0478e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "04955313-5e98-4dac-ac7e-8be543354f66": {"doc_hash": "46c6ca855d86a7bf77732033071d3813f0f35614ba26d82754ce089640f52d79", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f": {"doc_hash": "24ce77abf9301585059ea3db3a635d6225b3322326d50d89a0e7587d1fa19ad5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fd7f50aa-239d-4bed-8ebb-2e891e2f7699": {"doc_hash": "dbdcf45c96e0d821d55eee467dc5ff819b9678ec780a8e12965f59d0b3c13841", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a3456c28-65fd-43a7-bb74-95c655782de6": {"doc_hash": "5ee115d825afed3c8a817249dac03be622ca3c61ad7dc79bb302ea87ced79ccf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4e7518a7-74db-47af-a5a7-88798742d86b": {"doc_hash": "0878b7220b436038c389b98d5e3ec376aa9400ccd2a5558114d6893932656049", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "32a4bd78-ce21-4557-bd0e-3820c4bf251f": {"doc_hash": "700c0d709d90c23c320054d6ef531872d7a29937ec48657c97c1bc0dc710c98e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5cc8e828-cd04-4209-ba93-02a1493f6031": {"doc_hash": "a282f67b621792414d6006cf9daa76bee0a00523ee74f647229871e0222442f2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6c0c627e-ff85-47fb-82aa-9b8137d37c2a": {"doc_hash": "13798c7b3efff5c35790194481d6e81821248f0a8a40c63df308f8685708a25a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c6644d37-8b95-468b-b51c-05dca239d21b": {"doc_hash": "76dc7431dc49a692e98ece576097df06e26319e4a13845da0cbf152357c7558e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c64b9672-2727-499b-ba52-390a5cf914a4": {"doc_hash": "9431f54ac246edb50ca6e6ef7d9c4b05c2dd6828364a39eb3fab6ac28d056038", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "91d7b649-b55d-4ad0-b106-e58cf62417dc": {"doc_hash": "2169718964a763b0c77d1300e1ed0a4c30083e529a5927190946ebe36464994e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3bc7111c-8802-4ece-b0a5-039dd241f8f9": {"doc_hash": "31319b5e421e93cc221186a9c02f479056d170443b687f52515a8114193ab9de", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4173dc84-ff0d-43f6-a4b3-d70560f276e5": {"doc_hash": "f03585054045614303ec19e9f5bc69ceaaa7602d6c6fc4a61912cd945d817cc3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "589571bb-4c5b-45c2-bad7-aec9148a56ad": {"doc_hash": "0b5b8a37f5c6c59dea580931a110607469ef4665df0098701e594f51e851aaf5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd": {"doc_hash": "d37db395d9547506682330e3736636ee4f239fd61c9610b4d12f611185ade1c1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fca65d61-c2a9-4c40-8a5c-ba79928e4f93": {"doc_hash": "32c6ff59608fa0efcab9e1f8125c27ce21cb07cec2d2eef2499ed941f21c4fb1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4bfa5aa8-9161-4839-9fa4-3576431c7366": {"doc_hash": "dbf42e5ae15cca411e2092bb9cfec74e0a256bfc86460ac0f4ffb8468ca2e850", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8530befd-42df-404d-95fa-a53deda26645": {"doc_hash": "ac9f39f4a1d476857be1a7490ce78828825412202bff6ae4cf1c5cef5ee6fa4c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "be2c5a17-8d41-4779-b0fb-8e5347df6094": {"doc_hash": "951ea19c4ceb7d7c0b12dbd29e7db4da8a6a5e3b75e9833cfb79b068b9b3cf43", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b46d2d11-6158-428e-b752-95fd6dc22e75": {"doc_hash": "ba047c24f4a31949a8ff28eef8489dc6aa2d3d3e80a11c2f536be6f9db02dfda", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3889a560-080a-4373-bcb8-e2e7eb583a60": {"doc_hash": "e9ee62a54b1f9175344637369919f0b30e208e151df925e5fa2be794fd0be048", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "08209b0f-2b60-446a-bead-b92bb5cc7dba": {"doc_hash": "109177f31092038538de1e16de9fc8615691a55f21b07c0eb7d90d111802d917", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0257d759-b104-4f4e-a2c4-4ce6d280b37d": {"doc_hash": "fc910d261f38039b96cad43d977928ef4548729108ed83aaa37d3e65442f54a6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d21b5741-3f9a-48e2-8e90-714b3be77f1f": {"doc_hash": "664e87b728adc0192fbb6b8668301dc4b0d012739fce645291816911affe176e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c7dcbfea-d2ff-46b4-bf27-c28c877383e0": {"doc_hash": "5b2f7f004855bf7160eda28524f8aca255a2ee4fa9b0fa0520fc426c3b850cc1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "204c05cb-54d6-4537-a277-abe74093ca04": {"doc_hash": "e0efa08ed99de2f8c3271f223c80c34f3934792bdf3833602952f6f84f834522", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109": {"doc_hash": "95f29480e8776156569f1dd7a1ab65e1a7484fe605f2bf64516a16516d5fe991", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9d19bacc-0a1b-4583-82d4-d3296c92f966": {"doc_hash": "beea223f937afb8ad7cae98e83dbee129b3827a117e90a172f0b58fdddf7dcb8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e4826f2d-db11-476d-8a71-cb5a201400b9": {"doc_hash": "ff2dbdf0d06724dab2351d23b83bf0e53ce00b936fe198241619f623fb21a1c3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "16694feb-cd8d-4385-ba9b-139f317924ce": {"doc_hash": "34ce1809f1d2ead5dcb4efa178c2214b8865e0987a60b1d0acac4c59d30ae98c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1044b065-3193-47d5-85e8-3bc604b0912d": {"doc_hash": "2c02609097e8cf0e014549d99a9cdb5f4c0a0eeb8849f3d443453d1395db542a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3143592d-66cc-413e-8863-96948df4d304": {"doc_hash": "f3607f08f9aba5dbf5e525900526a987624a403eebcbaabd98a1ab8b4fa359ad", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a": {"doc_hash": "b003781d3b12e3ca59ade0fe5b5be1523173f55e024abb524cb4ca9b51b500d5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536": {"doc_hash": "c581103fc0f57ffd57c08e93e89b39a17bc94f2fbf026fe19bb2490eef3d52a2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403": {"doc_hash": "8609a5008c8fcb25a55bd0b27ea95aa9d976873b90495da86d09fd70c73c0d96", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "84275586-19f7-4104-953b-b90e13b18534": {"doc_hash": "ad4da5868e2c98cbc8626e03068c93dfb727cc92d25b1a96239ea978c30117c2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e1a3cb02-42df-474d-897f-d333d0e9c6c3": {"doc_hash": "3466bfbf29e50394783e351812aac043430c49de53b2687cd6b4886027a0c554", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "aff1d461-2af5-4425-ba68-ac4064a4c753": {"doc_hash": "d56462918c1bc20d2bd235f639f8bf9694b10bd2541d99fa58739977db1b7fde", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7": {"doc_hash": "1c9ce7fcde6c354c216a2a6b78f4e7ab884062db98bc836d3e98aa5c9b769b14", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "113fbf70-a4fb-4251-b7a8-1ad3a836d069": {"doc_hash": "f4cacbe6308f7084385cf6c4a7c3fee56b1a048d7df8a9ca577f42e0540e625a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "776336c2-5f9b-481e-b1be-d9d8ba4222cf": {"doc_hash": "36a7afe0398ff464371ea1878033c77e5e1beb8a06408273664fd20eaa26e603", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dccf6ee4-ac1c-40ce-b989-bcb736570a77": {"doc_hash": "6ccb73f22f9390f6296d198ed12b8a45166219210fc3362f90542c98f1b0b05f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0efd2111-1824-46e8-95fa-3e20e1d70250": {"doc_hash": "61df5048eb7fd0fd2f02164dc7791f61df5aa30f79b6f2550e74544c294cf2cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977": {"doc_hash": "cceaaa7819e7101c241571dd97039f36dfc9e49ffd1a22a336a059c8c9f3988c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "40a81e57-e9c3-4560-b0bf-a6f10222a132": {"doc_hash": "f21a51adc4795c58c4783250525e3bb02dc75a69af0b71fc4c71ef6606b44f71", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "706a5828-df7e-476c-987f-670acd804d7e": {"doc_hash": "22905e48a0e4857d895ac56f554a44649923fa8ef9e9652e8acf9deb13919aef", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1821963a-b804-4d05-b64c-be8ba740d1ed": {"doc_hash": "e1d48f4ed4b52c7e0cbb5bd78f81ab5c2f2a90a382cdcec8eaee82f9315871bd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cfaadb94-b123-4d12-97db-974bcc660886": {"doc_hash": "6dc1f361fcfa5c31a56f21edcd07154d6bcb8d0b96b585dcaa932631e482269f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "496520a6-fe70-4fd8-8976-b29afd3d09c4": {"doc_hash": "0f422971f2ea5f31d34524d7e55296804862aacad8b3779e66ab31bb1eed361f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "37563649-99cb-41a7-aabd-3cc56c744fc4": {"doc_hash": "768b0c1c470f40cfffd0af21bb7bb6d286c1d06917bc3fe2135aefdf78d569b6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e9187751-7cf6-4291-9668-99baabcb567c": {"doc_hash": "c470ec7851adf10c24ba76fe7732e1c3b18d166667754ac430eeccd7a684f88c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c22e25a4-be23-467a-866d-aa26ace2571c": {"doc_hash": "3ebd912bbd4e56cd519625b2a11ba23a2707d023df973c25393e40ac054dc020", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3243c420-1d1f-49df-b4e6-fbcb6db504c5": {"doc_hash": "ebeaf48069c4dbbde78b79a16da27e92d1590ee987720fff9f8713f9a483c916", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ba263390-366f-4e75-917f-fd6004ef7724": {"doc_hash": "57c2788c7ea55cea13dbcd9ae92fda35b99ff295826075f7783e152d8acc344d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c9efbc85-e15b-4451-aaba-5b629f5c340b": {"doc_hash": "2b632c761cc999e6a5c37289d460da7961335d83c15094cb04e24a66a010364d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ff507ead-b1a4-4569-8992-e94ede22decb": {"doc_hash": "f184222268de2648d0b50bd8e6f6460c9a663faa8750a5cef44f72288876b71d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "48d21dad-6d4b-4448-b6d9-6426a7535ce6": {"doc_hash": "21d2b9272a4d67901d7a93fa6a52ee088507ae995f9a7d7b3fb79b1fa628f900", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f": {"doc_hash": "d72d876e508e912593d4502279a9f9cbf1aa557330844e8fbcf8f92670ab4b9f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "70ccc0a2-3007-47b6-9917-0b0a3c5397f8": {"doc_hash": "6b0bbcb7fe9fc34a57f9081eae9d714dcf8b1f5ac260aa60cfebee786e1584e0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e59321b5-64ce-4cd2-8554-da9157d59aa5": {"doc_hash": "3efd28553506f90169ba8cd1630aa88e72521cb8c7b3864e4f651725e1ea6dd3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ad658f22-79a2-4a69-9c2d-aaccdd525120": {"doc_hash": "ef4406eebcb0df52a430af1708f3780dbe466e9c9a9af2a326af4b44ba51380e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "71277d93-6365-40c7-bda9-5e28d6de9d03": {"doc_hash": "7221fd8d7109378243c22d9a2161f74a5fea4de631e53f282081bcc65dc500b4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f488999e-c3c3-4817-b2aa-5cd7ae5854d3": {"doc_hash": "210d142d32d6df3ee2fb102e5b9b2dae9775239cca2f35606cbd0d0f793ece5f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da": {"doc_hash": "309ae2e66d73feb243b39cc9c1b8b5c208e37e6338a8649af76c05cd73d389f8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "41de8e14-b71e-4776-8fa1-46d827c36c27": {"doc_hash": "6ab847af0ee2bf21b56a1f86519c1068e101be0e76b2445a58da4fab8fec306f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9b602795-25d9-4792-93ed-eb9598d2226b": {"doc_hash": "b47cdd7a96319637ffff00a3b586c77766650a1de9e91d35db023bb65126cd26", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6": {"doc_hash": "b4880f9ac0bba1b457bb3a4f6332caf266d7f74be7f98b2bcee8bae0ae63f2ff", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6513f047-6a0e-45cc-aa50-aa6de5575b12": {"doc_hash": "9d5e736967d62a1b08bb8ede5dff07fc527c40e0a589f341ba2c3017a6f2a99e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1ff53ebc-4cf2-405f-9567-bdbefb7593cc": {"doc_hash": "bfec59d2b9df57fb6aef2e6ab4ffa8f3ae8a71fcc3dd2ced44d9e2cc8716efc7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "231f8202-66da-42ad-b657-843c826b99e2": {"doc_hash": "52055d982543e18c9a95c0f7157c82ea23e55c47aaa89d54abde05e4f8be16cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "90799028-d669-4363-a648-78b55b29d8bf": {"doc_hash": "c4cf00939023e93d087ea55cf3ce1ae0a2e703fddbcdb53c7ae1c50616c23472", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "902ec6b2-376e-4c52-adcd-f2333c2cc3d0": {"doc_hash": "42e3a20ac343b71e50ce6a1f983f4cb9b1be8aab445d0d0d9f0d91064468e5cb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3ab02bc8-feec-4598-ba88-eea311e8f611": {"doc_hash": "53defd76fd86e8d1eaca8a8e55fa1bfaf71e43998ba9544154d05e7d64648c36", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "69be9a93-8601-459a-8edd-e6a1e32c64c8": {"doc_hash": "39ff1c6a9935aa46ac3012d8a49702ac77f0d298710b9e1f91e7e6ae31f23c64", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dc6106f0-3725-4577-93a3-340b050513dd": {"doc_hash": "2f520db2bc635ad94cd28ef644af31a67d382560b5c286f835789465f58b17f2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8be10f36-de61-469b-996c-091c339025d5": {"doc_hash": "fd443aa738e46855919f79d99adc8ab147d9178f3961d88fad27f3552e5adfa1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "eb1f9fc6-8635-43b1-b74c-13af19cd13e2": {"doc_hash": "422f79218c72a51d2a7ec5bf503e672de1e008fae02ca69374ef5774487bf589", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6800bda3-1851-4c38-89ae-6fe1689386de": {"doc_hash": "679840a611b81a254006059b8a89d619bfa08196215c173c8bb7e55885cc1722", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dd56ff93-dc0b-4f49-8a36-6736740e4025": {"doc_hash": "aa333dac89aef4be9f3b663b642ef809cb927128bc4f02351199e8f5a2ce2e49", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a0aebecb-ab78-4711-ac52-2b5630666c9f": {"doc_hash": "b08d8cc5a2937f37517a7439fb7500d82f4047c283e94cb91f2a37a718c50030", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b323cdcc-4d57-498c-82c6-e50f3a92fea0": {"doc_hash": "c9129a7fe28b8cf93468bbc9956ff3e2a48071c1018672a89af14633f64bc9df", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ec9dd294-7f95-489e-8524-054b012640cd": {"doc_hash": "2a38ec6f035a68a13d580aca4b2d1354f0acf50d4b229122f1bcb939694d1c90", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "88df90ff-1ed7-4b48-a970-0b6f8a6dd027": {"doc_hash": "d1c65dd70ada6e140efcbfc0d155d235a0b1d3e70a0f2dd348c1bbb714fe8afb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7c4cf198-8c52-47a3-9011-a82b9d8db5a9": {"doc_hash": "81b0e251bcb88a53507505df8b56aa216e47cb8afd7a2909c1cb77228a587a44", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dd2c935b-3880-44f9-84ea-49280825005f": {"doc_hash": "7661a5781961cdd713ea2859ce5d618d3915e37a0383f9e232775dd843284a51", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "539f36a1-045e-4dc8-976d-af4d371158a6": {"doc_hash": "541e6ba0b72701f969fb4e12500047cbcb906aac1d90bca3e0ec260848953eda", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5d19ecea-21d6-4fec-9603-a2051bdb8078": {"doc_hash": "d87411f145cbefa400898c185a93f931fba34854c76fbc28f66e84738de75508", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed": {"doc_hash": "b3e60720461c8fa5efdd72bacf0c93364e3513877e57e64bc6bcfa268be2b0d8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "eff11646-636b-4f05-a193-ca69282c17ef": {"doc_hash": "5f0dbb800b0a8e4b07f2b56f52f9d58a6cc4141a81c5d3bc2c1f9bbda84409eb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ea148f05-65b9-412c-97ff-da12acd35136": {"doc_hash": "3984e9beae567a6abfda1d4842033d0051ed30740a1d003e06c5c5b04a725f24", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8777d75b-6951-453a-8947-8fcff76542bb": {"doc_hash": "4238f4b3ef751f9815109e2b78d4bb9318411f7c7d0e259bd88ce25d0c83f53a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6a5a8151-c060-41a6-a81f-438158a969cd": {"doc_hash": "9b2287e3370530964c78d6854ec956f16cc2a75ee0fb2db67f9e4b120c4cfbec", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8f09da6d-5228-4569-9cf4-e0466584b90e": {"doc_hash": "71f54726046bd555ae6707b9c6222ef50d0e74858cdb0a26159af20bcda7b7cc", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f70dde24-41af-487b-aa84-f3e731a78bd6": {"doc_hash": "42bbe4d4edadcecc227c3ddd399235b257ccc893a2b5f592239135b0426ffc97", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5": {"doc_hash": "79f58ab16a74f1218232f9b4910c3d87700a1b4765979e36cab672cfbf343f69", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "35d17a88-58e9-4ca6-b300-752255957f9f": {"doc_hash": "153a11418d6be5f3a5b633e5f6e689663590f4f718c6eaafc8c3e96045b4769c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2e63f8e2-d303-48b0-bfca-38a585648486": {"doc_hash": "2df677306ca79182f688c6b6f06b582efb43da07b3d830d92ec8b1f471083dc1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cd3bd027-5004-4674-be09-1c9a43db5dac": {"doc_hash": "8cec2cf72b531035e7c6177b3320365c878ec3d9bc5b6e50aaca5bfe28168847", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ca121c4e-3189-4cb6-bd61-1a6333682543": {"doc_hash": "bc63f9d7caef605ffba33ac19c7d9bbad575f75dee4ee2854ad461fa2b8da7f7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "54e1a5c8-4834-4780-9a19-88880306b6f9": {"doc_hash": "ac7021cca2790064c6ad7a068f44a391ed96f036369323ec638b5719e6781104", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7": {"doc_hash": "a39f79c28078a946124294eaf0b1e3afd162fd3bb90e5e90a546ab4e661c515c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b7f2ee58-0602-49fd-8efd-e75054637752": {"doc_hash": "f79ef63320a4ef6e65e9b7e39b7d74a84657ece1e1aa7da7f251f76a03db15bb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415": {"doc_hash": "d114f4cf8a649c6ca368e0040657063cb3c0c3db1c714f47321ff56464f152ea", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58": {"doc_hash": "d0b53646bc067c3b8b18dde4b923f0c20280e8b576542fba29bcc1f4f58d35a3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd": {"doc_hash": "eb250a3c3e6ff956a13cb36a5e6063f81ee704060f7b85cc3c503cd3e51c444b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8b57fb07-0350-4055-a7fa-240752a182d6": {"doc_hash": "1a6a87fd8f16a6e6614da06e75f9de04ef9e1d57b116cd7f141a5a14ce953500", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "16f9e688-d85a-4691-96f1-5d1b875e76b4": {"doc_hash": "d35ec4150cf7fb4210507f6a25899974d87a52fefc634bcbb8ec951e9b9c6245", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4ecd8fee-c08e-4d27-8246-aad6f761d987": {"doc_hash": "f71e1cfa7294d842a11919d23ad5a12caa5178444d1ada1a6059ac6966a8722c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "83a8bf1c-3073-45ec-8558-8ffd210cd308": {"doc_hash": "caafbfc2afe5cb3bf7a8feb5b44243c06a43100b72df1f8ef0e07a75de61d28a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14": {"doc_hash": "9f13b5d508b0514c12f55ce70d7b1e239a209dd716062afe8047da3dd391e293", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ccaa9034-6d31-4c57-9751-64c8355f1f01": {"doc_hash": "42b18287de868aef53f62e455fcc49fda2d4506cff4838f61ba122141541e188", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad": {"doc_hash": "555298e4330cec1b7f5873d02f3e5831381b944e1dfbf6413eebe1f38e7056af", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "61fdc6f7-900d-49ea-83f6-c19c89253afa": {"doc_hash": "639a5208a62abf6fbf23c34875107c427a30b9e078968b1ee15362d5767d2d90", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9d556006-0617-495c-b540-7fb518542a6b": {"doc_hash": "0a79be61354f3f3835c8523ef76bb9b5eba82a60e23dc984e4776d1d808f1c8f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6": {"doc_hash": "824094064465d0689664bae333a4f415bde8fca4eb7c0d78d4b2a3101b90e01f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "60fc0fb3-cc6b-40dc-b835-f62bdd50414b": {"doc_hash": "ee406ad4c630c33aa8f4ccc9d45fe0ec6315818234243a944087d2e3d6e0d195", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c794b653-5beb-423d-b311-fd822b2117e9": {"doc_hash": "d77a96776245e30a4ddaddc9f302206b4e01b09b513030e3c951c58fd1ad6fa4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "165f9ba9-22f6-4923-8d69-29e47af3a02b": {"doc_hash": "5c56904e543cfe3dd4e28c9b061aec9a7e2117f21c8d89197dd6743a426b6e9e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5965ad36-a7d6-460c-b76f-621eb3f6cf67": {"doc_hash": "de2320e00045ec130fb40fb3e230a68aa740a2af9c849f6eff62a385d1c5e19c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fd075574-8afe-417e-bb55-a7d0701dc1af": {"doc_hash": "8d73dca3cce14af4d8502d4067e1dc12c9c33de6219a5fb8a8178e8852c0960f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "db35fb43-d82a-43f6-9b8e-b40dfec5b99a": {"doc_hash": "69005b66d367748ee5e1c83dac7090e6c4b8921f6f589b8381aae1ef936d1124", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "27a48193-8aca-459c-8ba1-b3bc484eae80": {"doc_hash": "16c86e0fc9bc70fe8e31579a370470ea1a891483176b02b00268385141be92c9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "960464e9-fc8a-4bd3-bb58-e784d1e302af": {"doc_hash": "851a2a1b84d220b0cdefcb6fe0d5e81f644d32657999d56f0411f3586b99f703", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fa12d595-377e-4bbb-aa58-f2cab7e8a57f": {"doc_hash": "5af2933b157c713f5769a7b610c3447d2e55bda373f10fb08410e5bb1535c581", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bc6c5046-b198-454f-9620-a9a5d74f9fa2": {"doc_hash": "ff0f37005c0f4a19b04cd2439be5859bf0e686d37d0385f0c4c9300f4c2458e0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5910b5be-4267-4ea2-9135-65cb147c7c39": {"doc_hash": "affcf45c1565c27adfd64175bda55694401f926d7f578f7d32338ea5055b63a5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373": {"doc_hash": "d61bdd5b72e0d6566305513333ecba4e02486ea2f2f70a05376b13d853bb96d6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cefbdf0f-869e-426e-ba24-39be83615381": {"doc_hash": "a361947d14392b5c0320975fb6019b16bc34182b941b3c5d9362b0900b92b135", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "266183ce-a17a-448d-8aa1-7bffe8d254d5": {"doc_hash": "f979461ab29794c4f8659d99c1cb1695686e376345c1190fe1c9b1fe4aec76be", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "63e2ec22-3381-49f0-ac9e-fb540705c9fd": {"doc_hash": "ea76d49acaccea85624724fe44a2c1e78ffbfa1d8dc5f5a6e702c99458fd1cd3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "05acb078-a0cf-46da-8ce1-4f58448535d2": {"doc_hash": "2d72abe7d00962c26688197e47239063b0e81317a7538547ffccfff8685a378d", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "92536d3a-746f-4ccd-8190-9b4a6dd95b18": {"doc_hash": "ffb7b7bd38664a130f319471726c32f2832f6a56aa3a8a8ce29efd57f6c20891", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "443dfd09-54ff-47da-82ae-501a641369a3": {"doc_hash": "28b9e5bfd04b53401f9c4d6cc6898680799e87b3cfa89b0a80b627550a1f699a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a5d255df-d61d-4ff0-b190-6f4ffc52e94b": {"doc_hash": "f9859384758b5d3a223887c4b9186905559c484729ea1aad1103d7f5a50d1d05", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a": {"doc_hash": "43c49a03ac84919bb877b2577fce6a5d2e751c9dfda2748e566d7197b957aeae", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "65faffbf-4c40-469a-8216-834a9ff4041d": {"doc_hash": "d12a355a4e3b21ee8c0e83cc0e48404973f48a7f1b610a31194a8b95941f78ed", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "37c938d0-d3d3-42ed-9a20-73fd187a1c59": {"doc_hash": "ba5f64bfbe78760407770adbbdfb3e7aa8a2c673a17467f5bfa02fd3599c58a8", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ffd9d661-fd05-4848-b224-57e2a42d678c": {"doc_hash": "1d5cbe3533561637ce38a22f9f9360503d3fca61784367ba7129db1f42732096", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "693f011b-ec1e-4d38-86fc-99572ab4b862": {"doc_hash": "c22f3249b9aa858823db5b80a8834a2211e48beae1a99de8fbe95377e6dc5dcd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b5751895-b3d1-4cff-b0e1-c135cf785f19": {"doc_hash": "a595b50bc70546356274a45f908210dfe32704ba0627dea58ed713a0fd6bec35", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f0d2b075-7766-4445-bb88-256551ecadac": {"doc_hash": "4f74d218f7d904b3b6af1da0acc0aac9144bd6a60a8404ce3be8e4d9dcb08b83", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "00a5bf42-d5a3-45b6-b151-87159f937a11": {"doc_hash": "64a98af1dbda5f49774610fd9f9a6e41136750c6e9b3c3354e84b45ec0c19f18", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8c230598-c4da-4bc7-9300-45b148ac5377": {"doc_hash": "319621d005208d7649725469e0582eee0be37a1eb121ea788a94e690991b5b05", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7": {"doc_hash": "7635f24d4912042dbf0bd976cd42e25ccdf9bbdb8814fe677f399ae496f6b937", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a7cb94a1-7139-4540-9335-22009c7d151e": {"doc_hash": "b9da684ed7f26e831ae6c6d445f28769c7d8bfa58e511f1fdd8fcc2e5a85b4bf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cd813494-2c4f-492c-b0b6-5ce159aacebf": {"doc_hash": "8d355d6c28d84fb00e253074d921f932ab5195f25436eca0a7737e9ce7d62e77", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dfc1edf9-64d3-4107-9238-076f724b3f42": {"doc_hash": "691a6daed7e8b499a872aea6d34c4eb4ba7a9b136484b4b24661f10842ab8b05", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "bb08a789-9991-4a62-8535-d8c2d119ff7d": {"doc_hash": "cf217e127c7864f87c003f5e70dcfb4c734a795eca88844b7f125640bbc10ad1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1d8ea624-c29e-4971-9cf5-bc20283668e0": {"doc_hash": "6f4a8054cf3e845190f51d3197255fe8bab0ae10b0ebe5e94957264b11f38ec7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "787f9c30-5634-4c73-9919-79abe8828d15": {"doc_hash": "df7dd3a52a3db0239fb78aaed00b0353b7408a79216511d9b20129fa0a34117e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c35a4f8d-15d6-41e0-b4c8-a94810415cc2": {"doc_hash": "dcf8f9ce192413438f9a5273068c4b4d62e7274c56c795b7d8d3522c340cf678", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "eff9877d-a2a3-4a14-991f-0b71c20fe109": {"doc_hash": "27b5a14cf497a56f535a54e9bbca0fdc9ac2c05472cb319b3a4f70f1ca449196", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a1674922-8c3e-44af-a3f2-e44025eb77a5": {"doc_hash": "287d5d3143b9d643c88ebd86dd10c1c4e1cfd819d19d0ae99f052f9d1e52c47a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "70e3c771-f466-4eff-980a-cfce511b758c": {"doc_hash": "6fb161dea1057eed96e706f858b0b553aad4f188c4654681a53313e5219fc4a9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5bf3a187-af73-40c4-bb1a-0930404fc821": {"doc_hash": "60f2668b15ce857862e11d0ff90b367002dcecdff3cd54ff8bc7f9b5247b5abb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4": {"doc_hash": "bbc127d93381ab5f51945f8f3b3ea446b619ea59203c163f346bd9068c369fa0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "476a25de-34d5-4641-8e65-40eb694c34fc": {"doc_hash": "dcaef618e95376709ff6d1f093aa3fc0be0859ce782965f29d1db15b9904aafb", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ff28540a-e68b-4306-bad2-7b96b0a9d3e2": {"doc_hash": "b93f793ea9edeb2a717bab4200ea93d00a80a14732f4a880bcc215989df298fa", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f23ca4c5-064f-4d32-85a2-d27d994bd92f": {"doc_hash": "9d5e6d437c94131fbd9956c78a4c55a11f6325161e90c018a6ae9a8418bea1c4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2": {"doc_hash": "d94f484fe99ea32b20b1dc983f0ae3b198d4a822d813e01088e6cd10e6526444", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a5640bde-89b8-41e4-a8c2-042b4e6180a6": {"doc_hash": "fcb1f9c59bde3cd46490afe5d8475c5e7d8b8b3f20321bc8f81c4cb60bf2f9d6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "73f10ddd-c20e-4e55-a685-1570d3e37c0e": {"doc_hash": "52945598259cb49a685c5150f5a55f906e38f3079c8e94bb8dcb7e9b07be03de", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a410a23a-b234-4c96-92b5-15b8a2b73cbe": {"doc_hash": "95237c79ec5762ffdf8cd2d7c8764f34fc0772628048c7913e4402dfd4dd6b9a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "79770ee6-0798-4906-922d-180f21e449a8": {"doc_hash": "b98d5abca784819b8675dfa8cc9b81eff67a32c37aa87b460caf99e692ee8a26", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b": {"doc_hash": "7588ee301f08cbb9a671fa19d41bae87432037f2773373e1e0ab1c343eafc274", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2": {"doc_hash": "a37a2204672709adfe615136e1d90d888f7c891d1d27bdb14b8306431a90baaa", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "de2ad7c2-f46f-400d-bbb5-75104477c545": {"doc_hash": "10e6d2821a70e7fe6f82943084b3c59c91c7540a92c80ce24cde82e1c018428f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe": {"doc_hash": "1363e22399ff7e0755e7dc40f06f12f65f9a2a3b9ce4d759c1ae381249d6b087", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "faf06094-75cc-43cd-a3b7-48709a602788": {"doc_hash": "c52189c660741a36220da56f62ee5df411830a7f8aa801343506be74ce417122", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0c6e5bf7-5917-4245-96f5-69674b123640": {"doc_hash": "8a34cf361fb26a5f489d168778212d8043eca842001c3859898e192c81a79041", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "1371a77d-5622-4ba8-abdd-ccf4227cf76f": {"doc_hash": "1a2eeb501c96340afa9f22a678e0c5900c7bbf7e6ae1c3752b7d56c861c5f8f7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f126aed6-3569-4d33-8dcf-e136f9e77880": {"doc_hash": "71384e7422e848b0fc816ffdd6a1459a7c316cc7a79a72231dd66ef4b2873aa2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a71b0b0f-490e-4dce-b09f-3b0e403f73a5": {"doc_hash": "724ae1762db989e3f0828aa0d8fdbd42e74b8611b8a4d409f78a5053f792d5e9", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "62857948-04f1-4c60-86a7-f43a859465ef": {"doc_hash": "2a032a165e94601afa19181429d14fcfde81b0606177b95fafe1422e9a198838", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7": {"doc_hash": "0f59384d12170b8f2a991fca53a0ec85376e2b7290b5b5a326e7ac49233f3e44", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a2aca2bf-0501-48c3-b8c3-897167c19b36": {"doc_hash": "e3daee0d452112949df82f9c70a1824d11856201bce9611fafde461bd599197b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "411ab0f0-1d4d-4c98-ab72-fabcef265118": {"doc_hash": "afd767da5026485dec45a63edb69f18b53e08afe85636f857145c8905b2d9e30", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ff5efbe0-f2e2-4c85-ba00-048af7280fd1": {"doc_hash": "0bb625ace8f9252feb0f70a330b3bf034d2f283f060cb52e6c5432506aceba90", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b680c56a-403d-4cb0-8ac0-2caa70e22897": {"doc_hash": "b044072a5c21f961cdc4e25377a2c960c6e6d94efbbd5bd14a0b987726d44e74", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9": {"doc_hash": "9db9bd22ab409ff4e783819f2ca2eaab996211adacfa991b15f5507272e8b7c5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "b46f3a14-c8e1-4b2a-833a-efc217e15506": {"doc_hash": "620f2758487badbc6a47e9b0454884f10834c5cb3fac5eede82d903c4e4a8e22", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3d1ae5c1-fb8e-488f-baf5-06c71192c66f": {"doc_hash": "22d01955b560fcfbaf9e1c5abe8031b56a4cb05ee6a048d205e1afca45b9baaf", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "82e7fdcc-3872-4afe-a876-c0f6a4729dc1": {"doc_hash": "cf1a24f85694d8da749581451e38cad662bfaf754018a9898742f16d8f80fb05", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "13f1038b-c82c-4105-bc97-ccce611ef899": {"doc_hash": "15440d033fe92f54344b13af261a0d12cbafeb81fbd2ff2cb42ef0086a9fc0aa", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fb53b8c4-f434-457b-bb22-b65ff52d837a": {"doc_hash": "bc67ec1f0482b12863154d5b06a0ac17b5731ae5846c885050624d37cfbbf6f4", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "531880ca-8e66-4f4f-8783-fed0a61b6ca0": {"doc_hash": "95db3f7870cad3e2e32c7ded212f5076b2bd78f5204022c278032afacec0a077", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a856c980-97cd-4458-b0f5-a774f6f540b1": {"doc_hash": "4f5fac4500066a88a6700eb76c729f9e22aa1de3b5a8ef4ae29c505e2b195db3", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3be5db69-892a-4f30-8f8f-579119d8d870": {"doc_hash": "e4f5fe6abde113c07d7bcbd7863b6cd418fa90b2d44313bd7a0ba22dbf8e96ae", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433": {"doc_hash": "46e0061d60bc72e2734f766437eadab1843650e295c76c84338e0ed7f21b0870", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07": {"doc_hash": "5a183d2a7dfa09a1964236ba58b559ed59c87c08f2aa24382cc2f19648510ab7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "3c8979b2-5470-4054-a04c-e08b4efd6381": {"doc_hash": "809c8c54bc300cd14dd6d89b84e48114c81f615381e3491628ffafd321b33435", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "77fea69e-7952-4016-97dd-23e453e6af4e": {"doc_hash": "6e71ed3c340f239cc9f88802285d11e260a5410e122bf3f72fe7e7a1096b53a6", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a775a992-453d-4e07-9f58-27ebd8679bbe": {"doc_hash": "d93b8578a6a704f852fcb3a67b965d14df59ece290d18b61b862911d61221e5f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cb25a683-4134-4aaa-9806-24d34b346ba4": {"doc_hash": "ec0643a327123f7fbb0e6d1dca86b1a031ddb05cf76ec81774fe2705cc59a196", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f65a7f6f-d547-450f-8a8a-de48de825c76": {"doc_hash": "a3ae5c40b779faf66922b80c5447da5d1083d4aa7c3cfd37ae5a1c856a0be190", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c5d44fab-1ec8-45d1-9128-c872b56aa921": {"doc_hash": "78c0d46c28f54794266b1c624c93fe0846a709eab5b6a298a4c198df42eee9c7", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e4923b01-f539-49f6-8039-d5a2e73374a3": {"doc_hash": "8ca25913a4f3614221ea00ab01fe94ce3904bf26dbc3ac0b7c0192002e4a07d1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "06c27224-881f-440b-8d24-2eedc771cf3b": {"doc_hash": "9e41cd3f9c0989415906f2b04af48b96e5638a1aa2f6c80a70afff40d4fdd880", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "416f1092-cdd6-4a53-bf45-f08ef0b4df4d": {"doc_hash": "288826590b75d44265ac85dccd0b3a07582ee07fee763324c62ad5fbbc61e22a", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "7f8a60a1-7373-4c1b-8966-a95fb158f23f": {"doc_hash": "1e68194cc338d8c5ea50b330bc10384a70ebc48d266ed0dc4bafa1be1e06b008", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "dc8d754f-e798-4605-9c37-5e3978383ca9": {"doc_hash": "7e738ae5ff8855a44c0a6408d715375af4d49a65dca61d5b7c887fd91ceb2a2e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae": {"doc_hash": "f180f483d2b2cf18d5ed693bfa1e57fa5b97605d63adf28896b5a08f344fe46f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2": {"doc_hash": "742b742f10791a0a802658f85294dd260ea679ee691ce55cc37a354dd8de628e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5aa112ad-5b84-494b-85b3-8436f2cfeaa2": {"doc_hash": "cfcf1cd40e4b595b4e66bb6e94b0be5ad16fddc7474b22b8373292082c916649", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "164e1c2b-07bb-496a-aa48-34dbd2253102": {"doc_hash": "5d73e79c62705cd865169f04e043b57fa18585a348700dff4d7b1f59f7754df0", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e2147ce1-2c9a-472b-928a-984d2329ad01": {"doc_hash": "b1dc3c75cfc0d8ec7d8f72924cbfaf13ea064bc057ac5bbd9a2f365c13f2e0ce", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "5eb0fb65-28df-41fd-b755-23ab423a2dda": {"doc_hash": "94cbcde0f9265f4ce8a7da6f446dd1ef820bd4c0c1ade4e464d1cb4a50c96b07", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f7e345af-a79f-4c15-9319-47e83c14034b": {"doc_hash": "1c9ccc5b153ad4c2043bff32cae58353e3f7c4ea2a1facfa1c5b63416833fe52", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "230fc3b2-8b8b-4ff8-be28-587e563a578d": {"doc_hash": "634a5dd33cab7b8f5abec3c0a2b702b412c114db67d3d512caed0f86329b116b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332": {"doc_hash": "72fb8f8647dddc8eec1de7d07bfc28f5e3126824d61273ebc1c7191947644616", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "293b03d2-491d-47ee-8f74-0277e644c3dc": {"doc_hash": "de1f2e37a4c1db265e15bf59146f56cc40ff1757146a67592be5efa09d27184e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "956f83eb-73b4-4cf2-aded-318b665f4bb6": {"doc_hash": "9ad9e7452a9eaae151e3ad9e7f48e70ade00b9c292968c58809e41837a737919", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "cb904b2a-e3cf-4192-962c-7412286f24a1": {"doc_hash": "e740ec86963473457ed29990905b25d230b325dd04971bf524de637a40003e9c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "0315fd1d-f86d-437b-804e-aa0d24e7bdc0": {"doc_hash": "c11dee5638b8fa0f6cd26383bf3c4679991732dd254573a40fa74d77b453cf8b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8": {"doc_hash": "57bd4d65637cd2071145e9fc76b816c2a3b055ac599b3c83a19866c75ca7752f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b": {"doc_hash": "9192bf05ef5c9c2eae4a7b638e6a67bd4e224d946f049e83848cbc26fd4e81e2", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "8d51831c-08cd-492d-a872-0c11266808cf": {"doc_hash": "e7ffce70f174871ee2ff54239a5f04a3dfe5fed4e3f56bd824c3a66810ede861", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "4fda3f3f-65ea-4c26-babf-48d8761086fb": {"doc_hash": "f5e59404c3e83402e3b025b28cca821d340f7b051b37b24fb5986b35307ed22c", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "6c6f3e73-f130-4fc0-acc6-693c353f4725": {"doc_hash": "2f69e4104a4c6f2e1eb5dc2ba71da67b2d3fab6157bd7f3eb5bb1999a9d0b700", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "2fa2aac6-e1be-48e6-9438-dbc221d8b634": {"doc_hash": "2994cb371be17f65dfc2052d99fea7c3f8b574c8edec9de4a9bcd02b195691f1", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4": {"doc_hash": "8da989b7ccb075429ba3b8ca3b5a4d1fd95ce443aa9171a7e8fa1147725ab98b", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "92aa068c-c24c-4b8d-a496-104c1fd81b1e": {"doc_hash": "73dc283b958e9c272d6a12195771f5102d4000b11074fd6dfe80ede0a26bbce5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "49c04758-5c7f-4ca5-9dff-36766898c7dc": {"doc_hash": "b2eab9bdd9e66a4aff219deaed9bd08507900c9a6b1377c0ab4102e329dac5be", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "03013851-f68b-4beb-adee-2ebfe0dab352": {"doc_hash": "1a4d23892b895b2512c59b17ef1212f8e0e5c48575f02b79f7281881108d44f5", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be": {"doc_hash": "b830de06a56fae46610aaa050a5afc073bd7396047e1240701852535649d1f3f", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "e44afe30-1077-48d5-be6f-b761db49ccc1": {"doc_hash": "f3a8d1d764c68d3b0a5b5eea30fe668f0e0a712204b525020d5b65cd62b7e4fd", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}, "fef73a08-75f7-4516-95af-041751750caf": {"doc_hash": "b01dc98e25234fdbd11e6ea2b71f19824e3a22f580ac045cbdb97cd4a8fd758e", "ref_doc_id": "13890c15-bd07-4cec-910d-21908a050065"}}, "docstore/ref_doc_info": {"13890c15-bd07-4cec-910d-21908a050065": {"node_ids": ["2afa0d72-7f52-4451-a38d-3322fb2a08cf", "a6917fa1-934d-4975-8793-ef67d7c9acf9", "9faf2d21-870f-4215-bdd2-9b39f53126cb", "1da6c849-6f7c-4859-bba5-ab0c6a8b5a1f", "fccfcde2-2c44-4979-9a61-b81bcfea9602", "42fe3c04-6a3a-4434-9e7d-3ad7cda93957", "0345a04c-8d7e-4d6c-b5b5-9935453a1423", "8f2511fc-3a2b-4d8b-884a-e14b56568452", "f8421a5b-6167-46d5-ad75-d1cba902608c", "ea93af9a-efce-40a1-8bf2-0f0168c5505a", "4c65f5a0-ca53-4eb0-8416-520bc6cb86e0", "0f186c54-f6c2-44a5-b000-8454a7cd2583", "46aa50c7-6504-4f14-a013-f630ee4f702e", "85d2fc81-458b-4d94-9e69-621021c8db1e", "625aafe0-01af-4e9d-876f-a9b193273e54", "581d5a9f-fe03-4dca-adbb-836bd8a8daad", "7d474804-d117-42ce-9cc9-e77f5f16dadb", "afb5be51-ce36-454a-9eaa-6c70f3a2103e", "e03dfe0d-b27b-4dec-b027-02ef8723ee8d", "2ee2bba7-5109-4c22-aa5c-a5c1aeb79413", "71194748-3662-41af-abf5-46b3bcf89469", "77657998-374f-43cd-8fa7-3ab3b8bb6481", "c9025230-0639-4a62-89e7-ab148d9c4d46", "6fb75d86-4dd7-44bc-87cb-50028a021caa", "04bab471-ccde-4f95-9ca2-9eb8bd7fdab6", "7ca3e729-9fdd-45ba-bf62-de9a769a736b", "89951293-a6a9-4b08-a15f-b1dc99ff4743", "c14088ed-de4e-4045-8cb4-15738b17d83a", "2e615998-dd28-429a-93b9-73dae6d44046", "0ef8788d-aeb8-44d7-ae94-741ac6e7b271", "895ea528-317b-4787-82c4-a4222d01dd4f", "431d04f7-ec0f-438d-bc97-d3ab5d917981", "d13972a6-39df-4843-96a1-9e4a22242409", "6600a00e-bfe9-4cb3-8eff-747df138760e", "bd2b5ffb-9dcd-4894-a023-6c32e08d4956", "d7efac5f-e3a5-4130-b855-7072616332fc", "6f3bd974-df10-46dc-92e7-f9341b6adb91", "1d5074aa-b778-4a5f-9a11-d2440f4256c5", "5f864c98-1fcc-437d-8d6e-33ea59fb02d5", "0f63a341-4c7b-4e0a-9cbf-be1ef4e2bb6d", "f332dc8b-f45f-494d-b5f2-7cd1f5fe3084", "d7773923-c9de-490f-bbc2-5f8f24e17f0e", "86f4b4ef-75c3-44d3-9db7-fc0059332b91", "61eb9bc5-1853-493a-b728-b123ac9cd444", "c6128816-6a90-46fb-a9d6-cbdef2f47640", "e2d4acba-d7c4-4788-9b3c-e928b0c37782", "b3c4fe98-3ae0-45e1-bc74-8de67b7ad9d2", "48a046e4-1c7a-473e-998d-245b521c1691", "cbe2c640-e6b8-49e9-9e3b-59d4ddad4699", "85c9e32b-8614-4f2a-8762-3c97bf6d081f", "5496aa0c-7d5b-4b69-8313-5c153bd66a8d", "83ed72d7-e2d7-44c1-9b81-8f99a1ac0275", "08fbe868-004f-4339-a7c9-c37576f9a7fb", "091a7f7b-ca71-4b81-927a-845f08daf909", "261ec8b5-cd9c-47ec-a0b5-497dcf198cfd", "ccf39d00-db37-4c60-8ab9-c7d6f500cc3f", "dc45d385-72c3-4b81-98bf-d87784465900", "a2f22521-0da6-4d25-904b-1019b606b2a7", "fcf50002-0242-47e1-8c2d-7bf4523eeeb5", "0a9da0f0-20c8-49de-9e17-8ff0ece48fa8", "5c7f513d-77ff-4298-bd80-35e20c97617d", "e23e7096-4355-4676-8497-4f0585b9255c", "ad757761-937b-46db-a109-75fc66bce2e5", "8fa74f19-76a5-40f7-8410-3a0c5c8cad31", "5fa54a80-7cfb-4a52-b0a6-6f97bc96b173", "df43e7e7-f667-4f2d-8c64-2b4a22816bf7", "e7695727-f089-4ed4-9b73-724a5998cd38", "877fe78c-f363-4c45-8dd7-7b38af52adbc", "bd9b58ff-eeda-46df-b657-2564ffa240c7", "1b97d233-1317-4e1e-a8c2-1850424a8396", "da1c3963-f8b5-4712-8366-c7c4a6f5efeb", "c61e2a43-19d3-4ce5-b28e-bf5911134da0", "01b9bf15-aff1-4eef-8cf8-8cd68f773fd7", "d81b5d39-861b-4d5b-9aa7-7b11a7c68d87", "e489e613-2c89-4859-a726-340a417e5eda", "4df34045-d2fe-4848-9b30-b1676fb1309b", "1060dad7-138a-45e5-bba5-fe384bc7569b", "02330974-dbba-4011-aba2-0e6b4f1c0a67", "994ea49c-462d-46f7-abb0-6b29b08749dc", "342c4650-0350-4b78-a2bb-7f325b5aa8fa", "29dfc462-225b-4b63-909f-08375cfdfbe0", "1cf3bb7d-906a-4127-a5d8-7f6031e7e95c", "a7e6b31b-969e-475c-9ac4-e3369d1e290e", "4d4439fb-acc2-4330-83e6-29c175f72014", "7405b6be-90e1-4a29-8eda-84001617cc06", "64de67c2-506d-48dd-afaa-326decdcc837", "eb6eb391-0300-40f8-a61e-b107bbbc0c2f", "32b284e1-bb6f-4eaa-933a-ebe480aa6dee", "5be36c37-bd33-4489-9e82-30ec2f71e715", "ecd7a1a6-bbc7-468d-92c6-c762f1973fdb", "a5bd8d71-1d42-48a6-b126-30be2ae64025", "5db9014d-9b3a-404d-8196-e20299ef608c", "bf374a0a-d4a5-4d5b-a175-0d70b1c414aa", "82281413-8084-44d1-8d81-fc1b08032af0", "82c0aeef-ca56-4431-81df-3627966fc03e", "38507cad-f8a5-4c49-8a94-afd7c4ab52e1", "7931d982-c024-41bc-b292-15c647a18a57", "fcf22dec-41ed-4d55-9432-d48f0b4acbae", "afea7bf3-6755-4d28-aa13-10523385b153", "b6d10179-db9e-492b-a0c5-6966a7837c79", "0a4d8153-2591-4e70-8dff-a92c0dc0496d", "5d75e662-c547-4ba3-8f2e-4d05ad991f10", "90baf5a8-651b-4fb1-8eef-d90220395627", "5084e129-7b29-4991-b338-355406171b1a", "4db0373e-93b6-415a-94cf-0493a82f1ba0", "cd0039d0-ab32-45f3-aa93-10834e76228e", "ea6031a2-bfc0-4656-bb6d-4e23fd9fe293", "82e6c009-d0f2-4faa-ac24-fd1786c0443e", "c532a4fe-07a5-4940-b7f4-ad3f5fe7231b", "18da6f96-247f-4fec-b2c7-74777312f774", "d73e25cf-43f1-47ef-9b5c-0b0f7ef5f0e4", "f3ee30a3-9d65-47a6-811b-1c0c37cc00e9", "de3a88b4-0bab-4599-8b00-c305d8aa693f", "9075e7da-a7c3-49d3-86e3-98a8f728c72c", "6d22501c-1b2b-4249-9a81-22f372250627", "9e003a72-00e0-4457-9fd8-b24ade8c6215", "367f703b-3d43-45ea-aaa9-7e6c1fd00231", "8b096f3e-379b-4790-8e1b-20c4050d6c04", "32b30a21-99c5-4984-84e3-8aa7fb3da840", "c1473f48-3fd4-4d9d-ae71-6ef716f6a443", "a4450f7d-86ea-4489-a8f8-592d9e1106b2", "bffc1a77-e752-46ff-98d0-926587199147", "730711b2-f2e6-4881-97ae-60f2723fbcbd", "1710651a-5562-4711-a26a-d5cd21cf1ce5", "ff7e4b28-68d9-4b8a-943d-51fc51745a35", "05948547-c438-4254-a7fe-093e21294490", "92faecb8-d912-4363-9a3c-a1d659aaafcf", "4653ce15-0e56-4981-9575-4e1c769d11e4", "64eafbef-8222-4b69-a671-e92a39719c87", "84713e50-4c3a-458f-a6c6-ea140ae0edac", "e2d0444c-fb6b-4f0c-940c-8bcd78139052", "18eaf744-9237-4f6e-b329-07628e71762a", "f23d879f-3d91-4cfd-b415-f049b77e4ecc", "9c288223-633c-432b-8af7-34a92f6d8db0", "80e0ec46-be06-42f1-ac08-be8ea7e0d8d4", "ccbc9915-12f6-4cc0-a1b1-0f036101603d", "d3f88296-eaf4-4f23-8b61-b2581e660d97", "118680ef-4928-4cc0-ab0c-dd869b4d5046", "08599053-8a3c-42f2-bc72-866dc55b67fb", "1cc734f1-2a46-4402-997f-ee1631ca6210", "22280c5a-dd62-4469-8ff3-40ca7f2738a9", "d8af5916-e05b-43a3-aef7-c50f5abfe8dd", "597869a5-0f27-457e-9e04-0b9792ee348e", "5305be54-edac-476d-b3bb-37ce1fbf92a6", "d3b9173a-e00b-4e15-8ebe-a9722495df0b", "b967033a-0a65-42ba-bb95-35facc7eb69a", "14d6ba39-f710-4c04-b382-741d40c6e92f", "ee3bc039-1de2-4475-8e4c-87fdb311afd1", "24010dc3-1bed-4e7a-98e8-3c5e4bc908c0", "ff9f6d4e-5e6d-44b9-802e-42168c61411d", "4c5572c9-1cc3-4654-ad74-81c1fca9e1f3", "f4b4b28e-143c-4f39-b291-0cbbe163dc3f", "fadc2540-05d9-4332-9ef0-d8a2edfd32e3", "ad5ec67d-3e88-4584-9be9-099f20811dde", "d2909a32-6aba-48fc-aae9-12e5e4a339e7", "419ff4c2-100f-461d-b09c-49d92f3e86d2", "8a284450-61e3-41c9-a5f5-7d7db3173362", "24619ca6-683b-40ed-bb32-292688ae6e01", "1decaea8-06b5-4566-be70-a1e74bf48e42", "a1dba6fb-db28-409f-b136-4c523f2861f9", "c48c526e-d899-4330-8526-d2092bb7224a", "8e374941-a670-48ec-80df-6a23e3195988", "3e503861-ceec-4d66-b756-c7e683b42921", "360f4755-ca31-4e83-8d8e-4f95e708e11e", "f7e7eb72-b500-4003-baeb-0161f13c7965", "c18b7eba-b3ff-468b-88a7-3dd7d5959a98", "938f022d-1d5b-4d15-9096-ab57320bed3f", "9810f677-9bff-476d-8b03-c7717759f185", "690e4acf-bdef-438a-8a32-2852a380eec5", "d4efb1a4-0128-4c52-993b-7f0e3c0fced0", "be79215a-2e71-486f-b310-529a9ef91973", "023eff69-4e7e-41c2-b396-50cb5a708842", "3ec356b5-477a-4745-ba6e-88e5ae8c4627", "fc7742f7-e13f-434b-96ba-66b8bb2ad86b", "55f1ddd3-a23b-47bb-8d45-85ef1ee68645", "7e8b3264-ab20-490b-b14c-c89aa4be3022", "c139b91f-c56b-4745-9d0a-bdd63c9f44c6", "a1233690-dd7a-45b4-9f14-6f108b61db4e", "67263779-31fd-46ae-862e-7c83bcae98bd", "e427c614-9315-4228-bea1-d8f0459d6009", "01d02fa3-cfb1-40e2-88cc-801891efe245", "3031ccef-e341-488b-a3f2-80fc1d2718b4", "bff44c06-3069-4bf7-aa12-9c1662f37155", "040f2da3-b2b9-4ea8-93e2-1e900b82ff7e", "96f76069-1dd9-4587-9155-c0280df6b1d6", "6a098279-cd1d-45be-a61b-9e549be819fe", "7654d8bb-f7d1-4b01-afc8-117ea54494d6", "b40fb0d2-cc97-4642-901a-68026202faa4", "c79739bb-d186-4311-8c11-c17ef8c0e999", "039645ce-26b7-40d6-aac6-d032fc917186", "beb4bfe1-ae2b-45db-89bc-1092d6a866aa", "7ffb57a7-3515-4979-a2ac-aa466597d7a2", "2fe9b0a2-5081-4e1a-a90f-568e78017e3e", "4c583bcb-05fb-4848-a8fa-ac187bb2b9d4", "c151e3f2-0172-4485-868a-8a31aa20a095", "aef326da-0073-4176-9d55-1dfe4fb9fec5", "1d04f3a5-b9e5-4e12-bc87-bfa511016c78", "62e8cab7-b60e-46b9-9cbe-77a678d1e7e7", "9c69731a-be9a-4009-b3aa-8f4626ba5adb", "e3483b7b-6c2b-48f3-8756-a3f79c09770c", "c70def48-f634-49de-88ae-da400c8f32fd", "75540c36-8d19-44ae-80e4-7e2d00207ad0", "43afd3f1-c638-47bf-868e-81fbb74bfa43", "d16d0d0e-b450-4a7a-a5bb-e8f643f635f9", "7f566fa3-378e-499c-9fc9-e413776e17ba", "314f3be1-65e6-401a-9e89-4ce9a0dbab08", "af07c2b6-0049-4443-8273-5d7f2dd1e868", "d07bed13-a8d7-47bf-a416-a27f19e2eee3", "d6ce810e-7f5c-45a7-84ba-709c4d5dd10b", "e314e09e-92ba-41d7-ab2b-979237ffee8e", "b9526e2d-83b4-4945-a15e-bf9a5c214db0", "c0acda8b-015b-4e2f-9303-638f791d170a", "8468dbe4-714e-4770-951c-e498ad245e3f", "5822eeba-5a06-4278-8df1-1ca75b61a3ec", "06cba9e4-c6ff-48a5-b3e3-a0508d82c529", "092ccb19-fb27-49da-9d9d-4b3397a7315d", "4443e3fa-29f9-4278-a011-af8e3042177c", "2053d1a9-a0eb-4913-adca-5f10204c5c21", "e5cdd13b-20d9-4291-989b-e407b1183b83", "a3b01a48-b84e-40b7-9025-de2b14b2fd09", "c3967a15-6aeb-4631-9d46-d6477070a86f", "a736e4f9-15a0-40de-967a-6e35134ae524", "d716e990-de77-4d34-a4a5-fd9f39686f53", "4d45b420-02a7-4774-b242-06fa4268df16", "ae4e376a-2014-4c43-a327-8b33bc9c2196", "a506b590-0a4d-410c-b8c4-489e5beab966", "b5613560-6433-4e09-8a9a-13a7d194d8db", "7609f27f-9005-45e6-bf37-cd28313e0f1e", "cf4ce08f-d95a-4d0b-b0f9-514709d96e28", "46b090a9-cfc8-4bf9-8c54-3085dd3539c3", "a23b8f52-cca3-4f99-a381-fef16cf0754b", "03342b7f-810a-47d0-9213-0267839e7c4e", "d78fe297-fc46-4458-b14d-1456143799a2", "d072a57f-da17-4ec5-8da5-66a9091b9d73", "52d8f2aa-cc48-49ff-8feb-e959db97b6cb", "fde529aa-0156-4c5b-994a-b64edb137a6e", "b7d87110-7365-4065-90e1-354cc47b22c0", "a6b23879-4ebb-42ba-8c2f-60e3f8ac3af5", "ee3b3b0d-63cb-4867-9f4b-5a5ecd14b4df", "a56b61a4-bad7-463d-96c4-2c49c5a9a2fe", "6bd26748-b23b-495b-aae8-438cd255d06c", "5fe145b0-c0a5-4220-b961-c08a3da9ca00", "31d9931b-bf83-4fed-b610-b3de97494e34", "1bb40e13-bd0f-4554-9d77-37c1049a86cf", "15bb5fa7-cd60-4856-80e8-c097e50c0891", "f7946f54-1cd8-40fe-aeed-5a5996f21735", "92bf1c5c-34a2-4149-b604-b4edc88ad719", "72ba7687-abff-4483-8f87-33301c608868", "31120862-6715-4546-81e3-d43f5ebbf3db", "8e3a5d30-e8cf-4ed5-ad22-06b44f55fab8", "303bcf00-4f3d-4f6c-ba6e-b3cc0f1f6e06", "76fbad2e-6400-438b-a560-be46e856503e", "918faf00-ebb7-492d-b043-0aff7fedfbe5", "83059430-e440-42ad-aa3e-a9cfc4a3be03", "e984b3bc-8c6a-4f5f-b11c-26f258848ce7", "4860aa7e-93d2-4057-9273-b61143af6859", "e27568c8-9294-4ec9-bc11-5208a849a15e", "67db6a3f-4b76-4b66-960a-b52084360442", "f149ad65-76ea-49ad-b1ec-e8be8ec89a23", "21d0cef0-d28d-4096-9c2d-c859a873286e", "65f9b2e0-b3e7-4056-88a4-859663f08582", "4b8758c7-9793-4811-8d87-814dd645f6d7", "635520b9-4c10-44b4-b9ef-6e63f415f6df", "19b239f1-aca6-4429-bb9c-8895798fc507", "aed3baf7-f4f7-414d-9b02-fa6fdaaa7367", "f93f9750-eb9d-4469-85f2-ca95c8aa1824", "90699bf6-8283-49e6-a729-33c82ff4a77e", "064db35c-1d84-41e9-ad4a-91ba40ad0888", "34f1216a-326d-4042-b7ae-82fc20263a7c", "89aea79b-b545-4b89-8247-6fdbfe3a63dd", "dd164969-2607-472d-a2ce-a2a0e078d1e7", "f07533c3-5f01-4172-b371-ac2859efe5b5", "f2f7e06c-d897-4881-87fc-4cc7fa506c0e", "2a770de1-cef2-494e-98f9-06ae5abd048b", "464985a9-2857-4151-888c-5c6072a051b3", "9fd8ad0b-305c-464e-bc28-68b84b940f6b", "3ed3543b-380a-47b4-aeae-0f2c88e59108", "cd63fa0e-04ff-4403-9eee-127f17f44a26", "dbb2288d-ef75-42d7-ab0c-96b58e170a53", "6a167a58-de5c-461c-bbef-e11384d4c374", "0460ea95-4d81-43c0-bd74-78234b16f512", "07a5fec4-52fb-41dc-b0a6-bee707405cfc", "e13dad54-f9db-41a8-8cc2-ecb68cb7dfc8", "da583324-82ae-47e8-a773-196fd3a1afb3", "c754b380-afd3-444f-bf07-b93620cb348f", "66a026d4-de66-4c29-ac34-65539d538063", "76a5fa63-76a0-4a2d-baf2-be68f493517e", "26a69f3e-922a-4b3c-a435-6aadd83fe08f", "4e1521ad-d30a-43ee-99c9-e977ad27b0d5", "d603efb8-292d-4488-852a-4b2911132cf9", "b12523b1-6057-4ae1-96f5-6d68b51e5b0f", "93aa6d6d-4f7a-4b4b-97a0-d2d3b0f8fc78", "0119dc57-76f9-4054-ae8b-99be2122b676", "2ba2df28-af2e-4f36-8fba-aa3b89410093", "ed893ac2-63e3-4735-99b4-d2d5341f41e9", "2ff3cea2-27d4-47a5-8116-4139af197c5f", "a0f42204-a9f4-4e63-82d6-cd4b452ccf18", "c1cfb007-d9ab-4278-9294-ed1db025ec33", "c38611bf-f02c-45bc-9b89-236ac7424030", "07adbcfb-e300-462a-8794-4d0d3adc0efc", "b640beeb-69a8-4b36-b994-3d53d5aa6d10", "6cf877e8-cc97-40df-ad9f-aec4e3299025", "365d6a92-e13a-463b-887d-b24c4a770740", "0fbe16cd-03a4-4d99-b25f-d27cc4c9fb0f", "46f837f3-36c4-4da4-ab5d-24b1ff5feec1", "726fed06-4af9-45cd-9095-db49e93a1682", "f72c7f07-7c7a-440c-9f04-a726e5ca0abb", "4a34583f-86a7-4439-80ff-5f60c350afe0", "ce9ba9fd-c907-47e0-a6d0-fbbca47b5d66", "7adf3fea-86b1-4bf2-ac04-1fff31ce545e", "727aee78-1ea7-49a4-88b6-03d5f68aad5c", "1cbf0323-5071-4235-92af-2c3614f43de5", "bbcef697-a7a6-485c-b633-32b07ed65641", "27185349-25e4-4e6d-91da-e59d0b74a82f", "65148b00-abc0-42fa-afc1-0f1e2bd69461", "31c25542-a888-4561-b4a9-aa0a07a08d85", "b8915a20-ad26-4ff9-bcd0-751c77caac3c", "ad8aca8d-8edb-4cbc-b5cf-606e5655ca8c", "529858d9-8603-4c08-ba49-00545531b1b4", "efca3ca7-2c18-4498-aa42-d647297b6ad7", "827a96ad-1d49-4efa-b344-7152368b2f5d", "a4fc6b85-7233-4ad0-8178-1217f14817a7", "278365a7-7adb-420b-9c85-a10842275f43", "e0300cdd-ff72-4a91-a797-1bec8fda1677", "1b7b4486-5104-46f9-a50b-a08c8b5a6093", "3ba9b9bd-8df4-4c5f-89c9-313880c233fd", "f2a6e367-9c40-4ed4-8868-565271494414", "0bfdcb50-771c-4b3c-859d-43bd6a49e7f3", "ebf56c4e-4e13-4b06-92b4-331a09d3b374", "f635381b-6b58-4a99-bed7-488cab94aefd", "9c7a80a4-3e78-4a0a-9798-d32dda1aaebe", "542fe48c-dee1-47dc-8ca7-75936d21d08f", "c63f546a-24f7-4733-a52c-013e7b99cb9a", "3a59884e-fd56-4b15-a9ea-4947007b65b7", "4ec86be3-a5a9-42db-b554-9b804aa096d3", "dca2672f-a0f1-437f-a468-a331230b858d", "92701578-19f9-410c-9b3b-68375798b1ca", "14b60704-7953-4ed0-8cd9-1a4746ad1992", "41e3336e-316b-481c-a93f-8bdf0880e291", "6678ef83-b426-4666-a2d3-72d33cbe5e87", "dcb43b7e-7506-4082-abd0-0e8c312a1966", "11b26dad-728e-44b1-8a97-aa93e0ea750e", "96e83b06-b357-4f31-a650-ce86436d5a83", "c64efb4e-fce5-4301-8d52-723b6cf02edc", "3635ae71-2ec9-4b0d-b2cf-565873e2018a", "9a50eded-d210-4ff3-aad8-33e5223d4030", "8b7e53f3-cc00-4892-adf3-dc0c545cc9f5", "518b0288-4861-42ab-8a85-d57486b02147", "37daaec4-bf1e-4bc5-9809-0ab95e0bccc6", "a9ebaba8-da3c-4572-a9d4-932f6dedc9e7", "a3da216f-0f16-4b1c-b84b-d36f973e35b8", "938f102a-8d57-44d2-affb-e138b54e0297", "0c53f415-0e52-48a9-9476-927d9fd4a8c8", "f8ad1767-2616-42d5-96d2-06ebee597123", "695dbcd0-7698-4b83-928d-b40a7b2b33cc", "6fa276ad-9114-4e2a-b5a0-5b6a8fd1b6cd", "1aa8bf93-4bcf-4895-9c36-4f186903949f", "403ce972-c506-48d0-8f0b-7ab4eb14daae", "435e7b89-847f-4f05-8b16-0cad3200a604", "099615d1-37f0-435c-8582-005f9b081529", "bb9fe5d4-a59c-43dc-bc12-0ffdc4363913", "dfffcb70-34aa-4ae4-a6a5-685eae048205", "ee2be34e-8be5-41b8-a7ba-93cd0d853104", "5c17b870-b05b-4dc0-bb9f-5befcb47075f", "4b68c5cc-5fca-448c-96df-961092921b05", "46676818-433d-4dda-84e2-020463a3af95", "30ed7bf9-0514-40fd-8b16-ce142d3a610e", "39884f18-d161-41e0-bd57-93c788fc3788", "2e5ef68d-a7a4-4077-9c37-1649f46c629b", "bccef02b-9e01-4803-9211-0917a54f8a91", "28ae4f16-7cce-424e-bc91-5102891a8aeb", "d7258df4-833f-412e-bdc2-024a826cba20", "54f633a2-ea36-4a31-a64d-acbaea796743", "983c1fe3-f347-48a0-ab56-42b5631800d3", "e9d90b45-2a94-4b72-a1b6-8c78c390c260", "cf3f631f-86ca-464f-8ca4-ba18ee455221", "9303bed2-daf6-46a7-af36-967b548f27f0", "e2a8054c-f853-44db-8e14-f6078e68b90d", "8a4a01bf-2da2-45c4-9308-a0105bb17ddb", "a1a181f9-c6c2-4b6d-a336-03f6a1a9b829", "5b753f5f-0758-4880-992c-183d9bd368a5", "2386b8b7-b35d-48d1-b683-bf4d97667f12", "807b3734-458d-44ff-a08c-26d72136d815", "21c2eb3b-bceb-4a85-acc3-5545e6f8a985", "9799d198-6519-4412-a983-efff37170970", "bc5a30b6-ea4f-43e8-b17f-f52a1c3f5c17", "e1bbc1ea-e6de-4e06-8ea5-23aa60e69076", "40c989b7-a919-43d8-96bb-b8f3f4663938", "29604c5c-b6e1-42db-a6d4-63ac3a7e3d84", "ddab7232-fd5b-474f-b865-d333d3ed0af5", "5c4f1ec5-b7fd-4412-9b82-ef84d2320779", "68511911-f243-49f7-9a48-79753276a56e", "a11b71cc-ff01-4d32-97f8-27970c8a22e4", "c77e3048-a464-477d-94b0-4a31e7c96a55", "167a1ef0-9ed4-4cc8-a523-dfccf6e98066", "2085a59e-fb6b-46be-b090-7bf1c92ea15e", "df9ff41f-0747-475a-8d43-7caae5e3b199", "26094046-8c48-46b7-9ba4-9271dc0fef0c", "bd802c7c-e9c2-49e4-aa0f-ede7eb7c277a", "691d35b6-2000-4e81-ac07-6a1905056a7c", "4c3228df-0c16-4652-81e5-70005bbb573a", "86be2ce0-124b-413d-a8bd-6d3436f50424", "5916bf83-f09d-4ef5-8bcd-4f9359bc66ae", "62dd54ac-704c-4ee0-a449-966779825fdd", "85479058-7981-4c8c-a0a4-987f8cb82697", "e69f2877-a20a-4d40-9bf5-68e22ade50a7", "c8e69be8-2bf0-4b01-8c62-71fda435f610", "3fd25c0d-af7c-4ad6-82d1-f9a16ca64120", "d85dd27c-afa6-472f-93a6-0dd68a5f93a3", "64e6f783-13db-4080-92b8-1686925a45e5", "ea0ab7c6-6c4b-4010-96da-0ff91a9b07a9", "04955313-5e98-4dac-ac7e-8be543354f66", "d109bfdf-72e7-4ee0-bf93-d65f4eb9f46f", "fd7f50aa-239d-4bed-8ebb-2e891e2f7699", "a3456c28-65fd-43a7-bb74-95c655782de6", "4e7518a7-74db-47af-a5a7-88798742d86b", "32a4bd78-ce21-4557-bd0e-3820c4bf251f", "5cc8e828-cd04-4209-ba93-02a1493f6031", "6c0c627e-ff85-47fb-82aa-9b8137d37c2a", "c6644d37-8b95-468b-b51c-05dca239d21b", "c64b9672-2727-499b-ba52-390a5cf914a4", "91d7b649-b55d-4ad0-b106-e58cf62417dc", "3bc7111c-8802-4ece-b0a5-039dd241f8f9", "4173dc84-ff0d-43f6-a4b3-d70560f276e5", "589571bb-4c5b-45c2-bad7-aec9148a56ad", "26c9c8ae-b4e8-4889-bcdf-6de759d80ccd", "fca65d61-c2a9-4c40-8a5c-ba79928e4f93", "4bfa5aa8-9161-4839-9fa4-3576431c7366", "8530befd-42df-404d-95fa-a53deda26645", "be2c5a17-8d41-4779-b0fb-8e5347df6094", "b46d2d11-6158-428e-b752-95fd6dc22e75", "3889a560-080a-4373-bcb8-e2e7eb583a60", "08209b0f-2b60-446a-bead-b92bb5cc7dba", "0257d759-b104-4f4e-a2c4-4ce6d280b37d", "d21b5741-3f9a-48e2-8e90-714b3be77f1f", "c7dcbfea-d2ff-46b4-bf27-c28c877383e0", "204c05cb-54d6-4537-a277-abe74093ca04", "4e4b7609-8c2e-4c32-bac3-5a44d2a2f109", "9d19bacc-0a1b-4583-82d4-d3296c92f966", "e4826f2d-db11-476d-8a71-cb5a201400b9", "16694feb-cd8d-4385-ba9b-139f317924ce", "1044b065-3193-47d5-85e8-3bc604b0912d", "3143592d-66cc-413e-8863-96948df4d304", "127f3d65-f0a5-43af-a00d-ad09c5fa5d6a", "4ceabbbf-bff9-4bcf-bba8-1933bd6c1536", "5db3cb52-6949-48b1-bdc8-8a9d8b5ee403", "84275586-19f7-4104-953b-b90e13b18534", "e1a3cb02-42df-474d-897f-d333d0e9c6c3", "aff1d461-2af5-4425-ba68-ac4064a4c753", "1af66d2a-0a87-43ed-a2f1-8411d1f9e2e7", "113fbf70-a4fb-4251-b7a8-1ad3a836d069", "776336c2-5f9b-481e-b1be-d9d8ba4222cf", "dccf6ee4-ac1c-40ce-b989-bcb736570a77", "0efd2111-1824-46e8-95fa-3e20e1d70250", "5dc9d723-b0fc-4c8b-95ae-00c8c4b4e977", "40a81e57-e9c3-4560-b0bf-a6f10222a132", "706a5828-df7e-476c-987f-670acd804d7e", "1821963a-b804-4d05-b64c-be8ba740d1ed", "cfaadb94-b123-4d12-97db-974bcc660886", "496520a6-fe70-4fd8-8976-b29afd3d09c4", "37563649-99cb-41a7-aabd-3cc56c744fc4", "e9187751-7cf6-4291-9668-99baabcb567c", "c22e25a4-be23-467a-866d-aa26ace2571c", "3243c420-1d1f-49df-b4e6-fbcb6db504c5", "ba263390-366f-4e75-917f-fd6004ef7724", "c9efbc85-e15b-4451-aaba-5b629f5c340b", "ff507ead-b1a4-4569-8992-e94ede22decb", "48d21dad-6d4b-4448-b6d9-6426a7535ce6", "4ad5f457-ab8b-4985-a59f-a8fc7c7c418f", "70ccc0a2-3007-47b6-9917-0b0a3c5397f8", "e59321b5-64ce-4cd2-8554-da9157d59aa5", "ad658f22-79a2-4a69-9c2d-aaccdd525120", "71277d93-6365-40c7-bda9-5e28d6de9d03", "f488999e-c3c3-4817-b2aa-5cd7ae5854d3", "4b8c07d8-3d79-4ff6-9e20-6bff7d07d7da", "41de8e14-b71e-4776-8fa1-46d827c36c27", "9b602795-25d9-4792-93ed-eb9598d2226b", "9fb0cb77-c236-4cc4-a3dc-6078f3a72ee6", "6513f047-6a0e-45cc-aa50-aa6de5575b12", "1ff53ebc-4cf2-405f-9567-bdbefb7593cc", "231f8202-66da-42ad-b657-843c826b99e2", "90799028-d669-4363-a648-78b55b29d8bf", "902ec6b2-376e-4c52-adcd-f2333c2cc3d0", "3ab02bc8-feec-4598-ba88-eea311e8f611", "69be9a93-8601-459a-8edd-e6a1e32c64c8", "dc6106f0-3725-4577-93a3-340b050513dd", "8be10f36-de61-469b-996c-091c339025d5", "eb1f9fc6-8635-43b1-b74c-13af19cd13e2", "6800bda3-1851-4c38-89ae-6fe1689386de", "dd56ff93-dc0b-4f49-8a36-6736740e4025", "a0aebecb-ab78-4711-ac52-2b5630666c9f", "b323cdcc-4d57-498c-82c6-e50f3a92fea0", "ec9dd294-7f95-489e-8524-054b012640cd", "88df90ff-1ed7-4b48-a970-0b6f8a6dd027", "7c4cf198-8c52-47a3-9011-a82b9d8db5a9", "dd2c935b-3880-44f9-84ea-49280825005f", "539f36a1-045e-4dc8-976d-af4d371158a6", "5d19ecea-21d6-4fec-9603-a2051bdb8078", "4d7a89ab-8d23-42c1-bf3e-7b45a32afaed", "eff11646-636b-4f05-a193-ca69282c17ef", "ea148f05-65b9-412c-97ff-da12acd35136", "8777d75b-6951-453a-8947-8fcff76542bb", "6a5a8151-c060-41a6-a81f-438158a969cd", "8f09da6d-5228-4569-9cf4-e0466584b90e", "f70dde24-41af-487b-aa84-f3e731a78bd6", "efee4ff5-96e6-4f1e-bbbe-a37e4b7474e5", "35d17a88-58e9-4ca6-b300-752255957f9f", "2e63f8e2-d303-48b0-bfca-38a585648486", "cd3bd027-5004-4674-be09-1c9a43db5dac", "ca121c4e-3189-4cb6-bd61-1a6333682543", "54e1a5c8-4834-4780-9a19-88880306b6f9", "c7a67ea4-292e-4f88-bbd0-e2a0d30974f7", "b7f2ee58-0602-49fd-8efd-e75054637752", "cde31051-b3ee-4a0c-bc85-a9c2b8b2a415", "3f9cc2f5-656c-4b8d-8588-8793ecfa0f58", "ceb7d5f4-f469-4938-bd45-8ca7dc0aa4dd", "8b57fb07-0350-4055-a7fa-240752a182d6", "16f9e688-d85a-4691-96f1-5d1b875e76b4", "4ecd8fee-c08e-4d27-8246-aad6f761d987", "83a8bf1c-3073-45ec-8558-8ffd210cd308", "81ddefcc-c2a1-469d-aa2d-9ba2371e0e14", "ccaa9034-6d31-4c57-9751-64c8355f1f01", "fd1f8fdd-ef08-4b72-8096-cba1dbd9cbad", "61fdc6f7-900d-49ea-83f6-c19c89253afa", "9d556006-0617-495c-b540-7fb518542a6b", "4548a89e-d6ac-4a6f-a5ea-ea2e175748b6", "60fc0fb3-cc6b-40dc-b835-f62bdd50414b", "c794b653-5beb-423d-b311-fd822b2117e9", "165f9ba9-22f6-4923-8d69-29e47af3a02b", "5965ad36-a7d6-460c-b76f-621eb3f6cf67", "fd075574-8afe-417e-bb55-a7d0701dc1af", "db35fb43-d82a-43f6-9b8e-b40dfec5b99a", "27a48193-8aca-459c-8ba1-b3bc484eae80", "960464e9-fc8a-4bd3-bb58-e784d1e302af", "fa12d595-377e-4bbb-aa58-f2cab7e8a57f", "bc6c5046-b198-454f-9620-a9a5d74f9fa2", "5910b5be-4267-4ea2-9135-65cb147c7c39", "3b7e5f9e-cd23-4c33-a60b-b11eac1d1373", "cefbdf0f-869e-426e-ba24-39be83615381", "266183ce-a17a-448d-8aa1-7bffe8d254d5", "63e2ec22-3381-49f0-ac9e-fb540705c9fd", "05acb078-a0cf-46da-8ce1-4f58448535d2", "92536d3a-746f-4ccd-8190-9b4a6dd95b18", "443dfd09-54ff-47da-82ae-501a641369a3", "a5d255df-d61d-4ff0-b190-6f4ffc52e94b", "c7ee0cd6-b5c1-4403-96ca-c4bc7820c64a", "65faffbf-4c40-469a-8216-834a9ff4041d", "37c938d0-d3d3-42ed-9a20-73fd187a1c59", "ffd9d661-fd05-4848-b224-57e2a42d678c", "693f011b-ec1e-4d38-86fc-99572ab4b862", "b5751895-b3d1-4cff-b0e1-c135cf785f19", "f0d2b075-7766-4445-bb88-256551ecadac", "00a5bf42-d5a3-45b6-b151-87159f937a11", "8c230598-c4da-4bc7-9300-45b148ac5377", "dbf9fa1c-46e4-4dfb-8c24-8cb5a7f748f7", "a7cb94a1-7139-4540-9335-22009c7d151e", "cd813494-2c4f-492c-b0b6-5ce159aacebf", "dfc1edf9-64d3-4107-9238-076f724b3f42", "bb08a789-9991-4a62-8535-d8c2d119ff7d", "1d8ea624-c29e-4971-9cf5-bc20283668e0", "787f9c30-5634-4c73-9919-79abe8828d15", "c35a4f8d-15d6-41e0-b4c8-a94810415cc2", "eff9877d-a2a3-4a14-991f-0b71c20fe109", "a1674922-8c3e-44af-a3f2-e44025eb77a5", "70e3c771-f466-4eff-980a-cfce511b758c", "5bf3a187-af73-40c4-bb1a-0930404fc821", "3ec7fc73-44b3-4a83-b7d1-f58bd47053b4", "476a25de-34d5-4641-8e65-40eb694c34fc", "ff28540a-e68b-4306-bad2-7b96b0a9d3e2", "f23ca4c5-064f-4d32-85a2-d27d994bd92f", "6b37dedf-63fe-4b55-9f1a-e7255dbbb7c2", "a5640bde-89b8-41e4-a8c2-042b4e6180a6", "73f10ddd-c20e-4e55-a685-1570d3e37c0e", "a410a23a-b234-4c96-92b5-15b8a2b73cbe", "79770ee6-0798-4906-922d-180f21e449a8", "d5dfe852-82c0-4fe8-bd02-8fa8f56c581b", "1fd477ca-8532-48ec-9f5c-cabf36f6f9f2", "de2ad7c2-f46f-400d-bbb5-75104477c545", "ee7350d4-ec09-48af-92aa-c0f13a8b0dfe", "faf06094-75cc-43cd-a3b7-48709a602788", "0c6e5bf7-5917-4245-96f5-69674b123640", "1371a77d-5622-4ba8-abdd-ccf4227cf76f", "f126aed6-3569-4d33-8dcf-e136f9e77880", "a71b0b0f-490e-4dce-b09f-3b0e403f73a5", "62857948-04f1-4c60-86a7-f43a859465ef", "9e930f9f-9e14-43ff-b6c9-ad76edfcbff7", "a2aca2bf-0501-48c3-b8c3-897167c19b36", "411ab0f0-1d4d-4c98-ab72-fabcef265118", "ff5efbe0-f2e2-4c85-ba00-048af7280fd1", "b680c56a-403d-4cb0-8ac0-2caa70e22897", "aee7b29e-e5a0-48c1-b00a-3aa4e5300ee9", "b46f3a14-c8e1-4b2a-833a-efc217e15506", "3d1ae5c1-fb8e-488f-baf5-06c71192c66f", "82e7fdcc-3872-4afe-a876-c0f6a4729dc1", "13f1038b-c82c-4105-bc97-ccce611ef899", "fb53b8c4-f434-457b-bb22-b65ff52d837a", "531880ca-8e66-4f4f-8783-fed0a61b6ca0", "a856c980-97cd-4458-b0f5-a774f6f540b1", "3be5db69-892a-4f30-8f8f-579119d8d870", "eb4c6c77-f05e-46ef-b0d1-0ba9c0bd1433", "3fc9ad98-6dc9-49a2-91dc-26ae882e2f07", "3c8979b2-5470-4054-a04c-e08b4efd6381", "77fea69e-7952-4016-97dd-23e453e6af4e", "a775a992-453d-4e07-9f58-27ebd8679bbe", "cb25a683-4134-4aaa-9806-24d34b346ba4", "f65a7f6f-d547-450f-8a8a-de48de825c76", "c5d44fab-1ec8-45d1-9128-c872b56aa921", "e4923b01-f539-49f6-8039-d5a2e73374a3", "06c27224-881f-440b-8d24-2eedc771cf3b", "416f1092-cdd6-4a53-bf45-f08ef0b4df4d", "7f8a60a1-7373-4c1b-8966-a95fb158f23f", "dc8d754f-e798-4605-9c37-5e3978383ca9", "5c60c8d8-0e81-42e3-ad78-11ab7c5083ae", "d8250ff5-1bf1-4201-aba4-ce470a6bc5f2", "5aa112ad-5b84-494b-85b3-8436f2cfeaa2", "164e1c2b-07bb-496a-aa48-34dbd2253102", "e2147ce1-2c9a-472b-928a-984d2329ad01", "5eb0fb65-28df-41fd-b755-23ab423a2dda", "f7e345af-a79f-4c15-9319-47e83c14034b", "230fc3b2-8b8b-4ff8-be28-587e563a578d", "a4bc69de-9fb5-41c2-85ae-8bcf2abd4332", "293b03d2-491d-47ee-8f74-0277e644c3dc", "956f83eb-73b4-4cf2-aded-318b665f4bb6", "cb904b2a-e3cf-4192-962c-7412286f24a1", "0315fd1d-f86d-437b-804e-aa0d24e7bdc0", "e123fac1-9eb0-4b97-8fa3-1c0e0ddf25f8", "c1bdd2f0-97b1-49c6-8633-f0f3c15d760b", "8d51831c-08cd-492d-a872-0c11266808cf", "4fda3f3f-65ea-4c26-babf-48d8761086fb", "6c6f3e73-f130-4fc0-acc6-693c353f4725", "2fa2aac6-e1be-48e6-9438-dbc221d8b634", "f6fc0c8b-f152-4cca-8a52-dbe5ddebe8e4", "92aa068c-c24c-4b8d-a496-104c1fd81b1e", "49c04758-5c7f-4ca5-9dff-36766898c7dc", "03013851-f68b-4beb-adee-2ebfe0dab352", "ec2727a8-d2ed-4dc2-a4c8-52b39ff754be", "e44afe30-1077-48d5-be6f-b761db49ccc1", "fef73a08-75f7-4516-95af-041751750caf"], "metadata": {}}}}